{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Wireless sensor networm combined with autonomous drone swarm and communication reduction\n",
    "#https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391193&casa_token=wZ2spLDNZroAAAAA:YDmwxnfhCvPGV002JGv_1lSta5d7yBgcY3P0YYrw24wKr7-hJWuTdR5tTvuWe1Z4vZgFr-pgs8Y\n",
    "\n",
    "import random as rand\n",
    "import numpy as np\n",
    "#rand.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "\n",
    "ViewRange = 2\n",
    "CommRange = 5#5\n",
    "AgentAmmount = 5\n",
    "\n",
    "#double distance = 2/3 as efficient transfer\n",
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.priority = rand.randint(1, 10)\n",
    "        self.size = rand.randint(100, 1000)\n",
    "\n",
    "\n",
    "\n",
    "#Vessel\n",
    "#Constraints: Bandwidth - Num of Chanels - communication distance\n",
    "#Objective: Energy Reduction - Task priority \n",
    "class Drone:\n",
    "    def __init__(self, x, y, viewRange, commRange, width, height, index, Sea):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.task = None\n",
    "        self.viewRange = viewRange\n",
    "        self.commRange = commRange\n",
    "        self.observation = [[0]*width]*height\n",
    "        self.id = index\n",
    "        #self.seen = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.punish = 0\n",
    "    def getView(self):\n",
    "        return None\n",
    "\n",
    "    def getObservation(self, Sea):\n",
    "        #Get view\n",
    "        #obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        reward = 0 \n",
    "        for i in range(self.y-self.viewRange, self.y+self.viewRange):\n",
    "            for j in range(self.x-self.viewRange, self.x+self.viewRange):\n",
    "                if i < 50 and i >= 0  and j < 50 and j >= 0:\n",
    "                    if Sea.board[i][j] == 0:\n",
    "                        self.obs[i][j] = 1\n",
    "                    else:\n",
    "                        self.obs[i][j] = Sea.board[i][j]\n",
    "\n",
    "                    if Sea.seen[i][j] == 0:\n",
    "                        Sea.seen[i][j] = 1\n",
    "                        reward += 1\n",
    "        reward += self.punish\n",
    "        res = deepcopy(self.obs)\n",
    "        res[self.y][self.x] = 3\n",
    "        return res, reward\n",
    "\n",
    "    def move(self,x, y, see):\n",
    "        x = x + self.x\n",
    "        y = y + self.y\n",
    "\n",
    "        self.punish = 0\n",
    "\n",
    "        if (x < 50 and x >= 0 and y < 50 and y >= 0) and (see.board[y][x] == 0 or see.board[y][x] == 2 or see.board[y][x] == -2) :\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "        else:\n",
    "            #punishment\n",
    "            self.punish -= 4\n",
    "\n",
    "    \n",
    "    def addData(self, drone):\n",
    "        pass\n",
    "    def setData(self, obs):\n",
    "        self.observation = abs\n",
    "#Constraints: Bandwidth, Num of Chanels\n",
    "#Objective Explore the sea\n",
    "class Ship:\n",
    "    def __init__(self, x, y, bandwidth):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "\n",
    "#Actions move up down left right \n",
    "class Sea:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        #self.objects = objects\n",
    "        #int array -2 = dead zone (ie no communication) -1 = object 0 = sea 1 = ship 2 = drone\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "\n",
    "        self.cmap = ListedColormap([ 'k', 'b'])\n",
    "\n",
    "    def calculateDeadZone2(self, board):\n",
    "        shipx = self.ship.x\n",
    "        shipy = self.ship.y\n",
    "\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if board[y][x] != 0:\n",
    "                    continue\n",
    "                startx = x\n",
    "                starty = y\n",
    "                x0 = startx\n",
    "                y0 = starty\n",
    "                x1 = shipx\n",
    "                y1 = shipy\n",
    "                dx = abs(x1 - x0)\n",
    "                sx = -1\n",
    "                if x0 < x1:\n",
    "                    sx = 1\n",
    "                dy = -abs(y1 - y0)\n",
    "                sy = -1\n",
    "                if y0 < y1:\n",
    "                    sy = 1\n",
    "                error = dx + dy\n",
    "\n",
    "                while True:\n",
    "                    if(board[y0][x0] == -1):\n",
    "                        board[y][x] = -2\n",
    "                        break\n",
    "                    if x0 == x1 and y0 == y1:\n",
    "                        break\n",
    "                    e2 = 2 * error\n",
    "                    if e2 >= dy:\n",
    "                        if x0 == x1: \n",
    "                            break\n",
    "                        error = error + dy\n",
    "                        x0 = x0 + sx\n",
    "                    \n",
    "                    if e2 <= dx:\n",
    "                        if y0 == y1:\n",
    "                            break\n",
    "                        error = error + dx\n",
    "                        y0 = y0 + sy\n",
    "\n",
    "    def AddShip(self, ship):\n",
    "        self.ship = ship\n",
    "        self.board[ship.y][ship.x] = 2\n",
    "        for i in range(ship.y - 2, ship.y+2):\n",
    "            for j in range(ship.x - 2, ship.x + 2):\n",
    "                if i >= 0 and j >= 0 and i < self.height and j < self.width and self.board[i][j] == -1:\n",
    "                    self.board[i][j] = 0\n",
    "\n",
    "    def display(self):\n",
    "        newBoard = np.copy(self.board)\n",
    "        if ( hasattr(self, 'ship')):\n",
    "            newBoard[self.ship.y][self.ship.x] = 2\n",
    "            #self.calculateDeadZone2(newBoard)\n",
    "            self.cmap = ListedColormap([ 'k',  'b', 'g', 'y', 'r'])\n",
    "\n",
    "        for drone in self.drones:\n",
    "            newBoard[drone.y][drone.x] = 3\n",
    "\n",
    "        plt.matshow(newBoard, cmap=self.cmap)\n",
    "\n",
    "    def interestMap(self):\n",
    "        interest = [[0]*self.width]*self.height\n",
    "        samples = np.random.multivariate_normal([-0.5, -0.5], [[1, 0],[0, 1]], 50)\n",
    "        huh  = np.reshape(samples, (10,10))\n",
    "        print(huh)\n",
    "        plt.close()\n",
    "        plt.matshow(huh)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "        \n",
    "        shipx = rand.randint(0, 49)\n",
    "        shipy = rand.randint(1, 49)\n",
    "        ship = Ship(shipx, shipy, 100)\n",
    "        self.AddShip(ship)\n",
    "        self.seen = np.array([[0]*50]*50)\n",
    "        self.drones = []\n",
    "\n",
    "        for i in range(AgentAmmount):\n",
    "            self.drones.append(Drone(shipx, shipy-1,ViewRange, CommRange, self.width, self.height, i, self))\n",
    "        observations, rewards = self.getObservation()\n",
    "\n",
    "        return observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        droneIdx= 0\n",
    "        for act in actions:\n",
    "            match act:\n",
    "                case 0:\n",
    "                    self.drones[droneIdx].move(1,0, self)\n",
    "                case 1:\n",
    "                    self.drones[droneIdx].move(-1,0, self)\n",
    "                case 2:\n",
    "                    self.drones[droneIdx].move(0,1, self)\n",
    "                case 3:\n",
    "                    self.drones[droneIdx].move(0, -1, self)\n",
    "                case 4:\n",
    "                    print(\"Im not meantr to thbe there\")\n",
    "            droneIdx+= 1\n",
    "                \n",
    "                \n",
    "        #count  = [0] * AgentAmmount\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        for drone in self.drones:\n",
    "        #            if drone.seen[i][j] == 1:\n",
    "        #                count[drone.id] += 1\n",
    "                        \n",
    "        #reward = count / float(self.width*self.height)\n",
    "        #reward = [t/float(self.width*self.height) for t in count]\n",
    "\n",
    "\n",
    "        observations, rewards = self.getObservation()\n",
    "\n",
    "        return observations, rewards, [False]*AgentAmmount, None\n",
    "\n",
    "    def getObservation(self):\n",
    "        currentIndex = 1\n",
    "        droneConnection = [0]*AgentAmmount\n",
    "\n",
    "        for drone in self.drones:\n",
    "            \n",
    "            for connectDrone in self.drones:\n",
    "                if drone.id == connectDrone.id:\n",
    "                    continue\n",
    "                if (droneConnection[drone.id] == 0 or droneConnection[drone.id] != droneConnection[connectDrone.id]) \\\n",
    "                            and math.sqrt( (drone.x - connectDrone.x)**2 + (drone.y - connectDrone.y)**2 ) < CommRange:\n",
    "                    #do stuff\n",
    "                    if droneConnection[drone.id] == 0 and droneConnection[connectDrone.id] == 0:\n",
    "                        droneConnection[drone.id] = currentIndex\n",
    "                        droneConnection[connectDrone.id] = currentIndex\n",
    "                        currentIndex += 1\n",
    "                    elif droneConnection[drone.id] != 0 and droneConnection[connectDrone.id] != 0:\n",
    "                        swap = droneConnection[connectDrone.id]\n",
    "                        for i in droneConnection:\n",
    "                            if i == swap:\n",
    "                                i = droneConnection[drone.id]\n",
    "                    else:\n",
    "                        if(droneConnection[drone.id] == 0):\n",
    "                            droneConnection[drone.id] = droneConnection[connectDrone.id]\n",
    "                        else:\n",
    "                            droneConnection[connectDrone.id] = droneConnection[drone.id]\n",
    "\n",
    "        for t in range(len(droneConnection)):\n",
    "            if droneConnection[t] == 0:\n",
    "                droneConnection[t] = currentIndex\n",
    "                currentIndex += 1\n",
    "\n",
    "        obsDict = {}\n",
    "        rewardList = [0] * AgentAmmount\n",
    "        index = 0\n",
    "        for i in droneConnection:\n",
    "\n",
    "            values, reward = self.drones[index].getObservation(self)\n",
    "\n",
    "            if str(i) in obsDict:\n",
    "                values = np.array(values).flatten()\n",
    "                curr = obsDict[str(i)]\n",
    "                \n",
    "                for i in range(len(curr)):\n",
    "                    \n",
    "                    if(curr[i] == 1 and values[i] != 0):\n",
    "                        curr[i] = values[i]\n",
    "                    if(curr[i] == 0):\n",
    "                        curr[i] = values[i]\n",
    "\n",
    "            else:\n",
    "                obsDict[str(i)] = np.array(values).flatten()\n",
    "            \n",
    "            rewardList[index]= reward\n",
    "            index += 1\n",
    "\n",
    "        observations = []\n",
    "\n",
    "\n",
    "        for i in droneConnection:\n",
    "            observations.append(obsDict[str(i)])\n",
    "\n",
    "        for index, i in enumerate(observations):\n",
    "            t = deepcopy(i)\n",
    "            t[t == 3] = 1\n",
    "            self.drones[index].obs = np.reshape(t, (50,50))\n",
    "\n",
    "\n",
    "        return observations, rewardList\n",
    "class Object:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "\n",
    "see = Sea(50, 50)\n",
    "\n",
    "see.reset()\n",
    "#obs, reward, _, _ = see.step([0, 1])\n",
    "#plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "#plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "see.display()\n",
    "obs, reward, _, _ = see.step([0, 4])\n",
    "plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "see.display()\n",
    "#plt.matshow(seen)\n",
    "print(reward)\n",
    "#obs, reward, _, _ = see.step([0, 1])\n",
    "#plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "#plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "#see.display()\n",
    "\n",
    "#print(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perlin(x, y):\n",
    "    # permutation table\n",
    "    p = np.arange(256, dtype=int)\n",
    "    np.random.shuffle(p)\n",
    "    p = np.stack([p, p]).flatten()\n",
    "    # coordinates of the top-left\n",
    "    xi, yi = x.astype(int), y.astype(int)\n",
    "    # internal coordinates\n",
    "    xf, yf = x - xi, y - yi\n",
    "    # fade factors\n",
    "    u, v = fade(xf), fade(yf)\n",
    "    # noise components\n",
    "    n00 = gradient(p[p[xi] + yi], xf, yf)\n",
    "    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n",
    "    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n",
    "    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n",
    "    # combine noises\n",
    "    x1 = lerp(n00, n10, u)\n",
    "    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n",
    "    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\n",
    "\n",
    "def lerp(a, b, x):\n",
    "    \"linear interpolation\"\n",
    "    return a + x * (b - a)\n",
    "\n",
    "def fade(t):\n",
    "    \"6t^5 - 15t^4 + 10t^3\"\n",
    "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
    "\n",
    "def gradient(h, x, y):\n",
    "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
    "    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n",
    "    g = vectors[h % 4]\n",
    "    return g[:, :, 0] * x + g[:, :, 1] * y\n",
    "\n",
    "lin = np.linspace(0, 5, 50, endpoint=False)\n",
    "x, y = np.meshgrid(lin, lin)  # FIX3: I thought I had to invert x and y here but it was a mistake\n",
    "data = perlin(x, y)\n",
    "data = data*7\n",
    "data = np.round(data)\n",
    "data[data < 0] = 0\n",
    "print(data.max())\n",
    "print(data.min())\n",
    "plt.imshow(data, origin='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machin.frame.algorithms import MADDPG \n",
    "\n",
    "from machin.frame.algorithms import MADDPG\n",
    "from machin.utils.logging import default_logger as logger\n",
    "from copy import deepcopy\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import machin.model.nets as net\n",
    "\n",
    "\n",
    "#Same, playing around with the initialization, parameters, optimizers and normalization really helped Here is the article how I fought it:\n",
    "\n",
    "#TL,DR: critic init sould match the reward distro. Minmax values should be meaningful. Delayed update for actor is good, so is gradient clipping and weight decay. \\\n",
    "# Use Adamw/RAdam/Ranger as an optimizer.\n",
    "\n",
    "#https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011?source=activity---post_recommended\n",
    "\n",
    "# Important note:\n",
    "# In order to successfully run the environment, please git clone the project\n",
    "# then run:\n",
    "#    pip install -e ./test_lib/multiagent-particle-envs/\n",
    "# in project root directory\n",
    "\n",
    "\n",
    "def create_env(env_name):\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "    import multiagent.scenarios as scenarios\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(env_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    env = MultiAgentEnv(\n",
    "        world,\n",
    "        scenario.reset_world,\n",
    "        scenario.reward,\n",
    "        scenario.observation,\n",
    "        info_callback=None,\n",
    "        shared_viewer=False,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "# configurations\n",
    "#env = create_env(\"simple_spread\")\n",
    "env = Sea(50,50)\n",
    "env.discrete_action_input = True\n",
    "\n",
    "import gym.spaces as spaces\n",
    "obs = spaces.Box(low=-2, high=2, shape=(50,50), dtype=np.int32)\n",
    "action_space = spaces.Box(low=-1, high=1, shape= (2,), dtype=np.int32)\n",
    "\n",
    "observe_dim = 50*50#env.observation_space[0].shape[0]\n",
    "action_num = 4#action_space[0].n\n",
    "max_episodes = 1000\n",
    "max_steps = 200\n",
    "# number of agents in env, fixed, do not change\n",
    "agent_num = 5\n",
    "solved_reward = -15\n",
    "solved_repeat = 5\n",
    "\n",
    "print(observe_dim)\n",
    "print(action_num)\n",
    "\n",
    "\n",
    "class weightConstraint(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'weight'):\n",
    "            w=module.weight.data\n",
    "            w=w.clamp(0.5,0.7)\n",
    "            module.weight.data=w\n",
    "\n",
    "# model definition\n",
    "class ActorDiscrete(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, 1024).to(device=\"cuda\")\n",
    "        self.fc2 = nn.Linear(1024, 256).to(device=\"cuda\")\n",
    "        self.fc3 = nn.Linear(256, 128).to(device=\"cuda\")\n",
    "        self.fc4 = nn.Linear(128, 64).to(device=\"cuda\")\n",
    "        self.fc5 = nn.Linear(64, 64).to(device=\"cuda\")\n",
    "        self.fc6 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        \n",
    "        temp = nn.Linear(32, action_dim)\n",
    "        temp.weight.data.fill_(3e-1)\n",
    "        self.fc7 =  temp.to(device=\"cuda\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        #state.to(device=\"cuda\")\n",
    "        a = t.relu(self.fc1(state))\n",
    "        a = t.relu(self.fc2(a))\n",
    "        a = t.relu(self.fc3(a))\n",
    "        a = t.relu(self.fc4(a))\n",
    "        a = t.relu(self.fc5(a))\n",
    "        a = t.relu(self.fc6(a))\n",
    "        a = t.softmax(self.fc7(a), dim=1)\n",
    "        return a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # This critic implementation is shared by the prey(DDPG) and\n",
    "        # predators(MADDPG)\n",
    "        # Note: For MADDPG\n",
    "        #       state_dim is the dimension of all states from all agents.\n",
    "        #       action_dim is the dimension of all actions from all agents.\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 1024).to(device=\"cuda\")\n",
    "        self.fc2 = nn.Linear(1024, 256).to(device=\"cuda\")\n",
    "        self.fc3 = nn.Linear(256, 128).to(device=\"cuda\")\n",
    "        self.fc4 = nn.Linear(128, 64).to(device=\"cuda\")\n",
    "        self.fc5 = nn.Linear(64, 64).to(device=\"cuda\")\n",
    "        self.fc6 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        temp = nn.Linear(32, 1)\n",
    "        temp.weight.data.fill_(3e-5)\n",
    "        self.fc7 =  temp.to(device=\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = t.cat([state, action], 1)\n",
    "        #state_action = state_action.cuda()\n",
    "        q = t.relu(self.fc1(state_action))\n",
    "        q = t.relu(self.fc2(q))\n",
    "        q = t.relu(self.fc3(q))\n",
    "        q = t.relu(self.fc4(q))\n",
    "        q = t.relu(self.fc5(q))\n",
    "        q = t.relu(self.fc6(q))\n",
    "        q = self.fc7(q)\n",
    "        #try clamping the reward\n",
    "        #q = t.clamp(q, -200, 200)\n",
    "        return q\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actor = ActorDiscrete(observe_dim, action_num)\n",
    "    critic = Critic(observe_dim * agent_num, action_num * agent_num)\n",
    "\n",
    "    print(critic.fc3.weight)\n",
    "    \n",
    "    net.static_module_wrapper(actor, \"cuda\", \"cuda\")\n",
    "    net.static_module_wrapper(critic, \"cuda\", \"cuda\")\n",
    "    maddpg = MADDPG(\n",
    "        [deepcopy(actor) for _ in range(agent_num)],\n",
    "        [deepcopy(actor) for _ in range(agent_num)],\n",
    "        [deepcopy(critic) for _ in range(agent_num)],\n",
    "        [deepcopy(critic) for _ in range(agent_num)],\n",
    "        t.optim.Adam,\n",
    "        #t.optim.RAdam,\n",
    "        #t.optim.NAdam,\n",
    "        nn.MSELoss(reduction=\"mean\"),\n",
    "        critic_visible_actors=[list(range(agent_num))] * agent_num,\n",
    "        replay_device=\"cuda\",\n",
    "        discount = 0.99,\n",
    "        replay_size=20000,\n",
    "        actor_learning_rate=0.000005,\n",
    "        critic_learning_rate=0.00005,\n",
    "        update_rate=0.0005\n",
    "    )\n",
    "    \n",
    "\n",
    "    episode, step, reward_fulfilled = 0, 0, 0\n",
    "    smoothed_total_reward = 0\n",
    "\n",
    "    while episode < max_episodes:\n",
    "        episode += 1\n",
    "        total_reward = 0\n",
    "        terminal = False\n",
    "        step = 0\n",
    "        states = [\n",
    "            t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in env.reset()\n",
    "        ]\n",
    "        tmp_observations_list = [[] for _ in range(agent_num)]\n",
    "\n",
    "        while not terminal and step <= max_steps:\n",
    "            step += 1\n",
    "            with t.no_grad():\n",
    "                old_states = states\n",
    "                # agent model inference\n",
    "                results = maddpg.act_discrete_with_noise(\n",
    "                    [{\"state\": st} for st in states]\n",
    "                )\n",
    "                actions = [int(r[0]) for r in results]\n",
    "                action_probs = [r[1] for r in results]\n",
    "                #print(actions)\n",
    "                states, rewards, terminals, _ = env.step(actions)\n",
    "                states = [\n",
    "                    t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in states\n",
    "                ]\n",
    "                total_reward += float(sum(rewards))# / agent_num\n",
    "\n",
    "                for tmp_observations, ost, act, st, rew, term in zip(\n",
    "                    tmp_observations_list,\n",
    "                    old_states,\n",
    "                    action_probs,\n",
    "                    states,\n",
    "                    rewards,\n",
    "                    terminals,\n",
    "                ):\n",
    "                    tmp_observations.append(\n",
    "                        {\n",
    "                            \"state\": {\"state\": ost},\n",
    "                            \"action\": {\"action\": act},\n",
    "                            \"next_state\": {\"state\": st},\n",
    "                            \"reward\": float(rew),\n",
    "                            \"terminal\": term or step == max_steps,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        maddpg.store_episodes(tmp_observations_list)\n",
    "        # total reward is divided by steps here, since:\n",
    "        # \"Agents are rewarded based on minimum agent distance\n",
    "        #  to each landmark, penalized for collisions\"\n",
    "        #total_reward /= \n",
    "        totalAvgReward = total_reward / 50 / 50#step\n",
    "        totalSpaceVisited = np.sum(env.seen)\n",
    "\n",
    "        #maddpg.discount = episode / max_episodes *0.29 + 0.7\n",
    "        print(f\"Episode {episode}\")\n",
    "        print(f\"Number visited {totalSpaceVisited}\")\n",
    "        print(f\"Episode {episode} discount {maddpg.discount}\")\n",
    "        # update, update more if episode is longer, else less\n",
    "        if episode > 100:\n",
    "            for _ in range(step):\n",
    "                maddpg.update()\n",
    "\n",
    "            # show reward\n",
    "        smoothed_total_reward = smoothed_total_reward * 0.9 + totalAvgReward * 0.1\n",
    "        print(f\"Episode {episode} total reward={smoothed_total_reward:.2f}\")\n",
    "        print(f\"Episode {episode} reward={total_reward}\")\n",
    "        print(f\"Episode {episode} avg reward={totalAvgReward}\")\n",
    "        \n",
    "        #if smoothed_total_reward > solved_reward and episode > 100:\n",
    "        #    reward_fulfilled += 1\n",
    "        #    if reward_fulfilled >= solved_repeat:\n",
    "        #        logger.info(\"Environment solved!\")\n",
    "        #        exit(0)\n",
    "        #else:\n",
    "        #    reward_fulfilled = 0\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPPO https://github.com/marlbenchmark/on-policy\n",
    "\n",
    "episode = 0\n",
    "while episode < 1:\n",
    "    episode += 1\n",
    "    total_reward = 0\n",
    "    terminal = False\n",
    "    step = 0\n",
    "    states = [\n",
    "        t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in env.reset()\n",
    "    ]\n",
    "    tmp_observations_list = [[] for _ in range(agent_num)]\n",
    "\n",
    "    while not terminal and step <= max_steps:\n",
    "        step += 1\n",
    "        with t.no_grad():\n",
    "            old_states = states\n",
    "            # agent model inference\n",
    "            results = maddpg.act_discrete_with_noise(\n",
    "                [{\"state\": st} for st in states]\n",
    "            )\n",
    "            actions = [int(r[0]) for r in results]\n",
    "            action_probs = [r[1] for r in results]\n",
    "            #print(actions)\n",
    "            states, rewards, terminals, _ = env.step(actions)\n",
    "            states = [\n",
    "                t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in states\n",
    "            ]\n",
    "            total_reward += float(sum(rewards))# / agent_num\n",
    "            #plt.matshow(np.reshape(states[0], (50,50)) )\n",
    "            for tmp_observations, ost, act, st, rew, term in zip(\n",
    "                tmp_observations_list,\n",
    "                old_states,\n",
    "                action_probs,\n",
    "                states,\n",
    "                rewards,\n",
    "                terminals,\n",
    "            ):\n",
    "                tmp_observations.append(\n",
    "                    {\n",
    "                        \"state\": {\"state\": ost},\n",
    "                        \"action\": {\"action\": act},\n",
    "                        \"next_state\": {\"state\": st},\n",
    "                        \"reward\": float(rew),\n",
    "                        \"terminal\": term or step == max_steps,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    #maddpg.store_episodes(tmp_observations_list)\n",
    "    env.display()\n",
    "    plt.matshow(env.seen)\n",
    "    # total reward is divided by steps here, since:\n",
    "    # \"Agents are rewarded based on minimum agent distance\n",
    "    #  to each landmark, penalized for collisions\"\n",
    "    #total_reward /= \n",
    "    totalAvgReward = total_reward / 50 / 50#step\n",
    "    maddpg.discount = 0.99#episode / max_episodes *0.7 + 0.3\n",
    "    print(f\"Episode {episode}\")\n",
    "    # update, update more if episode is longer, else less\n",
    "    #if episode > 100:\n",
    "    #    for _ in range(step):\n",
    "    #        maddpg.update()\n",
    "\n",
    "        # show reward\n",
    "    smoothed_total_reward = smoothed_total_reward * 0.9 + totalAvgReward * 0.1\n",
    "    print(f\"Episode {episode} total reward={smoothed_total_reward:.2f}\")\n",
    "    print(f\"Episode {episode} reward={total_reward}\")\n",
    "    print(f\"Episode {episode} avg reward={totalAvgReward}\")\n",
    "    \n",
    "    #if smoothed_total_reward > solved_reward and episode > 100:\n",
    "    #    reward_fulfilled += 1\n",
    "    #    if reward_fulfilled >= solved_repeat:\n",
    "    #        logger.info(\"Environment solved!\")\n",
    "    #        exit(0)\n",
    "    #else:\n",
    "    #    reward_fulfilled = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maddpg.save(\"Rugght here\", version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.display()\n",
    "for i in range(len(states)):\n",
    "    plt.matshow( np.reshape(states[i], (50,50)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c7f822a9fda7ffd10530ef71b8007e75e5e59461c46d04a19e3026085bccd1b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
