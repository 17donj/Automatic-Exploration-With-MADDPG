{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im not meantr to thbe there\n",
      "Im not meantr to thbe there\n",
      "Im not meantr to thbe there\n",
      "Im not meantr to thbe there\n",
      "[0.09184423218221896, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091, 0.09090909090909091]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Wireless sensor networm combined with autonomous drone swarm and communication reduction\n",
    "#https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391193&casa_token=wZ2spLDNZroAAAAA:YDmwxnfhCvPGV002JGv_1lSta5d7yBgcY3P0YYrw24wKr7-hJWuTdR5tTvuWe1Z4vZgFr-pgs8Y\n",
    "\n",
    "import random as rand\n",
    "import numpy as np\n",
    "#rand.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "\n",
    "ViewRange = 2\n",
    "CommRange = 5#5\n",
    "AgentAmmount = 5\n",
    "\n",
    "#double distance = 2/3 as efficient transfer\n",
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.priority = rand.randint(1, 10)\n",
    "        self.size = rand.randint(100, 1000)\n",
    "\n",
    "\n",
    "\n",
    "#Vessel\n",
    "#Constraints: Bandwidth - Num of Chanels - communication distance\n",
    "#Objective: Energy Reduction - Task priority \n",
    "class Drone:\n",
    "    def __init__(self, x, y, viewRange, commRange, width, height, index, Sea):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.task = None\n",
    "        self.viewRange = viewRange\n",
    "        self.commRange = commRange\n",
    "        self.observation = [[0]*width]*height\n",
    "        self.id = index\n",
    "        #self.seen = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.punish = 0\n",
    "        self.totalReward = 0\n",
    "    def getView(self):\n",
    "        return None\n",
    "\n",
    "    def getObservation(self, Sea):\n",
    "        #Get view\n",
    "        #obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        reward = 0 \n",
    "        for i in range(self.y-self.viewRange, self.y+self.viewRange):\n",
    "            for j in range(self.x-self.viewRange, self.x+self.viewRange):\n",
    "                if i < 50 and i >= 0  and j < 50 and j >= 0:\n",
    "                    if Sea.board[i][j] == 0:\n",
    "                        self.obs[i][j] = 1\n",
    "                    else:\n",
    "                        self.obs[i][j] = Sea.board[i][j]\n",
    "\n",
    "                    if Sea.seen[i][j] == 0:\n",
    "                        Sea.seen[i][j] = 1\n",
    "                        reward += 1\n",
    "                        self.totalReward += 1\n",
    "        reward += self.punish\n",
    "        res = deepcopy(self.obs)\n",
    "        #make them seperate\n",
    "        reward = float(self.totalReward)/50/50\n",
    "        reward = -1/(10*(reward - 1.1))\n",
    "\n",
    "        res[self.y][self.x] = 3 + self.id\n",
    "        return res, reward\n",
    "\n",
    "    def move(self,x, y, see):\n",
    "        x = x + self.x\n",
    "        y = y + self.y\n",
    "\n",
    "        self.punish = 0\n",
    "\n",
    "        if (x < 50 and x >= 0 and y < 50 and y >= 0) and (see.board[y][x] == 0 or see.board[y][x] == 2 or see.board[y][x] == -2) :\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "        else:\n",
    "            #punishment\n",
    "            self.punish -= 4\n",
    "\n",
    "    \n",
    "    def addData(self, drone):\n",
    "        pass\n",
    "    def setData(self, obs):\n",
    "        self.observation = abs\n",
    "#Constraints: Bandwidth, Num of Chanels\n",
    "#Objective Explore the sea\n",
    "class Ship:\n",
    "    def __init__(self, x, y, bandwidth):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "\n",
    "#Actions move up down left right \n",
    "class Sea:\n",
    "\n",
    "    def __init__(self):#, width, height):\n",
    "        self.width = 50#width\n",
    "        self.height =50# height\n",
    "\n",
    "        self.observe_dim = 50*50#env.observation_space[0].shape[0]\n",
    "        self.action_num = 4\n",
    "        self.max_step = 200\n",
    "        self.state_dim = 50*50\n",
    "        self.action_dim = 4\n",
    "        self.target_return = 50*50\n",
    "        self.env_num = 500\n",
    "        self.if_discrete = True\n",
    "        self.env_name = \"Sea\"\n",
    "        self.reward_range = (-200*4, 50*50)\n",
    "        #self.objects = objects\n",
    "        #int array -2 = dead zone (ie no communication) -1 = object 0 = sea 1 = ship 2 = drone\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "\n",
    "        self.cmap = ListedColormap([ 'k', 'b'])\n",
    "\n",
    "    def calculateDeadZone2(self, board):\n",
    "        shipx = self.ship.x\n",
    "        shipy = self.ship.y\n",
    "\n",
    "        for y in range(0, self.height):\n",
    "            for x in range(0, self.width):\n",
    "                if board[y][x] != 0:\n",
    "                    continue\n",
    "                startx = x\n",
    "                starty = y\n",
    "                x0 = startx\n",
    "                y0 = starty\n",
    "                x1 = shipx\n",
    "                y1 = shipy\n",
    "                dx = abs(x1 - x0)\n",
    "                sx = -1\n",
    "                if x0 < x1:\n",
    "                    sx = 1\n",
    "                dy = -abs(y1 - y0)\n",
    "                sy = -1\n",
    "                if y0 < y1:\n",
    "                    sy = 1\n",
    "                error = dx + dy\n",
    "\n",
    "                while True:\n",
    "                    if(board[y0][x0] == -1):\n",
    "                        board[y][x] = -2\n",
    "                        break\n",
    "                    if x0 == x1 and y0 == y1:\n",
    "                        break\n",
    "                    e2 = 2 * error\n",
    "                    if e2 >= dy:\n",
    "                        if x0 == x1: \n",
    "                            break\n",
    "                        error = error + dy\n",
    "                        x0 = x0 + sx\n",
    "                    \n",
    "                    if e2 <= dx:\n",
    "                        if y0 == y1:\n",
    "                            break\n",
    "                        error = error + dx\n",
    "                        y0 = y0 + sy\n",
    "\n",
    "    def AddShip(self, ship):\n",
    "        self.ship = ship\n",
    "        self.board[ship.y][ship.x] = 2\n",
    "        for i in range(ship.y - 2, ship.y+2):\n",
    "            for j in range(ship.x - 2, ship.x + 2):\n",
    "                if i >= 0 and j >= 0 and i < self.height and j < self.width and self.board[i][j] == -1:\n",
    "                    self.board[i][j] = 0\n",
    "\n",
    "    def display(self):\n",
    "        newBoard = np.copy(self.board)\n",
    "        if ( hasattr(self, 'ship')):\n",
    "            newBoard[self.ship.y][self.ship.x] = 2\n",
    "            #self.calculateDeadZone2(newBoard)\n",
    "            self.cmap = ListedColormap([ 'k',  'b', 'g', 'y', 'r'])\n",
    "\n",
    "        for drone in self.drones:\n",
    "            newBoard[drone.y][drone.x] = 3\n",
    "\n",
    "        plt.matshow(newBoard, cmap=self.cmap)\n",
    "\n",
    "    def interestMap(self):\n",
    "        interest = [[0]*self.width]*self.height\n",
    "        samples = np.random.multivariate_normal([-0.5, -0.5], [[1, 0],[0, 1]], 50)\n",
    "        huh  = np.reshape(samples, (10,10))\n",
    "        print(huh)\n",
    "        plt.close()\n",
    "        plt.matshow(huh)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "        \n",
    "        shipx = rand.randint(0, 49)\n",
    "        shipy = rand.randint(1, 49)\n",
    "        ship = Ship(shipx, shipy, 100)\n",
    "        self.AddShip(ship)\n",
    "        self.seen = np.array([[0]*50]*50)\n",
    "        self.drones = []\n",
    "\n",
    "        for i in range(AgentAmmount):\n",
    "            self.drones.append(Drone(shipx, shipy-1,ViewRange, CommRange, self.width, self.height, i, self))\n",
    "        observations, rewards = self.getObservation()\n",
    "\n",
    "        return observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        droneIdx= 0\n",
    "        for act in actions:\n",
    "            match act:\n",
    "                case 0:\n",
    "                    self.drones[droneIdx].move(1,0, self)\n",
    "                case 1:\n",
    "                    self.drones[droneIdx].move(-1,0, self)\n",
    "                case 2:\n",
    "                    self.drones[droneIdx].move(0,1, self)\n",
    "                case 3:\n",
    "                    self.drones[droneIdx].move(0, -1, self)\n",
    "                case 4:\n",
    "                    print(\"Im not meantr to thbe there\")\n",
    "            droneIdx+= 1\n",
    "                \n",
    "                \n",
    "        #count  = [0] * AgentAmmount\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        for drone in self.drones:\n",
    "        #            if drone.seen[i][j] == 1:\n",
    "        #                count[drone.id] += 1\n",
    "                        \n",
    "        #reward = count / float(self.width*self.height)\n",
    "        #reward = [t/float(self.width*self.height) for t in count]\n",
    "\n",
    "\n",
    "        observations, rewards = self.getObservation()\n",
    "\n",
    "        return observations, rewards, [False]*AgentAmmount, None\n",
    "\n",
    "    def getObservation(self):\n",
    "        currentIndex = 1\n",
    "        droneConnection = [0]*AgentAmmount\n",
    "\n",
    "        for drone in self.drones:\n",
    "            \n",
    "            for connectDrone in self.drones:\n",
    "                if drone.id == connectDrone.id:\n",
    "                    continue\n",
    "                if (droneConnection[drone.id] == 0 or droneConnection[drone.id] != droneConnection[connectDrone.id]) \\\n",
    "                            and math.sqrt( (drone.x - connectDrone.x)**2 + (drone.y - connectDrone.y)**2 ) < CommRange:\n",
    "                    #do stuff\n",
    "                    if droneConnection[drone.id] == 0 and droneConnection[connectDrone.id] == 0:\n",
    "                        droneConnection[drone.id] = currentIndex\n",
    "                        droneConnection[connectDrone.id] = currentIndex\n",
    "                        currentIndex += 1\n",
    "                    elif droneConnection[drone.id] != 0 and droneConnection[connectDrone.id] != 0:\n",
    "                        swap = droneConnection[connectDrone.id]\n",
    "                        for i in droneConnection:\n",
    "                            if i == swap:\n",
    "                                i = droneConnection[drone.id]\n",
    "                    else:\n",
    "                        if(droneConnection[drone.id] == 0):\n",
    "                            droneConnection[drone.id] = droneConnection[connectDrone.id]\n",
    "                        else:\n",
    "                            droneConnection[connectDrone.id] = droneConnection[drone.id]\n",
    "\n",
    "        for t in range(len(droneConnection)):\n",
    "            if droneConnection[t] == 0:\n",
    "                droneConnection[t] = currentIndex\n",
    "                currentIndex += 1\n",
    "\n",
    "        obsDict = {}\n",
    "        rewardList = [0] * AgentAmmount\n",
    "        index = 0\n",
    "        for i in droneConnection:\n",
    "\n",
    "            values, reward = self.drones[index].getObservation(self)\n",
    "\n",
    "            if str(i) in obsDict:\n",
    "                values = np.array(values).flatten()\n",
    "                curr = obsDict[str(i)]\n",
    "                \n",
    "                for i in range(len(curr)):\n",
    "                    \n",
    "                    if(curr[i] == 1 and values[i] != 0):\n",
    "                        curr[i] = values[i]\n",
    "                    if(curr[i] == 0):\n",
    "                        curr[i] = values[i]\n",
    "\n",
    "            else:\n",
    "                obsDict[str(i)] = np.array(values).flatten()\n",
    "            \n",
    "            rewardList[index]= reward\n",
    "            index += 1\n",
    "\n",
    "        observations = []\n",
    "\n",
    "\n",
    "        for i in droneConnection:\n",
    "            observations.append(obsDict[str(i)])\n",
    "\n",
    "        for index, i in enumerate(observations):\n",
    "            t = deepcopy(i)\n",
    "            t[t == 3] = 1\n",
    "            t[ t == 4] = 1\n",
    "            t[t == 5 ] = 1\n",
    "            t[t == 6 ] = 1\n",
    "            t[ t == 7 ] = 1\n",
    "            t[ t == 8 ] = 1\n",
    "            self.drones[index].obs = np.reshape(t, (50,50))\n",
    "\n",
    "\n",
    "        return observations, rewardList\n",
    "class Object:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "\n",
    "see = Sea()#50, 50)\n",
    "\n",
    "see.reset()\n",
    "#obs, reward, _, _ = see.step([0, 1])\n",
    "#plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "#plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "see.display()\n",
    "obs, reward, _, _ = see.step([0, 4])\n",
    "obs, reward, _, _ = see.step([0, 4])\n",
    "obs, reward, _, _ = see.step([0, 4])\n",
    "obs, reward, _, _ = see.step([0, 4])\n",
    "plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "see.display()\n",
    "#plt.matshow(seen)\n",
    "print(reward)\n",
    "#obs, reward, _, _ = see.step([0, 1])\n",
    "#plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "#plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "#see.display()\n",
    "\n",
    "#print(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e7f4802a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPiUlEQVR4nO3df6gd9ZnH8ffHeJNUq8R0JcTEri4rLQq7CkErbqWkK7i2VBdkqZQlLYH8swuWdqlxFxYK+4f9p1bYpSVU2yyUxtZ2UaTLko0pacFG46+uMbSmQmtsNF3aUH+wadI8+8eZ1Hsn5zhz55w5Z+Y8nxdccmbO3Jnnzpkn3/N85zszigjMbP6dM+sAzGw6nOxmSTjZzZJwspsl4WQ3S8LJbpbEWMku6WZJP5F0WNL2SQVlZpOnpufZJa0AfgrcBBwBngTuiIgXRv3OSq2K1ZzfaHtdcHrN+LGfc/zNCURST1W8Jy9Y/joXXq9eZpp/46y0sW+hev9W7dv/401+Fyc07L1zm4UEwLXA4Yh4CUDSLuBWYGSyr+Z8rtOHx9jkbL21+bqx13Hef+yfQCT1VMX7yxuHHhPv6JJ91Y3DNP/GWWlj30L1/q3at/tjz8j3xvkavwF4edH0kWKemXXQOC17LZK2AdsAVnNe25szsxHGSfZXgEsXTW8s5i0RETuAHQAXaq0H4veMv7a3Z9r7dpyv8U8CV0i6XNJK4OPAI5MJy8wmrXHLHhGnJP098F/ACuCBiDg4scjMbKLGqtkj4nvA9yYUi5m1yCPozJJovTfeLKM6nW/DtNnZ6ZbdLAknu1kSTnazJFyzv4O3/nr545+b1mp94QE0A+X9UHWsDPudaXPLbpaEk90sCSe7WRKu2Qt1aq461yiXl5m3Gn7Yfpp1LdoFfdgHbtnNknCymyXhZDdLwslulkSaDro6HXBlTW8aOE/mvcMxE7fsZkk42c2ScLKbJdG7mr1J7T1MW/X4vNe0Qy/+Yeln0ocBJhm5ZTdLwsluloST3SwJJ7tZEp3voJvU1Wiz0qXOqjr76cbr3/k5H/sev6pyvX3rsCsfY12Ptym37GZJONnNknCymyXRuZq9XD/1bfDLvNZ7Zwyr6YfV8V1Vpw9oXmt4t+xmSTjZzZJwspslMdOafVIXtdTRRo3e91qu6px6U1Xn3WF6+26ax1jXuWU3S8LJbpaEk90sicpkl/SApGOSnl80b62k3ZJeLP69qN0wzWxcdTrovg78K/Dvi+ZtB/ZExD2SthfTd00+vGbqdMb1vXNtWr723h8smf7ULz541jLljr4+DbLJpLJlj4h9wK9Ls28FdhavdwK3TTYsM5u0pqfe1kXE0eL1q8C6UQtK2gZsA1jNeQ03Z2bjGruDLiICGPm9OSJ2RMSmiNi0wKpxN2dmDTVt2V+TtD4ijkpaDxybZFDL5Rq9PcNqdOuWxQOHTj/2o5HLNW3ZHwG2FK+3AA83XI+ZTUmdU2/fBB4H3ifpiKStwD3ATZJeBP6ymDazDqv8Gh8Rd4x468MTjsXMWtS5m1eUNbmAxfX5cOV9uY+zz4c3uTimfF593p+KM0vjXNjj4bJmSTjZzZJwspsl4WQ3S2KmHXTDOtJ8Z5HZ6vtFLFXHT527FZc7GIets0t32ln8N518cvRybtnNknCymyXhZDdLonODasq1UJ2axYNohqval9Ma/NLW59PWE37PujvukP3Ux6fGuGU3S8LJbpaEk90sCQ1uNDMdF2ptXCdfLGfNtFWjN1HV3zGrfornHruPN37z8tCd4JbdLAknu1kSTnazJJzsZkl0blCN2RmTuKilLVUDb9q6eGaci8fcspsl4WQ3S8LJbpaEa/YemOXNE7pkljV6ly0+Fs6JN0cu55bdLAknu1kSTnazJNLU7FXnIrtUA9c5b9rHmye8ky7daHQSN/Xo4ufhlt0sCSe7WRJOdrMknOxmScxlB12Tzp6uP/XDJqOtO+p2sUOuzC27WRJOdrMkKpNd0qWS9kp6QdJBSXcW89dK2i3pxeLfi9oP18yaqlOznwI+GxFPS7oAeErSbuCTwJ6IuEfSdmA7cFd7oZotX50avQ/19iRUtuwRcTQini5evw4cAjYAtwI7i8V2Are1FKOZTcCyanZJlwHXAPuBdRFxtHjrVWDdZEMzs0mqneyS3g18B/h0RPx28XsxeNLE0O9LkrZJOiDpwElOjBWsmTVXK9klLTBI9G9ExHeL2a9JWl+8vx44Nux3I2JHRGyKiE0LrJpEzGbWQGUHnSQB9wOHIuKLi956BNgC3FP8+3ArEZotw6wey9QHdXrjbwD+FvgfSc8W8/6RQZJ/S9JW4OfA37QSoZlNRGWyR8QPgVE3//JTGs16wiPozJKYywthmjw1Y5a1XHnbvjDG2uCW3SwJJ7tZEk52syTmsmYfpk/nV+vU8H36e2bJ++ltbtnNknCymyXhZDdLwslulkSaDro+cydTc/P2mKxxuGU3S8LJbpaEk90sCdfsNld+eePSq7HbegJMH7llN0vCyW6WhJPdLAnX7DbXzqrhyXtRkVt2sySc7GZJONnNknCymyXhDrpE+nZRSLlzrezG6w9WrmPf41dVrrPcadf1/dKUW3azJJzsZkk42c2ScM0+x6qeLJPhrrXlur5cw2filt0sCSe7WRJOdrMkXLPbEl0+F1/nvHoTVRfLdGkfjMMtu1kSTnazJJzsZklUJruk1ZKekPScpIOSPl/Mv1zSfkmHJT0oaWX74ZpZU3U66E4AmyPiDUkLwA8l/SfwGeDeiNgl6SvAVuDLLcZqVulr7/3BkulP/eKDS6aHdfJlGWhT2bLHwBvF5ELxE8Bm4KFi/k7gtjYCNLPJqFWzS1oh6VngGLAb+BlwPCJOFYscATaM+N1tkg5IOnCSExMI2cyaqJXsEfH7iLga2AhcC7y/7gYiYkdEbIqITQusahalmY1tWYNqIuK4pL3A9cAaSecWrftG4JU2ApyWqotGhpmXwRZ9Ua6th9Xf5Ro9g8XH7unHfjRyuTq98RdLWlO8fhdwE3AI2AvcXiy2BXi4cbRm1ro6Lft6YKekFQz+c/hWRDwq6QVgl6R/AZ4B7m8xTjMbU2WyR8SPgWuGzH+JQf1uZj3gEXRmSaS96q1Jh1yddbjTrplh+6189VnV3WYzGOe4dctuloST3SwJJ7tZEmlq9nKtM4n675J9UbmdadXwdWq5s+7IMiT+Phl2AUsbd7OZZd/Mcj/Xk0+OXs4tu1kSTnazJJzsZknMZc0+iXPoXdekRq/zfpfq+HJdXOe8e52LZarU6dvo8l14R3HLbpaEk90sCSe7WRJOdrMk5rKDbpi2BtGUtdFRM4nOuHlQ1WEHZ++HSdw5tk5HZlsdduX1DDsWFsfyq9dHr8stu1kSTnazJJzsZkmkqdmbqKrR2xpI4Rq9nlne8KJq4E1bF88MW0fdQWRu2c2ScLKbJeFkN0tiLmr2SVz4Mqtz6LPUpYte2lL+G+exr2PxcXlOvDlyObfsZkk42c2ScLKbJeFkN0tiLjroyup0wnS5Q265Fz/Y26r23bT2Wxc7c92ymyXhZDdLwslulkTvavYMd44tG+fih+Wud97U+Ruz7Eu37GZJONnNkqid7JJWSHpG0qPF9OWS9ks6LOlBSSvbC9PMxrWcmv1O4BBwYTH9BeDeiNgl6SvAVuDLE47PN3IYoQ81Yl9k2Ze1WnZJG4GPAF8tpgVsBh4qFtkJ3NZCfGY2IXW/xn8J+Bxwuph+D3A8Ik4V00eADcN+UdI2SQckHTjJiXFiNbMxVCa7pI8CxyLiqSYbiIgdEbEpIjYtsKrJKsxsAurU7DcAH5N0C7CaQc1+H7BG0rlF674ReKW9MM1sXJXJHhF3A3cDSPoQ8A8R8QlJ3wZuB3YBW4CHq9Z1es35vLV5vAEMGTvj+qatgU9ZOtLaMs559ruAz0g6zKCGv38yIZlZG5Y1XDYivg98v3j9EnDt5EMyszZ4BJ1ZEp27EMY1ef9M6+KkOttxXT+aW3azJJzsZkk42c2SmGrNfvKC7tbkrvXa09ZnPuzmkeW63p/r29yymyXhZDdLwsluloST3SyJzg2qaYufoNKeOk+wacOwjj9/zqO5ZTdLwsluloST3SyJuazZu/yE1oyGfR5tDbQpr/cSPMjmDLfsZkk42c2ScLKbJeFkN0tiqh10C68v7ayZVCfNrDrkfBfV4eoMsil/ZnWOhRuvP1i5zL7Hr3rH9ZY77KD/+7sut+xmSTjZzZJwspslMdNBNU0GW8xywMy0Hh89rK4s61OdOSzW8r6c1MCbcl1fVcNDnoE3btnNknCymyXhZDdLYqo1+znH31xSD9U5/1pHl2usOueGy/axtM6cxxsyTOuGF1U1fCZu2c2ScLKbJeFkN0vCyW6WxFzeqWZahg3QaNIh12Q7WQaCTEOWu9u4ZTdLwsluloST3SwJRUxvwIakXwE/B/4I+N+pbXg8fYoV+hVvn2KFfsT7xxFx8bA3pprsf9iodCAiNk19ww30KVboV7x9ihX6F2+Zv8abJeFkN0tiVsm+Y0bbbaJPsUK/4u1TrNC/eJeYSc1uZtPnr/FmSUw12SXdLOknkg5L2j7Nbdch6QFJxyQ9v2jeWkm7Jb1Y/HvRLGM8Q9KlkvZKekHSQUl3FvO7Gu9qSU9Ieq6I9/PF/Msl7S+OiQclrZx1rGdIWiHpGUmPFtOdjbWOqSW7pBXAvwF/BVwJ3CHpymltv6avAzeX5m0H9kTEFcCeYroLTgGfjYgrgQ8Af1fsz67GewLYHBF/DlwN3CzpA8AXgHsj4k+B3wBbZxfiWe4EDi2a7nKslabZsl8LHI6IlyLid8Au4NYpbr9SROwDfl2afSuws3i9E7htmjGNEhFHI+Lp4vXrDA7KDXQ33oiIN4rJheIngM3AQ8X8zsQraSPwEeCrxbToaKx1TTPZNwAvL5o+UszrunURcbR4/SqwbpbBDCPpMuAaYD8djrf4WvwscAzYDfwMOB4Rp4pFunRMfAn4HHC6mH4P3Y21FnfQLUMMTl106vSFpHcD3wE+HRG/Xfxe1+KNiN9HxNXARgbf9N4/24iGk/RR4FhEPDXrWCZpmtezvwJcumh6YzGv616TtD4ijkpaz6BV6gRJCwwS/RsR8d1idmfjPSMijkvaC1wPrJF0btFiduWYuAH4mKRbgNXAhcB9dDPW2qbZsj8JXFH0aK4EPg48MsXtN/UIsKV4vQV4eIax/EFRQ94PHIqILy56q6vxXixpTfH6XcBNDPoZ9gK3F4t1It6IuDsiNkbEZQyO08ci4hN0MNZliYip/QC3AD9lUKv90zS3XTO+bwJHgZMMarKtDGq1PcCLwH8Da2cdZxHrXzD4iv5j4Nni55YOx/tnwDNFvM8D/1zM/xPgCeAw8G1g1axjLcX9IeDRPsRa9eMRdGZJuIPOLAknu1kSTnazJJzsZkk42c2ScLKbJeFkN0vCyW6WxP8DUzg1nf4OIGIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def perlin(x, y):\n",
    "    # permutation table\n",
    "    p = np.arange(256, dtype=int)\n",
    "    np.random.shuffle(p)\n",
    "    p = np.stack([p, p]).flatten()\n",
    "    # coordinates of the top-left\n",
    "    xi, yi = x.astype(int), y.astype(int)\n",
    "    # internal coordinates\n",
    "    xf, yf = x - xi, y - yi\n",
    "    # fade factors\n",
    "    u, v = fade(xf), fade(yf)\n",
    "    # noise components\n",
    "    n00 = gradient(p[p[xi] + yi], xf, yf)\n",
    "    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n",
    "    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n",
    "    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n",
    "    # combine noises\n",
    "    x1 = lerp(n00, n10, u)\n",
    "    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n",
    "    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\n",
    "\n",
    "def lerp(a, b, x):\n",
    "    \"linear interpolation\"\n",
    "    return a + x * (b - a)\n",
    "\n",
    "def fade(t):\n",
    "    \"6t^5 - 15t^4 + 10t^3\"\n",
    "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
    "\n",
    "def gradient(h, x, y):\n",
    "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
    "    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n",
    "    g = vectors[h % 4]\n",
    "    return g[:, :, 0] * x + g[:, :, 1] * y\n",
    "\n",
    "lin = np.linspace(0, 5, 50, endpoint=False)\n",
    "x, y = np.meshgrid(lin, lin)  # FIX3: I thought I had to invert x and y here but it was a mistake\n",
    "data = perlin(x, y)\n",
    "data = data*7\n",
    "data = np.round(data)\n",
    "data[data < 0] = 0\n",
    "print(data.max())\n",
    "print(data.min())\n",
    "plt.imshow(data, origin='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "4\n",
      "Episode 1\n",
      "Number visited 375\n",
      "Episode 1 exploratrion 0.19986666666666666\n",
      "Episode 1 total reward=9.33\n",
      "Episode 1 reward=93.28965272868786\n",
      "Episode 1 avg reward=93.28965272868786\n",
      "Episode 2\n",
      "Number visited 79\n",
      "Episode 2 exploratrion 0.19973333333333335\n",
      "Episode 2 total reward=17.57\n",
      "Episode 2 reward=91.75244148148694\n",
      "Episode 2 avg reward=91.75244148148694\n",
      "Episode 3\n",
      "Number visited 318\n",
      "Episode 3 exploratrion 0.1996\n",
      "Episode 3 total reward=25.14\n",
      "Episode 3 reward=93.25531667758781\n",
      "Episode 3 avg reward=93.25531667758781\n",
      "Episode 4\n",
      "Number visited 128\n",
      "Episode 4 exploratrion 0.19946666666666668\n",
      "Episode 4 total reward=31.82\n",
      "Episode 4 reward=91.93278404099789\n",
      "Episode 4 avg reward=91.93278404099789\n",
      "Episode 5\n",
      "Number visited 302\n",
      "Episode 5 exploratrion 0.19933333333333333\n",
      "Episode 5 total reward=37.93\n",
      "Episode 5 reward=92.97783001923007\n",
      "Episode 5 avg reward=92.97783001923007\n",
      "Episode 6\n",
      "Number visited 731\n",
      "Episode 6 exploratrion 0.19920000000000002\n",
      "Episode 6 total reward=43.60\n",
      "Episode 6 reward=94.54196315618843\n",
      "Episode 6 avg reward=94.54196315618843\n",
      "Episode 7\n",
      "Number visited 440\n",
      "Episode 7 exploratrion 0.19906666666666667\n",
      "Episode 7 total reward=48.60\n",
      "Episode 7 reward=93.64974681164412\n",
      "Episode 7 avg reward=93.64974681164412\n",
      "Episode 8\n",
      "Number visited 547\n",
      "Episode 8 exploratrion 0.19893333333333335\n",
      "Episode 8 total reward=53.12\n",
      "Episode 8 reward=93.83435547666019\n",
      "Episode 8 avg reward=93.83435547666019\n",
      "Episode 9\n",
      "Number visited 753\n",
      "Episode 9 exploratrion 0.1988\n",
      "Episode 9 total reward=57.32\n",
      "Episode 9 reward=95.09268145709632\n",
      "Episode 9 avg reward=95.09268145709632\n",
      "Episode 10\n",
      "Number visited 301\n",
      "Episode 10 exploratrion 0.19866666666666669\n",
      "Episode 10 total reward=60.90\n",
      "Episode 10 reward=93.10458781538371\n",
      "Episode 10 avg reward=93.10458781538371\n",
      "Episode 11\n",
      "Number visited 470\n",
      "Episode 11 exploratrion 0.19853333333333334\n",
      "Episode 11 total reward=64.19\n",
      "Episode 11 reward=93.76977276649355\n",
      "Episode 11 avg reward=93.76977276649355\n",
      "Episode 12\n",
      "Number visited 374\n",
      "Episode 12 exploratrion 0.19840000000000002\n",
      "Episode 12 total reward=67.11\n",
      "Episode 12 reward=93.39411052374513\n",
      "Episode 12 avg reward=93.39411052374513\n",
      "Episode 13\n",
      "Number visited 877\n",
      "Episode 13 exploratrion 0.19826666666666667\n",
      "Episode 13 total reward=69.86\n",
      "Episode 13 reward=94.63160278016137\n",
      "Episode 13 avg reward=94.63160278016137\n",
      "Episode 14\n",
      "Number visited 534\n",
      "Episode 14 exploratrion 0.19813333333333336\n",
      "Episode 14 total reward=72.18\n",
      "Episode 14 reward=93.04995707136658\n",
      "Episode 14 avg reward=93.04995707136658\n",
      "Episode 15\n",
      "Number visited 258\n",
      "Episode 15 exploratrion 0.198\n",
      "Episode 15 total reward=74.23\n",
      "Episode 15 reward=92.67216816618934\n",
      "Episode 15 avg reward=92.67216816618934\n",
      "Episode 16\n",
      "Number visited 777\n",
      "Episode 16 exploratrion 0.1978666666666667\n",
      "Episode 16 total reward=76.23\n",
      "Episode 16 reward=94.2549878176733\n",
      "Episode 16 avg reward=94.2549878176733\n",
      "Episode 17\n",
      "Number visited 829\n",
      "Episode 17 exploratrion 0.19773333333333334\n",
      "Episode 17 total reward=78.04\n",
      "Episode 17 reward=94.35698219791554\n",
      "Episode 17 avg reward=94.35698219791554\n",
      "Episode 18\n",
      "Number visited 301\n",
      "Episode 18 exploratrion 0.1976\n",
      "Episode 18 total reward=79.54\n",
      "Episode 18 reward=93.05525791671461\n",
      "Episode 18 avg reward=93.05525791671461\n",
      "Episode 19\n",
      "Number visited 90\n",
      "Episode 19 exploratrion 0.19746666666666668\n",
      "Episode 19 total reward=80.78\n",
      "Episode 19 reward=91.89331323993206\n",
      "Episode 19 avg reward=91.89331323993206\n",
      "Episode 20\n",
      "Number visited 358\n",
      "Episode 20 exploratrion 0.19733333333333333\n",
      "Episode 20 total reward=82.04\n",
      "Episode 20 reward=93.38287888434482\n",
      "Episode 20 avg reward=93.38287888434482\n",
      "Episode 21\n",
      "Number visited 528\n",
      "Episode 21 exploratrion 0.19720000000000001\n",
      "Episode 21 total reward=83.21\n",
      "Episode 21 reward=93.75872086941636\n",
      "Episode 21 avg reward=93.75872086941636\n",
      "Episode 22\n",
      "Number visited 1111\n",
      "Episode 22 exploratrion 0.19706666666666667\n",
      "Episode 22 total reward=84.45\n",
      "Episode 22 reward=95.61148429354343\n",
      "Episode 22 avg reward=95.61148429354343\n",
      "Episode 23\n",
      "Number visited 342\n",
      "Episode 23 exploratrion 0.19693333333333335\n",
      "Episode 23 total reward=85.32\n",
      "Episode 23 reward=93.13686803976692\n",
      "Episode 23 avg reward=93.13686803976692\n",
      "Episode 24\n",
      "Number visited 270\n",
      "Episode 24 exploratrion 0.1968\n",
      "Episode 24 total reward=86.08\n",
      "Episode 24 reward=92.89923151315979\n",
      "Episode 24 avg reward=92.89923151315979\n",
      "Episode 25\n",
      "Number visited 752\n",
      "Episode 25 exploratrion 0.19666666666666668\n",
      "Episode 25 total reward=86.93\n",
      "Episode 25 reward=94.63586195471957\n",
      "Episode 25 avg reward=94.63586195471957\n",
      "Episode 26\n",
      "Number visited 1004\n",
      "Episode 26 exploratrion 0.19653333333333334\n",
      "Episode 26 total reward=87.73\n",
      "Episode 26 reward=94.8934847304297\n",
      "Episode 26 avg reward=94.8934847304297\n",
      "Episode 27\n",
      "Number visited 206\n",
      "Episode 27 exploratrion 0.19640000000000002\n",
      "Episode 27 total reward=88.20\n",
      "Episode 27 reward=92.40137303515436\n",
      "Episode 27 avg reward=92.40137303515436\n",
      "Episode 28\n",
      "Number visited 350\n",
      "Episode 28 exploratrion 0.19626666666666667\n",
      "Episode 28 total reward=88.71\n",
      "Episode 28 reward=93.31447446020897\n",
      "Episode 28 avg reward=93.31447446020897\n",
      "Episode 29\n",
      "Number visited 390\n",
      "Episode 29 exploratrion 0.19613333333333335\n",
      "Episode 29 total reward=89.19\n",
      "Episode 29 reward=93.54079928216959\n",
      "Episode 29 avg reward=93.54079928216959\n",
      "Episode 30\n",
      "Number visited 244\n",
      "Episode 30 exploratrion 0.196\n",
      "Episode 30 total reward=89.55\n",
      "Episode 30 reward=92.7860086771592\n",
      "Episode 30 avg reward=92.7860086771592\n",
      "Episode 31\n",
      "Number visited 252\n",
      "Episode 31 exploratrion 0.1958666666666667\n",
      "Episode 31 total reward=89.88\n",
      "Episode 31 reward=92.79481244858026\n",
      "Episode 31 avg reward=92.79481244858026\n",
      "Episode 32\n",
      "Number visited 471\n",
      "Episode 32 exploratrion 0.19573333333333334\n",
      "Episode 32 total reward=90.17\n",
      "Episode 32 reward=92.85088813666027\n",
      "Episode 32 avg reward=92.85088813666027\n",
      "Episode 33\n",
      "Number visited 527\n",
      "Episode 33 exploratrion 0.19560000000000002\n",
      "Episode 33 total reward=90.55\n",
      "Episode 33 reward=93.93602618716743\n",
      "Episode 33 avg reward=93.93602618716743\n",
      "Episode 34\n",
      "Number visited 192\n",
      "Episode 34 exploratrion 0.19546666666666668\n",
      "Episode 34 total reward=90.75\n",
      "Episode 34 reward=92.50800631941078\n",
      "Episode 34 avg reward=92.50800631941078\n",
      "Episode 35\n",
      "Number visited 325\n",
      "Episode 35 exploratrion 0.19533333333333333\n",
      "Episode 35 total reward=90.99\n",
      "Episode 35 reward=93.22244724408306\n",
      "Episode 35 avg reward=93.22244724408306\n",
      "Episode 36\n",
      "Number visited 586\n",
      "Episode 36 exploratrion 0.1952\n",
      "Episode 36 total reward=91.24\n",
      "Episode 36 reward=93.5123855056872\n",
      "Episode 36 avg reward=93.5123855056872\n",
      "Episode 37\n",
      "Number visited 317\n",
      "Episode 37 exploratrion 0.19506666666666667\n",
      "Episode 37 total reward=91.43\n",
      "Episode 37 reward=93.08380216216784\n",
      "Episode 37 avg reward=93.08380216216784\n",
      "Episode 38\n",
      "Number visited 256\n",
      "Episode 38 exploratrion 0.19493333333333335\n",
      "Episode 38 total reward=91.56\n",
      "Episode 38 reward=92.70017879144605\n",
      "Episode 38 avg reward=92.70017879144605\n",
      "Episode 39\n",
      "Number visited 673\n",
      "Episode 39 exploratrion 0.1948\n",
      "Episode 39 total reward=91.84\n",
      "Episode 39 reward=94.43217447480671\n",
      "Episode 39 avg reward=94.43217447480671\n",
      "Episode 40\n",
      "Number visited 950\n",
      "Episode 40 exploratrion 0.19466666666666668\n",
      "Episode 40 total reward=92.14\n",
      "Episode 40 reward=94.7983416418859\n",
      "Episode 40 avg reward=94.7983416418859\n",
      "Episode 41\n",
      "Number visited 397\n",
      "Episode 41 exploratrion 0.19453333333333334\n",
      "Episode 41 total reward=92.26\n",
      "Episode 41 reward=93.38016391888003\n",
      "Episode 41 avg reward=93.38016391888003\n",
      "Episode 42\n",
      "Number visited 719\n",
      "Episode 42 exploratrion 0.19440000000000002\n",
      "Episode 42 total reward=92.42\n",
      "Episode 42 reward=93.83403802535445\n",
      "Episode 42 avg reward=93.83403802535445\n",
      "Episode 43\n",
      "Number visited 648\n",
      "Episode 43 exploratrion 0.19426666666666667\n",
      "Episode 43 total reward=92.59\n",
      "Episode 43 reward=94.13480436164049\n",
      "Episode 43 avg reward=94.13480436164049\n",
      "Episode 44\n",
      "Number visited 524\n",
      "Episode 44 exploratrion 0.19413333333333335\n",
      "Episode 44 total reward=92.72\n",
      "Episode 44 reward=93.84452590315317\n",
      "Episode 44 avg reward=93.84452590315317\n",
      "Episode 45\n",
      "Number visited 532\n",
      "Episode 45 exploratrion 0.194\n",
      "Episode 45 total reward=92.81\n",
      "Episode 45 reward=93.65304935213247\n",
      "Episode 45 avg reward=93.65304935213247\n",
      "Episode 46\n",
      "Number visited 194\n",
      "Episode 46 exploratrion 0.1938666666666667\n",
      "Episode 46 total reward=92.78\n",
      "Episode 46 reward=92.47809427690584\n",
      "Episode 46 avg reward=92.47809427690584\n",
      "Episode 47\n",
      "Number visited 394\n",
      "Episode 47 exploratrion 0.19373333333333334\n",
      "Episode 47 total reward=92.85\n",
      "Episode 47 reward=93.55195760817385\n",
      "Episode 47 avg reward=93.55195760817385\n",
      "Episode 48\n",
      "Number visited 597\n",
      "Episode 48 exploratrion 0.19360000000000002\n",
      "Episode 48 total reward=92.91\n",
      "Episode 48 reward=93.44227625673386\n",
      "Episode 48 avg reward=93.44227625673386\n",
      "Episode 49\n",
      "Number visited 483\n",
      "Episode 49 exploratrion 0.19346666666666668\n",
      "Episode 49 total reward=92.99\n",
      "Episode 49 reward=93.64502605198044\n",
      "Episode 49 avg reward=93.64502605198044\n",
      "Episode 50\n",
      "Number visited 377\n",
      "Episode 50 exploratrion 0.19333333333333336\n",
      "Episode 50 total reward=93.03\n",
      "Episode 50 reward=93.42040777906946\n",
      "Episode 50 avg reward=93.42040777906946\n",
      "Episode 51\n",
      "Number visited 205\n",
      "Episode 51 exploratrion 0.1932\n",
      "Episode 51 total reward=92.97\n",
      "Episode 51 reward=92.45703857558073\n",
      "Episode 51 avg reward=92.45703857558073\n",
      "Episode 52\n",
      "Number visited 534\n",
      "Episode 52 exploratrion 0.19306666666666666\n",
      "Episode 52 total reward=93.06\n",
      "Episode 52 reward=93.80645208565723\n",
      "Episode 52 avg reward=93.80645208565723\n",
      "Episode 53\n",
      "Number visited 761\n",
      "Episode 53 exploratrion 0.19293333333333335\n",
      "Episode 53 total reward=93.23\n",
      "Episode 53 reward=94.77970916620902\n",
      "Episode 53 avg reward=94.77970916620902\n",
      "Episode 54\n",
      "Number visited 369\n",
      "Episode 54 exploratrion 0.1928\n",
      "Episode 54 total reward=93.17\n",
      "Episode 54 reward=92.68655791999085\n",
      "Episode 54 avg reward=92.68655791999085\n",
      "Episode 55\n",
      "Number visited 200\n",
      "Episode 55 exploratrion 0.19266666666666668\n",
      "Episode 55 total reward=93.11\n",
      "Episode 55 reward=92.53341105855566\n",
      "Episode 55 avg reward=92.53341105855566\n",
      "Episode 56\n",
      "Number visited 790\n",
      "Episode 56 exploratrion 0.19253333333333333\n",
      "Episode 56 total reward=93.26\n",
      "Episode 56 reward=94.62603411287078\n",
      "Episode 56 avg reward=94.62603411287078\n",
      "Episode 57\n",
      "Number visited 227\n",
      "Episode 57 exploratrion 0.19240000000000002\n",
      "Episode 57 total reward=93.18\n",
      "Episode 57 reward=92.47358662222315\n",
      "Episode 57 avg reward=92.47358662222315\n",
      "Episode 58\n",
      "Number visited 922\n",
      "Episode 58 exploratrion 0.19226666666666667\n",
      "Episode 58 total reward=93.35\n",
      "Episode 58 reward=94.85358825039172\n",
      "Episode 58 avg reward=94.85358825039172\n",
      "Episode 59\n",
      "Number visited 342\n",
      "Episode 59 exploratrion 0.19213333333333335\n",
      "Episode 59 total reward=93.34\n",
      "Episode 59 reward=93.27240267906987\n",
      "Episode 59 avg reward=93.27240267906987\n",
      "Episode 60\n",
      "Number visited 441\n",
      "Episode 60 exploratrion 0.192\n",
      "Episode 60 total reward=93.36\n",
      "Episode 60 reward=93.51603273618088\n",
      "Episode 60 avg reward=93.51603273618088\n",
      "Episode 61\n",
      "Number visited 333\n",
      "Episode 61 exploratrion 0.19186666666666669\n",
      "Episode 61 total reward=93.33\n",
      "Episode 61 reward=93.07822399412932\n",
      "Episode 61 avg reward=93.07822399412932\n",
      "Episode 62\n",
      "Number visited 943\n",
      "Episode 62 exploratrion 0.19173333333333334\n",
      "Episode 62 total reward=93.50\n",
      "Episode 62 reward=95.01396238962076\n",
      "Episode 62 avg reward=95.01396238962076\n",
      "Episode 63\n",
      "Number visited 845\n",
      "Episode 63 exploratrion 0.19160000000000002\n",
      "Episode 63 total reward=93.64\n",
      "Episode 63 reward=94.89973304520221\n",
      "Episode 63 avg reward=94.89973304520221\n",
      "Episode 64\n",
      "Number visited 1030\n",
      "Episode 64 exploratrion 0.19146666666666667\n",
      "Episode 64 total reward=93.76\n",
      "Episode 64 reward=94.84011670135878\n",
      "Episode 64 avg reward=94.84011670135878\n",
      "Episode 65\n",
      "Number visited 633\n",
      "Episode 65 exploratrion 0.19133333333333336\n",
      "Episode 65 total reward=93.77\n",
      "Episode 65 reward=93.83616347555129\n",
      "Episode 65 avg reward=93.83616347555129\n",
      "Episode 66\n",
      "Number visited 404\n",
      "Episode 66 exploratrion 0.1912\n",
      "Episode 66 total reward=93.75\n",
      "Episode 66 reward=93.5676259719614\n",
      "Episode 66 avg reward=93.5676259719614\n",
      "Episode 67\n",
      "Number visited 909\n",
      "Episode 67 exploratrion 0.1910666666666667\n",
      "Episode 67 total reward=93.85\n",
      "Episode 67 reward=94.78149494431247\n",
      "Episode 67 avg reward=94.78149494431247\n",
      "Episode 68\n",
      "Number visited 84\n",
      "Episode 68 exploratrion 0.19093333333333334\n",
      "Episode 68 total reward=93.65\n",
      "Episode 68 reward=91.83362842604507\n",
      "Episode 68 avg reward=91.83362842604507\n",
      "Episode 69\n",
      "Number visited 342\n",
      "Episode 69 exploratrion 0.19080000000000003\n",
      "Episode 69 total reward=93.60\n",
      "Episode 69 reward=93.19439080971952\n",
      "Episode 69 avg reward=93.19439080971952\n",
      "Episode 70\n",
      "Number visited 463\n",
      "Episode 70 exploratrion 0.19066666666666668\n",
      "Episode 70 total reward=93.63\n",
      "Episode 70 reward=93.8445223195401\n",
      "Episode 70 avg reward=93.8445223195401\n",
      "Episode 71\n",
      "Number visited 956\n",
      "Episode 71 exploratrion 0.19053333333333333\n",
      "Episode 71 total reward=93.75\n",
      "Episode 71 reward=94.84255517440671\n",
      "Episode 71 avg reward=94.84255517440671\n",
      "Episode 72\n",
      "Number visited 601\n",
      "Episode 72 exploratrion 0.1904\n",
      "Episode 72 total reward=93.78\n",
      "Episode 72 reward=94.06121081296358\n",
      "Episode 72 avg reward=94.06121081296358\n",
      "Episode 73\n",
      "Number visited 205\n",
      "Episode 73 exploratrion 0.19026666666666667\n",
      "Episode 73 total reward=93.63\n",
      "Episode 73 reward=92.31074504174292\n",
      "Episode 73 avg reward=92.31074504174292\n",
      "Episode 74\n",
      "Number visited 874\n",
      "Episode 74 exploratrion 0.19013333333333335\n",
      "Episode 74 total reward=93.75\n",
      "Episode 74 reward=94.80185823309075\n",
      "Episode 74 avg reward=94.80185823309075\n",
      "Episode 75\n",
      "Number visited 353\n",
      "Episode 75 exploratrion 0.19\n",
      "Episode 75 total reward=93.71\n",
      "Episode 75 reward=93.31353785741152\n",
      "Episode 75 avg reward=93.31353785741152\n",
      "Episode 76\n",
      "Number visited 1095\n",
      "Episode 76 exploratrion 0.18986666666666668\n",
      "Episode 76 total reward=93.85\n",
      "Episode 76 reward=95.11541679191258\n",
      "Episode 76 avg reward=95.11541679191258\n",
      "Episode 77\n",
      "Number visited 83\n",
      "Episode 77 exploratrion 0.18973333333333334\n",
      "Episode 77 total reward=93.64\n",
      "Episode 77 reward=91.81119064322984\n",
      "Episode 77 avg reward=91.81119064322984\n",
      "Episode 78\n",
      "Number visited 883\n",
      "Episode 78 exploratrion 0.18960000000000002\n",
      "Episode 78 total reward=93.79\n",
      "Episode 78 reward=95.15394748433812\n",
      "Episode 78 avg reward=95.15394748433812\n",
      "Episode 79\n",
      "Number visited 191\n",
      "Episode 79 exploratrion 0.18946666666666667\n",
      "Episode 79 total reward=93.64\n",
      "Episode 79 reward=92.28603531019213\n",
      "Episode 79 avg reward=92.28603531019213\n",
      "Episode 80\n",
      "Number visited 983\n",
      "Episode 80 exploratrion 0.18933333333333335\n",
      "Episode 80 total reward=93.74\n",
      "Episode 80 reward=94.57235875574354\n",
      "Episode 80 avg reward=94.57235875574354\n",
      "Episode 81\n",
      "Number visited 573\n",
      "Episode 81 exploratrion 0.1892\n",
      "Episode 81 total reward=93.74\n",
      "Episode 81 reward=93.7335892378558\n",
      "Episode 81 avg reward=93.7335892378558\n",
      "Episode 82\n",
      "Number visited 624\n",
      "Episode 82 exploratrion 0.1890666666666667\n",
      "Episode 82 total reward=93.78\n",
      "Episode 82 reward=94.15652547452981\n",
      "Episode 82 avg reward=94.15652547452981\n",
      "Episode 83\n",
      "Number visited 513\n",
      "Episode 83 exploratrion 0.18893333333333334\n",
      "Episode 83 total reward=93.79\n",
      "Episode 83 reward=93.88955342866544\n",
      "Episode 83 avg reward=93.88955342866544\n",
      "Episode 84\n",
      "Number visited 689\n",
      "Episode 84 exploratrion 0.18880000000000002\n",
      "Episode 84 total reward=93.86\n",
      "Episode 84 reward=94.49924705259777\n",
      "Episode 84 avg reward=94.49924705259777\n",
      "Episode 85\n",
      "Number visited 796\n",
      "Episode 85 exploratrion 0.18866666666666668\n",
      "Episode 85 total reward=93.94\n",
      "Episode 85 reward=94.65162576928233\n",
      "Episode 85 avg reward=94.65162576928233\n",
      "Episode 86\n",
      "Number visited 158\n",
      "Episode 86 exploratrion 0.18853333333333333\n",
      "Episode 86 total reward=93.74\n",
      "Episode 86 reward=91.96586332811046\n",
      "Episode 86 avg reward=91.96586332811046\n",
      "Episode 87\n",
      "Number visited 1063\n",
      "Episode 87 exploratrion 0.1884\n",
      "Episode 87 total reward=93.90\n",
      "Episode 87 reward=95.33951370911664\n",
      "Episode 87 avg reward=95.33951370911664\n",
      "Episode 88\n",
      "Number visited 140\n",
      "Episode 88 exploratrion 0.18826666666666667\n",
      "Episode 88 total reward=93.70\n",
      "Episode 88 reward=91.92013259294147\n",
      "Episode 88 avg reward=91.92013259294147\n",
      "Episode 89\n",
      "Number visited 598\n",
      "Episode 89 exploratrion 0.18813333333333335\n",
      "Episode 89 total reward=93.73\n",
      "Episode 89 reward=94.01216737146494\n",
      "Episode 89 avg reward=94.01216737146494\n",
      "Episode 90\n",
      "Number visited 728\n",
      "Episode 90 exploratrion 0.188\n",
      "Episode 90 total reward=93.82\n",
      "Episode 90 reward=94.5724767771625\n",
      "Episode 90 avg reward=94.5724767771625\n",
      "Episode 91\n",
      "Number visited 147\n",
      "Episode 91 exploratrion 0.18786666666666668\n",
      "Episode 91 total reward=93.64\n",
      "Episode 91 reward=92.01529322231066\n",
      "Episode 91 avg reward=92.01529322231066\n",
      "Episode 92\n",
      "Number visited 678\n",
      "Episode 92 exploratrion 0.18773333333333334\n",
      "Episode 92 total reward=93.74\n",
      "Episode 92 reward=94.70368097993051\n",
      "Episode 92 avg reward=94.70368097993051\n",
      "Episode 93\n",
      "Number visited 674\n",
      "Episode 93 exploratrion 0.18760000000000002\n",
      "Episode 93 total reward=93.81\n",
      "Episode 93 reward=94.39343986480628\n",
      "Episode 93 avg reward=94.39343986480628\n",
      "Episode 94\n",
      "Number visited 787\n",
      "Episode 94 exploratrion 0.18746666666666667\n",
      "Episode 94 total reward=93.89\n",
      "Episode 94 reward=94.65602166064404\n",
      "Episode 94 avg reward=94.65602166064404\n",
      "Episode 95\n",
      "Number visited 880\n",
      "Episode 95 exploratrion 0.18733333333333335\n",
      "Episode 95 total reward=93.95\n",
      "Episode 95 reward=94.4760258751971\n",
      "Episode 95 avg reward=94.4760258751971\n",
      "Episode 96\n",
      "Number visited 357\n",
      "Episode 96 exploratrion 0.1872\n",
      "Episode 96 total reward=93.88\n",
      "Episode 96 reward=93.26806281573674\n",
      "Episode 96 avg reward=93.26806281573674\n",
      "Episode 97\n",
      "Number visited 93\n",
      "Episode 97 exploratrion 0.1870666666666667\n",
      "Episode 97 total reward=93.68\n",
      "Episode 97 reward=91.85431696076884\n",
      "Episode 97 avg reward=91.85431696076884\n",
      "Episode 98\n",
      "Number visited 257\n",
      "Episode 98 exploratrion 0.18693333333333334\n",
      "Episode 98 total reward=93.60\n",
      "Episode 98 reward=92.85450199983659\n",
      "Episode 98 avg reward=92.85450199983659\n",
      "Episode 99\n",
      "Number visited 236\n",
      "Episode 99 exploratrion 0.18680000000000002\n",
      "Episode 99 total reward=93.49\n",
      "Episode 99 reward=92.52135363564545\n",
      "Episode 99 avg reward=92.52135363564545\n",
      "Episode 100\n",
      "Number visited 199\n",
      "Episode 100 exploratrion 0.18666666666666668\n",
      "Episode 100 total reward=93.38\n",
      "Episode 100 reward=92.35333167625102\n",
      "Episode 100 avg reward=92.35333167625102\n",
      "Episode 101\n",
      "Number visited 129\n",
      "Episode 101 exploratrion 0.18653333333333336\n",
      "Episode 101 total reward=93.25\n",
      "Episode 101 reward=92.07226281602665\n",
      "Episode 101 avg reward=92.07226281602665\n",
      "Episode 102\n",
      "Number visited 384\n",
      "Episode 102 exploratrion 0.1864\n",
      "Episode 102 total reward=93.30\n",
      "Episode 102 reward=93.82258384825239\n",
      "Episode 102 avg reward=93.82258384825239\n",
      "Episode 103\n",
      "Number visited 384\n",
      "Episode 103 exploratrion 0.1862666666666667\n",
      "Episode 103 total reward=93.36\n",
      "Episode 103 reward=93.83418963955107\n",
      "Episode 103 avg reward=93.83418963955107\n",
      "Episode 104\n",
      "Number visited 384\n",
      "Episode 104 exploratrion 0.18613333333333335\n",
      "Episode 104 total reward=93.41\n",
      "Episode 104 reward=93.86493525845667\n",
      "Episode 104 avg reward=93.86493525845667\n",
      "Episode 105\n",
      "Number visited 384\n",
      "Episode 105 exploratrion 0.186\n",
      "Episode 105 total reward=93.45\n",
      "Episode 105 reward=93.86573075343267\n",
      "Episode 105 avg reward=93.86573075343267\n",
      "Episode 106\n",
      "Number visited 384\n",
      "Episode 106 exploratrion 0.18586666666666668\n",
      "Episode 106 total reward=93.49\n",
      "Episode 106 reward=93.82141342007031\n",
      "Episode 106 avg reward=93.82141342007031\n",
      "Episode 107\n",
      "Number visited 384\n",
      "Episode 107 exploratrion 0.18573333333333333\n",
      "Episode 107 total reward=93.53\n",
      "Episode 107 reward=93.8665991320243\n",
      "Episode 107 avg reward=93.8665991320243\n",
      "Episode 108\n",
      "Number visited 384\n",
      "Episode 108 exploratrion 0.18560000000000001\n",
      "Episode 108 total reward=93.56\n",
      "Episode 108 reward=93.84929182540068\n",
      "Episode 108 avg reward=93.84929182540068\n",
      "Episode 109\n",
      "Number visited 384\n",
      "Episode 109 exploratrion 0.18546666666666667\n",
      "Episode 109 total reward=93.59\n",
      "Episode 109 reward=93.86408912522415\n",
      "Episode 109 avg reward=93.86408912522415\n",
      "Episode 110\n",
      "Number visited 244\n",
      "Episode 110 exploratrion 0.18533333333333335\n",
      "Episode 110 total reward=93.53\n",
      "Episode 110 reward=92.94208745942788\n",
      "Episode 110 avg reward=92.94208745942788\n",
      "Episode 111\n",
      "Number visited 384\n",
      "Episode 111 exploratrion 0.1852\n",
      "Episode 111 total reward=93.56\n",
      "Episode 111 reward=93.83166975385309\n",
      "Episode 111 avg reward=93.83166975385309\n",
      "Episode 112\n",
      "Number visited 304\n",
      "Episode 112 exploratrion 0.18506666666666668\n",
      "Episode 112 total reward=93.53\n",
      "Episode 112 reward=93.33238940864344\n",
      "Episode 112 avg reward=93.33238940864344\n",
      "Episode 113\n",
      "Number visited 348\n",
      "Episode 113 exploratrion 0.18493333333333334\n",
      "Episode 113 total reward=93.54\n",
      "Episode 113 reward=93.61453406612165\n",
      "Episode 113 avg reward=93.61453406612165\n",
      "Episode 114\n",
      "Number visited 344\n",
      "Episode 114 exploratrion 0.18480000000000002\n",
      "Episode 114 total reward=93.55\n",
      "Episode 114 reward=93.58312213025489\n",
      "Episode 114 avg reward=93.58312213025489\n",
      "Episode 115\n",
      "Number visited 338\n",
      "Episode 115 exploratrion 0.18466666666666667\n",
      "Episode 115 total reward=93.54\n",
      "Episode 115 reward=93.51929839834165\n",
      "Episode 115 avg reward=93.51929839834165\n",
      "Episode 116\n",
      "Number visited 360\n",
      "Episode 116 exploratrion 0.18453333333333335\n",
      "Episode 116 total reward=93.56\n",
      "Episode 116 reward=93.66008388304563\n",
      "Episode 116 avg reward=93.66008388304563\n",
      "Episode 117\n",
      "Number visited 324\n",
      "Episode 117 exploratrion 0.1844\n",
      "Episode 117 total reward=93.55\n",
      "Episode 117 reward=93.47036883381479\n",
      "Episode 117 avg reward=93.47036883381479\n",
      "Episode 118\n",
      "Number visited 296\n",
      "Episode 118 exploratrion 0.1842666666666667\n",
      "Episode 118 total reward=93.52\n",
      "Episode 118 reward=93.26282089696934\n",
      "Episode 118 avg reward=93.26282089696934\n",
      "Episode 119\n",
      "Number visited 228\n",
      "Episode 119 exploratrion 0.18413333333333334\n",
      "Episode 119 total reward=93.45\n",
      "Episode 119 reward=92.82995897152571\n",
      "Episode 119 avg reward=92.82995897152571\n",
      "Episode 120\n",
      "Number visited 272\n",
      "Episode 120 exploratrion 0.184\n",
      "Episode 120 total reward=93.42\n",
      "Episode 120 reward=93.14307416249852\n",
      "Episode 120 avg reward=93.14307416249852\n",
      "Episode 121\n",
      "Number visited 272\n",
      "Episode 121 exploratrion 0.18386666666666668\n",
      "Episode 121 total reward=93.39\n",
      "Episode 121 reward=93.12780332374379\n",
      "Episode 121 avg reward=93.12780332374379\n",
      "Episode 122\n",
      "Number visited 316\n",
      "Episode 122 exploratrion 0.18373333333333336\n",
      "Episode 122 total reward=93.39\n",
      "Episode 122 reward=93.42032179890629\n",
      "Episode 122 avg reward=93.42032179890629\n",
      "Episode 123\n",
      "Number visited 376\n",
      "Episode 123 exploratrion 0.1836\n",
      "Episode 123 total reward=93.43\n",
      "Episode 123 reward=93.7575721308067\n",
      "Episode 123 avg reward=93.7575721308067\n",
      "Episode 124\n",
      "Number visited 236\n",
      "Episode 124 exploratrion 0.18346666666666667\n",
      "Episode 124 total reward=93.37\n",
      "Episode 124 reward=92.87392957718724\n",
      "Episode 124 avg reward=92.87392957718724\n",
      "Episode 125\n",
      "Number visited 232\n",
      "Episode 125 exploratrion 0.18333333333333335\n",
      "Episode 125 total reward=93.32\n",
      "Episode 125 reward=92.85731659746736\n",
      "Episode 125 avg reward=92.85731659746736\n",
      "Episode 126\n",
      "Number visited 172\n",
      "Episode 126 exploratrion 0.1832\n",
      "Episode 126 total reward=93.24\n",
      "Episode 126 reward=92.49257533218798\n",
      "Episode 126 avg reward=92.49257533218798\n",
      "Episode 127\n",
      "Number visited 268\n",
      "Episode 127 exploratrion 0.18306666666666668\n",
      "Episode 127 total reward=93.23\n",
      "Episode 127 reward=93.0998099320103\n",
      "Episode 127 avg reward=93.0998099320103\n",
      "Episode 128\n",
      "Number visited 296\n",
      "Episode 128 exploratrion 0.18293333333333334\n",
      "Episode 128 total reward=93.23\n",
      "Episode 128 reward=93.27320657422963\n",
      "Episode 128 avg reward=93.27320657422963\n",
      "Episode 129\n",
      "Number visited 308\n",
      "Episode 129 exploratrion 0.18280000000000002\n",
      "Episode 129 total reward=93.24\n",
      "Episode 129 reward=93.34679501330226\n",
      "Episode 129 avg reward=93.34679501330226\n",
      "Episode 130\n",
      "Number visited 72\n",
      "Episode 130 exploratrion 0.18266666666666667\n",
      "Episode 130 total reward=93.10\n",
      "Episode 130 reward=91.8430512566806\n",
      "Episode 130 avg reward=91.8430512566806\n",
      "Episode 131\n",
      "Number visited 40\n",
      "Episode 131 exploratrion 0.18253333333333335\n",
      "Episode 131 total reward=92.95\n",
      "Episode 131 reward=91.6311377130863\n",
      "Episode 131 avg reward=91.6311377130863\n",
      "Episode 132\n",
      "Number visited 220\n",
      "Episode 132 exploratrion 0.1824\n",
      "Episode 132 total reward=92.94\n",
      "Episode 132 reward=92.78867431000812\n",
      "Episode 132 avg reward=92.78867431000812\n",
      "Episode 133\n",
      "Number visited 264\n",
      "Episode 133 exploratrion 0.1822666666666667\n",
      "Episode 133 total reward=92.95\n",
      "Episode 133 reward=93.07411626370069\n",
      "Episode 133 avg reward=93.07411626370069\n",
      "Episode 134\n",
      "Number visited 128\n",
      "Episode 134 exploratrion 0.18213333333333334\n",
      "Episode 134 total reward=92.88\n",
      "Episode 134 reward=92.20901931840116\n",
      "Episode 134 avg reward=92.20901931840116\n",
      "Episode 135\n",
      "Number visited 196\n",
      "Episode 135 exploratrion 0.18200000000000002\n",
      "Episode 135 total reward=92.85\n",
      "Episode 135 reward=92.64316346757117\n",
      "Episode 135 avg reward=92.64316346757117\n",
      "Episode 136\n",
      "Number visited 264\n",
      "Episode 136 exploratrion 0.18186666666666668\n",
      "Episode 136 total reward=92.88\n",
      "Episode 136 reward=93.06497266484106\n",
      "Episode 136 avg reward=93.06497266484106\n",
      "Episode 137\n",
      "Number visited 292\n",
      "Episode 137 exploratrion 0.18173333333333336\n",
      "Episode 137 total reward=92.91\n",
      "Episode 137 reward=93.23718124596766\n",
      "Episode 137 avg reward=93.23718124596766\n",
      "Episode 138\n",
      "Number visited 21\n",
      "Episode 138 exploratrion 0.1816\n",
      "Episode 138 total reward=92.77\n",
      "Episode 138 reward=91.50342044020424\n",
      "Episode 138 avg reward=91.50342044020424\n",
      "Episode 139\n",
      "Number visited 184\n",
      "Episode 139 exploratrion 0.18146666666666667\n",
      "Episode 139 total reward=92.75\n",
      "Episode 139 reward=92.56977955450753\n",
      "Episode 139 avg reward=92.56977955450753\n",
      "Episode 140\n",
      "Number visited 140\n",
      "Episode 140 exploratrion 0.18133333333333335\n",
      "Episode 140 total reward=92.70\n",
      "Episode 140 reward=92.28651719403635\n",
      "Episode 140 avg reward=92.28651719403635\n",
      "Episode 141\n",
      "Number visited 228\n",
      "Episode 141 exploratrion 0.1812\n",
      "Episode 141 total reward=92.72\n",
      "Episode 141 reward=92.84984817858074\n",
      "Episode 141 avg reward=92.84984817858074\n",
      "Episode 142\n",
      "Number visited 212\n",
      "Episode 142 exploratrion 0.18106666666666668\n",
      "Episode 142 total reward=92.72\n",
      "Episode 142 reward=92.74905033558868\n",
      "Episode 142 avg reward=92.74905033558868\n",
      "Episode 143\n",
      "Number visited 208\n",
      "Episode 143 exploratrion 0.18093333333333333\n",
      "Episode 143 total reward=92.72\n",
      "Episode 143 reward=92.70386174927256\n",
      "Episode 143 avg reward=92.70386174927256\n",
      "Episode 144\n",
      "Number visited 268\n",
      "Episode 144 exploratrion 0.18080000000000002\n",
      "Episode 144 total reward=92.76\n",
      "Episode 144 reward=93.0895001878641\n",
      "Episode 144 avg reward=93.0895001878641\n",
      "Episode 145\n",
      "Number visited 256\n",
      "Episode 145 exploratrion 0.18066666666666667\n",
      "Episode 145 total reward=92.78\n",
      "Episode 145 reward=93.02391234280978\n",
      "Episode 145 avg reward=93.02391234280978\n",
      "Episode 146\n",
      "Number visited 156\n",
      "Episode 146 exploratrion 0.18053333333333335\n",
      "Episode 146 total reward=92.74\n",
      "Episode 146 reward=92.37331276229533\n",
      "Episode 146 avg reward=92.37331276229533\n",
      "Episode 147\n",
      "Number visited 360\n",
      "Episode 147 exploratrion 0.1804\n",
      "Episode 147 total reward=92.84\n",
      "Episode 147 reward=93.6800153412758\n",
      "Episode 147 avg reward=93.6800153412758\n",
      "Episode 148\n",
      "Number visited 268\n",
      "Episode 148 exploratrion 0.1802666666666667\n",
      "Episode 148 total reward=92.86\n",
      "Episode 148 reward=93.05641470581088\n",
      "Episode 148 avg reward=93.05641470581088\n",
      "Episode 149\n",
      "Number visited 204\n",
      "Episode 149 exploratrion 0.18013333333333334\n",
      "Episode 149 total reward=92.84\n",
      "Episode 149 reward=92.69384145215386\n",
      "Episode 149 avg reward=92.69384145215386\n",
      "Episode 150\n",
      "Number visited 272\n",
      "Episode 150 exploratrion 0.18\n",
      "Episode 150 total reward=92.87\n",
      "Episode 150 reward=93.1297537594917\n",
      "Episode 150 avg reward=93.1297537594917\n",
      "Episode 151\n",
      "Number visited 280\n",
      "Episode 151 exploratrion 0.17986666666666667\n",
      "Episode 151 total reward=92.90\n",
      "Episode 151 reward=93.19243225364579\n",
      "Episode 151 avg reward=93.19243225364579\n",
      "Episode 152\n",
      "Number visited 224\n",
      "Episode 152 exploratrion 0.17973333333333336\n",
      "Episode 152 total reward=92.89\n",
      "Episode 152 reward=92.80634328648591\n",
      "Episode 152 avg reward=92.80634328648591\n",
      "Episode 153\n",
      "Number visited 316\n",
      "Episode 153 exploratrion 0.1796\n",
      "Episode 153 total reward=92.94\n",
      "Episode 153 reward=93.40103861166371\n",
      "Episode 153 avg reward=93.40103861166371\n",
      "Episode 154\n",
      "Number visited 316\n",
      "Episode 154 exploratrion 0.17946666666666666\n",
      "Episode 154 total reward=92.99\n",
      "Episode 154 reward=93.41585385700701\n",
      "Episode 154 avg reward=93.41585385700701\n",
      "Episode 155\n",
      "Number visited 360\n",
      "Episode 155 exploratrion 0.17933333333333334\n",
      "Episode 155 total reward=93.06\n",
      "Episode 155 reward=93.65720812532828\n",
      "Episode 155 avg reward=93.65720812532828\n",
      "Episode 156\n",
      "Number visited 200\n",
      "Episode 156 exploratrion 0.17920000000000003\n",
      "Episode 156 total reward=93.02\n",
      "Episode 156 reward=92.64014570325949\n",
      "Episode 156 avg reward=92.64014570325949\n",
      "Episode 157\n",
      "Number visited 324\n",
      "Episode 157 exploratrion 0.17906666666666668\n",
      "Episode 157 total reward=93.06\n",
      "Episode 157 reward=93.47036883381479\n",
      "Episode 157 avg reward=93.47036883381479\n",
      "Episode 158\n",
      "Number visited 348\n",
      "Episode 158 exploratrion 0.17893333333333333\n",
      "Episode 158 total reward=93.12\n",
      "Episode 158 reward=93.60745549917874\n",
      "Episode 158 avg reward=93.60745549917874\n",
      "Episode 159\n",
      "Number visited 348\n",
      "Episode 159 exploratrion 0.17880000000000001\n",
      "Episode 159 total reward=93.17\n",
      "Episode 159 reward=93.61698607515291\n",
      "Episode 159 avg reward=93.61698607515291\n",
      "Episode 160\n",
      "Number visited 220\n",
      "Episode 160 exploratrion 0.17866666666666667\n",
      "Episode 160 total reward=93.13\n",
      "Episode 160 reward=92.80161965119393\n",
      "Episode 160 avg reward=92.80161965119393\n",
      "Episode 161\n",
      "Number visited 360\n",
      "Episode 161 exploratrion 0.17853333333333335\n",
      "Episode 161 total reward=93.18\n",
      "Episode 161 reward=93.6810648589812\n",
      "Episode 161 avg reward=93.6810648589812\n",
      "Episode 162\n",
      "Number visited 212\n",
      "Episode 162 exploratrion 0.1784\n",
      "Episode 162 total reward=93.14\n",
      "Episode 162 reward=92.74691001555351\n",
      "Episode 162 avg reward=92.74691001555351\n",
      "Episode 163\n",
      "Number visited 338\n",
      "Episode 163 exploratrion 0.17826666666666668\n",
      "Episode 163 total reward=93.18\n",
      "Episode 163 reward=93.4941382456887\n",
      "Episode 163 avg reward=93.4941382456887\n",
      "Episode 164\n",
      "Number visited 239\n",
      "Episode 164 exploratrion 0.17813333333333334\n",
      "Episode 164 total reward=93.15\n",
      "Episode 164 reward=92.89319522898369\n",
      "Episode 164 avg reward=92.89319522898369\n",
      "Episode 165\n",
      "Number visited 276\n",
      "Episode 165 exploratrion 0.17800000000000002\n",
      "Episode 165 total reward=93.15\n",
      "Episode 165 reward=93.1686936420721\n",
      "Episode 165 avg reward=93.1686936420721\n",
      "Episode 166\n",
      "Number visited 248\n",
      "Episode 166 exploratrion 0.17786666666666667\n",
      "Episode 166 total reward=93.13\n",
      "Episode 166 reward=92.98289346962244\n",
      "Episode 166 avg reward=92.98289346962244\n",
      "Episode 167\n",
      "Number visited 364\n",
      "Episode 167 exploratrion 0.17773333333333335\n",
      "Episode 167 total reward=93.19\n",
      "Episode 167 reward=93.6750963648433\n",
      "Episode 167 avg reward=93.6750963648433\n",
      "Episode 168\n",
      "Number visited 252\n",
      "Episode 168 exploratrion 0.1776\n",
      "Episode 168 total reward=93.17\n",
      "Episode 168 reward=93.01243705295694\n",
      "Episode 168 avg reward=93.01243705295694\n",
      "Episode 169\n",
      "Number visited 220\n",
      "Episode 169 exploratrion 0.17746666666666666\n",
      "Episode 169 total reward=93.13\n",
      "Episode 169 reward=92.80474836649643\n",
      "Episode 169 avg reward=92.80474836649643\n",
      "Episode 170\n",
      "Number visited 340\n",
      "Episode 170 exploratrion 0.17733333333333334\n",
      "Episode 170 total reward=93.18\n",
      "Episode 170 reward=93.55816394799453\n",
      "Episode 170 avg reward=93.55816394799453\n",
      "Episode 171\n",
      "Number visited 372\n",
      "Episode 171 exploratrion 0.17720000000000002\n",
      "Episode 171 total reward=93.23\n",
      "Episode 171 reward=93.76116797512215\n",
      "Episode 171 avg reward=93.76116797512215\n",
      "Episode 172\n",
      "Number visited 332\n",
      "Episode 172 exploratrion 0.17706666666666668\n",
      "Episode 172 total reward=93.26\n",
      "Episode 172 reward=93.50618401929744\n",
      "Episode 172 avg reward=93.50618401929744\n",
      "Episode 173\n",
      "Number visited 284\n",
      "Episode 173 exploratrion 0.17693333333333333\n",
      "Episode 173 total reward=93.26\n",
      "Episode 173 reward=93.20144991999902\n",
      "Episode 173 avg reward=93.20144991999902\n",
      "Episode 174\n",
      "Number visited 328\n",
      "Episode 174 exploratrion 0.1768\n",
      "Episode 174 total reward=93.28\n",
      "Episode 174 reward=93.49319583611924\n",
      "Episode 174 avg reward=93.49319583611924\n",
      "Episode 175\n",
      "Number visited 336\n",
      "Episode 175 exploratrion 0.17666666666666667\n",
      "Episode 175 total reward=93.31\n",
      "Episode 175 reward=93.54212758400637\n",
      "Episode 175 avg reward=93.54212758400637\n",
      "Episode 176\n",
      "Number visited 284\n",
      "Episode 176 exploratrion 0.17653333333333335\n",
      "Episode 176 total reward=93.30\n",
      "Episode 176 reward=93.2151660326994\n",
      "Episode 176 avg reward=93.2151660326994\n",
      "Episode 177\n",
      "Number visited 200\n",
      "Episode 177 exploratrion 0.1764\n",
      "Episode 177 total reward=93.23\n",
      "Episode 177 reward=92.65749688066812\n",
      "Episode 177 avg reward=92.65749688066812\n",
      "Episode 178\n",
      "Number visited 360\n",
      "Episode 178 exploratrion 0.17626666666666668\n",
      "Episode 178 total reward=93.28\n",
      "Episode 178 reward=93.68989443107186\n",
      "Episode 178 avg reward=93.68989443107186\n",
      "Episode 179\n",
      "Number visited 236\n",
      "Episode 179 exploratrion 0.17613333333333334\n",
      "Episode 179 total reward=93.24\n",
      "Episode 179 reward=92.90434240246313\n",
      "Episode 179 avg reward=92.90434240246313\n",
      "Episode 180\n",
      "Number visited 360\n",
      "Episode 180 exploratrion 0.17600000000000002\n",
      "Episode 180 total reward=93.28\n",
      "Episode 180 reward=93.66008388304563\n",
      "Episode 180 avg reward=93.66008388304563\n",
      "Episode 181\n",
      "Number visited 364\n",
      "Episode 181 exploratrion 0.17586666666666667\n",
      "Episode 181 total reward=93.32\n",
      "Episode 181 reward=93.68591231796512\n",
      "Episode 181 avg reward=93.68591231796512\n",
      "Episode 182\n",
      "Number visited 316\n",
      "Episode 182 exploratrion 0.17573333333333335\n",
      "Episode 182 total reward=93.33\n",
      "Episode 182 reward=93.40463897949111\n",
      "Episode 182 avg reward=93.40463897949111\n",
      "Episode 183\n",
      "Number visited 224\n",
      "Episode 183 exploratrion 0.1756\n",
      "Episode 183 total reward=93.28\n",
      "Episode 183 reward=92.79901302369024\n",
      "Episode 183 avg reward=92.79901302369024\n",
      "Episode 184\n",
      "Number visited 150\n",
      "Episode 184 exploratrion 0.1754666666666667\n",
      "Episode 184 total reward=93.18\n",
      "Episode 184 reward=92.32097907424222\n",
      "Episode 184 avg reward=92.32097907424222\n",
      "Episode 185\n",
      "Number visited 312\n",
      "Episode 185 exploratrion 0.17533333333333334\n",
      "Episode 185 total reward=93.20\n",
      "Episode 185 reward=93.3536625355645\n",
      "Episode 185 avg reward=93.3536625355645\n",
      "Episode 186\n",
      "Number visited 364\n",
      "Episode 186 exploratrion 0.17520000000000002\n",
      "Episode 186 total reward=93.25\n",
      "Episode 186 reward=93.69990017178783\n",
      "Episode 186 avg reward=93.69990017178783\n",
      "Episode 187\n",
      "Number visited 316\n",
      "Episode 187 exploratrion 0.17506666666666668\n",
      "Episode 187 total reward=93.27\n",
      "Episode 187 reward=93.4200564267502\n",
      "Episode 187 avg reward=93.4200564267502\n",
      "Episode 188\n",
      "Number visited 320\n",
      "Episode 188 exploratrion 0.17493333333333333\n",
      "Episode 188 total reward=93.28\n",
      "Episode 188 reward=93.4452579029984\n",
      "Episode 188 avg reward=93.4452579029984\n",
      "Episode 189\n",
      "Number visited 212\n",
      "Episode 189 exploratrion 0.1748\n",
      "Episode 189 total reward=93.23\n",
      "Episode 189 reward=92.72716175781967\n",
      "Episode 189 avg reward=92.72716175781967\n",
      "Episode 190\n",
      "Number visited 200\n",
      "Episode 190 exploratrion 0.17466666666666666\n",
      "Episode 190 total reward=93.17\n",
      "Episode 190 reward=92.67107892498713\n",
      "Episode 190 avg reward=92.67107892498713\n",
      "Episode 191\n",
      "Number visited 284\n",
      "Episode 191 exploratrion 0.17453333333333335\n",
      "Episode 191 total reward=93.18\n",
      "Episode 191 reward=93.2161953864925\n",
      "Episode 191 avg reward=93.2161953864925\n",
      "Episode 192\n",
      "Number visited 364\n",
      "Episode 192 exploratrion 0.1744\n",
      "Episode 192 total reward=93.23\n",
      "Episode 192 reward=93.67182210153058\n",
      "Episode 192 avg reward=93.67182210153058\n",
      "Episode 193\n",
      "Number visited 240\n",
      "Episode 193 exploratrion 0.17426666666666668\n",
      "Episode 193 total reward=93.20\n",
      "Episode 193 reward=92.92956945529045\n",
      "Episode 193 avg reward=92.92956945529045\n",
      "Episode 194\n",
      "Number visited 244\n",
      "Episode 194 exploratrion 0.17413333333333333\n",
      "Episode 194 total reward=93.17\n",
      "Episode 194 reward=92.9619013072356\n",
      "Episode 194 avg reward=92.9619013072356\n",
      "Episode 195\n",
      "Number visited 252\n",
      "Episode 195 exploratrion 0.17400000000000002\n",
      "Episode 195 total reward=93.16\n",
      "Episode 195 reward=93.01436290540845\n",
      "Episode 195 avg reward=93.01436290540845\n",
      "Episode 196\n",
      "Number visited 320\n",
      "Episode 196 exploratrion 0.17386666666666667\n",
      "Episode 196 total reward=93.19\n",
      "Episode 196 reward=93.44473498405164\n",
      "Episode 196 avg reward=93.44473498405164\n",
      "Episode 197\n",
      "Number visited 292\n",
      "Episode 197 exploratrion 0.17373333333333335\n",
      "Episode 197 total reward=93.19\n",
      "Episode 197 reward=93.26896309254349\n",
      "Episode 197 avg reward=93.26896309254349\n",
      "Episode 198\n",
      "Number visited 200\n",
      "Episode 198 exploratrion 0.1736\n",
      "Episode 198 total reward=93.14\n",
      "Episode 198 reward=92.66722422378902\n",
      "Episode 198 avg reward=92.66722422378902\n",
      "Episode 199\n",
      "Number visited 240\n",
      "Episode 199 exploratrion 0.17346666666666669\n",
      "Episode 199 total reward=93.12\n",
      "Episode 199 reward=92.91683777714653\n",
      "Episode 199 avg reward=92.91683777714653\n",
      "Episode 200\n",
      "Number visited 288\n",
      "Episode 200 exploratrion 0.17333333333333334\n",
      "Episode 200 total reward=93.13\n",
      "Episode 200 reward=93.22964112990297\n",
      "Episode 200 avg reward=93.22964112990297\n",
      "Episode 201\n",
      "Number visited 332\n",
      "Episode 201 exploratrion 0.17320000000000002\n",
      "Episode 201 total reward=93.17\n",
      "Episode 201 reward=93.50140558898016\n",
      "Episode 201 avg reward=93.50140558898016\n",
      "Episode 202\n",
      "Number visited 336\n",
      "Episode 202 exploratrion 0.17306666666666667\n",
      "Episode 202 total reward=93.20\n",
      "Episode 202 reward=93.514307178011\n",
      "Episode 202 avg reward=93.514307178011\n",
      "Episode 203\n",
      "Number visited 228\n",
      "Episode 203 exploratrion 0.17293333333333333\n",
      "Episode 203 total reward=93.17\n",
      "Episode 203 reward=92.84581340426709\n",
      "Episode 203 avg reward=92.84581340426709\n",
      "Episode 204\n",
      "Number visited 344\n",
      "Episode 204 exploratrion 0.1728\n",
      "Episode 204 total reward=93.21\n",
      "Episode 204 reward=93.5927599418535\n",
      "Episode 204 avg reward=93.5927599418535\n",
      "Episode 205\n",
      "Number visited 252\n",
      "Episode 205 exploratrion 0.1726666666666667\n",
      "Episode 205 total reward=93.19\n",
      "Episode 205 reward=93.0145157114494\n",
      "Episode 205 avg reward=93.0145157114494\n",
      "Episode 206\n",
      "Number visited 324\n",
      "Episode 206 exploratrion 0.17253333333333334\n",
      "Episode 206 total reward=93.22\n",
      "Episode 206 reward=93.45809325668736\n",
      "Episode 206 avg reward=93.45809325668736\n",
      "Episode 207\n",
      "Number visited 368\n",
      "Episode 207 exploratrion 0.1724\n",
      "Episode 207 total reward=93.27\n",
      "Episode 207 reward=93.73537537124837\n",
      "Episode 207 avg reward=93.73537537124837\n",
      "Episode 208\n",
      "Number visited 368\n",
      "Episode 208 exploratrion 0.17226666666666668\n",
      "Episode 208 total reward=93.32\n",
      "Episode 208 reward=93.73537537124837\n",
      "Episode 208 avg reward=93.73537537124837\n",
      "Episode 209\n",
      "Number visited 304\n",
      "Episode 209 exploratrion 0.17213333333333333\n",
      "Episode 209 total reward=93.32\n",
      "Episode 209 reward=93.34402101721375\n",
      "Episode 209 avg reward=93.34402101721375\n",
      "Episode 210\n",
      "Number visited 224\n",
      "Episode 210 exploratrion 0.17200000000000001\n",
      "Episode 210 total reward=93.27\n",
      "Episode 210 reward=92.83169932943424\n",
      "Episode 210 avg reward=92.83169932943424\n",
      "Episode 211\n",
      "Number visited 340\n",
      "Episode 211 exploratrion 0.17186666666666667\n",
      "Episode 211 total reward=93.30\n",
      "Episode 211 reward=93.55456319487615\n",
      "Episode 211 avg reward=93.55456319487615\n",
      "Episode 212\n",
      "Number visited 328\n",
      "Episode 212 exploratrion 0.17173333333333335\n",
      "Episode 212 total reward=93.32\n",
      "Episode 212 reward=93.49460040631374\n",
      "Episode 212 avg reward=93.49460040631374\n",
      "Episode 213\n",
      "Number visited 356\n",
      "Episode 213 exploratrion 0.1716\n",
      "Episode 213 total reward=93.35\n",
      "Episode 213 reward=93.64295644248098\n",
      "Episode 213 avg reward=93.64295644248098\n",
      "Episode 214\n",
      "Number visited 372\n",
      "Episode 214 exploratrion 0.17146666666666668\n",
      "Episode 214 total reward=93.39\n",
      "Episode 214 reward=93.760470473179\n",
      "Episode 214 avg reward=93.760470473179\n",
      "Episode 215\n",
      "Number visited 316\n",
      "Episode 215 exploratrion 0.17133333333333334\n",
      "Episode 215 total reward=93.39\n",
      "Episode 215 reward=93.42042903453071\n",
      "Episode 215 avg reward=93.42042903453071\n",
      "Episode 216\n",
      "Number visited 312\n",
      "Episode 216 exploratrion 0.17120000000000002\n",
      "Episode 216 total reward=93.39\n",
      "Episode 216 reward=93.39132581397529\n",
      "Episode 216 avg reward=93.39132581397529\n",
      "Episode 217\n",
      "Number visited 380\n",
      "Episode 217 exploratrion 0.17106666666666667\n",
      "Episode 217 total reward=93.44\n",
      "Episode 217 reward=93.80757241118626\n",
      "Episode 217 avg reward=93.80757241118626\n",
      "Episode 218\n",
      "Number visited 200\n",
      "Episode 218 exploratrion 0.17093333333333333\n",
      "Episode 218 total reward=93.36\n",
      "Episode 218 reward=92.66240054376486\n",
      "Episode 218 avg reward=92.66240054376486\n",
      "Episode 219\n",
      "Number visited 312\n",
      "Episode 219 exploratrion 0.1708\n",
      "Episode 219 total reward=93.36\n",
      "Episode 219 reward=93.39577282417886\n",
      "Episode 219 avg reward=93.39577282417886\n",
      "Episode 220\n",
      "Number visited 320\n",
      "Episode 220 exploratrion 0.1706666666666667\n",
      "Episode 220 total reward=93.37\n",
      "Episode 220 reward=93.42104264123671\n",
      "Episode 220 avg reward=93.42104264123671\n",
      "Episode 221\n",
      "Number visited 368\n",
      "Episode 221 exploratrion 0.17053333333333334\n",
      "Episode 221 total reward=93.40\n",
      "Episode 221 reward=93.69905903579433\n",
      "Episode 221 avg reward=93.69905903579433\n",
      "Episode 222\n",
      "Number visited 376\n",
      "Episode 222 exploratrion 0.1704\n",
      "Episode 222 total reward=93.44\n",
      "Episode 222 reward=93.75578165496029\n",
      "Episode 222 avg reward=93.75578165496029\n",
      "Episode 223\n",
      "Number visited 244\n",
      "Episode 223 exploratrion 0.17026666666666668\n",
      "Episode 223 total reward=93.39\n",
      "Episode 223 reward=92.93270464518697\n",
      "Episode 223 avg reward=92.93270464518697\n",
      "Episode 224\n",
      "Number visited 256\n",
      "Episode 224 exploratrion 0.17013333333333333\n",
      "Episode 224 total reward=93.35\n",
      "Episode 224 reward=93.0262603238352\n",
      "Episode 224 avg reward=93.0262603238352\n",
      "Episode 225\n",
      "Number visited 316\n",
      "Episode 225 exploratrion 0.17\n",
      "Episode 225 total reward=93.35\n",
      "Episode 225 reward=93.38497556353768\n",
      "Episode 225 avg reward=93.38497556353768\n",
      "Episode 226\n",
      "Number visited 364\n",
      "Episode 226 exploratrion 0.16986666666666667\n",
      "Episode 226 total reward=93.39\n",
      "Episode 226 reward=93.70703236722477\n",
      "Episode 226 avg reward=93.70703236722477\n",
      "Episode 227\n",
      "Number visited 324\n",
      "Episode 227 exploratrion 0.16973333333333335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jacob\\Desktop\\nbavy\\navy research.ipynb Cell 2'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000001?line=280'>281</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m>\u001b[39m \u001b[39m100\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000001?line=281'>282</a>\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(step):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000001?line=282'>283</a>\u001b[0m         maddpg\u001b[39m.\u001b[39;49mupdate()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000001?line=284'>285</a>\u001b[0m     \u001b[39m# show reward\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000001?line=285'>286</a>\u001b[0m smoothed_total_reward \u001b[39m=\u001b[39m smoothed_total_reward \u001b[39m*\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m+\u001b[39m totalAvgReward \u001b[39m*\u001b[39m \u001b[39m0.1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\machin\\frame\\algorithms\\maddpg.py:636\u001b[0m, in \u001b[0;36mMADDPG.update\u001b[1;34m(self, update_value, update_policy, update_target, concatenate_samples)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mfor\u001b[39;00m a_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactors)):\n\u001b[0;32m    604\u001b[0m         args\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    605\u001b[0m             (\n\u001b[0;32m    606\u001b[0m                 batch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m             )\n\u001b[0;32m    635\u001b[0m         )\n\u001b[1;32m--> 636\u001b[0m all_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mstarmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_sub_policy, args)\n\u001b[0;32m    637\u001b[0m mean_loss \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor(all_loss)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    639\u001b[0m \u001b[39m# returns action value and policy loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\machin\\parallel\\pool.py:1063\u001b[0m, in \u001b[0;36mPool.starmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1062\u001b[0m     \u001b[39m# DOC INHERITED\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstarmap(\n\u001b[0;32m   1064\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_caller,\n\u001b[0;32m   1065\u001b[0m         proxy_dumper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_recursive, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_copy_tensor, func, iterable),\n\u001b[0;32m   1066\u001b[0m         chunksize,\n\u001b[0;32m   1067\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\machin\\parallel\\pool.py:459\u001b[0m, in \u001b[0;36mBasePool.starmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\n\u001b[0;32m    441\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    442\u001b[0m     func: Callable[[Any], Any],\n\u001b[0;32m    443\u001b[0m     iterable: Collection[Tuple],\n\u001b[0;32m    444\u001b[0m     chunksize: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    445\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m    446\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39m        A list of result from applying the function on each tuple in the iterable.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmap_caller, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\machin\\parallel\\pool.py:129\u001b[0m, in \u001b[0;36mAsyncResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    114\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m    Return the result when it arrives.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m        The result.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[0;32m    131\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\machin\\parallel\\pool.py:111\u001b[0m, in \u001b[0;36mAsyncResult.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[39m    Wait until the result is available or until timeout seconds pass.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39m        timeout: Timeout in seconds.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.1520.0_x64__qbz5n2kfra8p0\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from machin.frame.algorithms import MADDPG \n",
    "\n",
    "from machin.frame.algorithms import MADDPG\n",
    "from machin.utils.logging import default_logger as logger\n",
    "from copy import deepcopy\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import machin.model.nets as net\n",
    "#https://github.com/instadeepai/Mava\n",
    "#https://github.com/PHRABAL/Tennis-MADDPG-PER/blob/master/MADDPG_PER.ipynb\n",
    "#Same, playing around with the initialization, parameters, optimizers and normalization really helped Here is the article how I fought it:\n",
    "\n",
    "#TL,DR: critic init sould match the reward distro. Minmax values should be meaningful. Delayed update for actor is good, so is gradient clipping and weight decay. \\\n",
    "# Use Adamw/RAdam/Ranger as an optimizer.\n",
    "\n",
    "#https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011?source=activity---post_recommended\n",
    "\n",
    "# Important note:\n",
    "# In order to successfully run the environment, please git clone the project\n",
    "# then run:\n",
    "#    pip install -e ./test_lib/multiagent-particle-envs/\n",
    "# in project root directory\n",
    "\n",
    "\n",
    "def create_env(env_name):\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "    import multiagent.scenarios as scenarios\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(env_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    env = MultiAgentEnv(\n",
    "        world,\n",
    "        scenario.reset_world,\n",
    "        scenario.reward,\n",
    "        scenario.observation,\n",
    "        info_callback=None,\n",
    "        shared_viewer=False,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "# configurations\n",
    "#env = create_env(\"simple_spread\")\n",
    "env = Sea()#50,50)\n",
    "env.discrete_action_input = True\n",
    "\n",
    "import gym.spaces as spaces\n",
    "obs = spaces.Box(low=-2, high=2, shape=(50,50), dtype=np.int32)\n",
    "action_space = spaces.Box(low=-1, high=1, shape= (2,), dtype=np.int32)\n",
    "\n",
    "observe_dim = 50*50#env.observation_space[0].shape[0]\n",
    "action_num = 4#action_space[0].n\n",
    "max_episodes = 1500\n",
    "max_steps = 200\n",
    "# number of agents in env, fixed, do not change\n",
    "agent_num = 5\n",
    "solved_reward = -15\n",
    "solved_repeat = 5\n",
    "exploration = 0.2\n",
    "\n",
    "\n",
    "print(observe_dim)\n",
    "print(action_num)\n",
    "\n",
    "\n",
    "class weightConstraint(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,module):\n",
    "        if hasattr(module,'weight'):\n",
    "            w=module.weight.data\n",
    "            w=w.clamp(0.5,0.7)\n",
    "            module.weight.data=w\n",
    "\n",
    "# model definition\n",
    "class ActorDiscrete(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 4, 5).to(device=\"cuda\")\n",
    "        self.conv2 = nn.Conv2d(4, 4, 5).to(device=\"cuda\")\n",
    "        self.conv3 = nn.Conv2d(4, 2, 5).to(device=\"cuda\")\n",
    "        self.maxPool = nn.MaxPool2d(5)\n",
    "\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(98, 64).to(device=\"cuda\")\n",
    "        self.fc2 = nn.Linear(64, 64).to(device=\"cuda\")\n",
    "        self.fc3 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        #self.fc4 = nn.Linear(512, 128).to(device=\"cuda\")\n",
    "        #self.fc5 = nn.Linear(128, 64).to(device=\"cuda\")\n",
    "        #self.fc6 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        \n",
    "        temp = nn.Linear(32, action_dim)\n",
    "        temp.weight.data.fill_(3e-1)\n",
    "        self.fc7 =  temp.to(device=\"cuda\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        #state.to(device=\"cuda\")\n",
    "        #q = np.reshape(state, (50, 50))\n",
    "        \n",
    "\n",
    "        q = t.reshape(state, (-1, 1, 50, 50))\n",
    "        q = t.relu(self.conv1(q))\n",
    "        q = t.relu(self.conv2(q))\n",
    "        q = t.relu(self.conv3(q))\n",
    "\n",
    "        q = self.maxPool(q)\n",
    "        q = t.flatten(q, 1)\n",
    "\n",
    "\n",
    "        a = t.relu(self.fc1(q))\n",
    "        a = t.relu(self.fc2(a))\n",
    "        a = t.relu(self.fc3(a))\n",
    "        #a = t.relu(self.fc4(a))\n",
    "        #a = t.relu(self.fc5(a))\n",
    "        #a = t.relu(self.fc6(a))\n",
    "        a = t.softmax(self.fc7(a), dim=1)\n",
    "        return a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        # This critic implementation is shared by the prey(DDPG) and\n",
    "        # predators(MADDPG)\n",
    "        # Note: For MADDPG\n",
    "        #       state_dim is the dimension of all states from all agents.\n",
    "        #       action_dim is the dimension of all actions from all agents.\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(5, 7, 5).to(device=\"cuda\")\n",
    "        self.conv2 = nn.Conv2d(7, 3, 5).to(device=\"cuda\")\n",
    "        self.conv3 = nn.Conv2d(3, 3, 5).to(device=\"cuda\")\n",
    "        self.maxPool = nn.MaxPool2d(5)\n",
    "\n",
    "        self.fc1 = nn.Linear(167, 64).to(device=\"cuda\")\n",
    "\n",
    "\n",
    "        self.fc2 = nn.Linear(64, 64).to(device=\"cuda\")\n",
    "        self.fc3 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        #self.fc4 = nn.Linear(512, 128).to(device=\"cuda\")\n",
    "        #self.fc5 = nn.Linear(128, 64).to(device=\"cuda\")\n",
    "        #self.fc6 = nn.Linear(64, 32).to(device=\"cuda\")\n",
    "        temp = nn.Linear(32, 1)\n",
    "        temp.weight.data.fill_(3e-5)\n",
    "        self.fc7 =  temp.to(device=\"cuda\")\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \n",
    "        #state_action = state_action.cuda()\n",
    "\n",
    "        try:\n",
    "            states = t.reshape(state, (-1, 5,50,50))\n",
    "\n",
    "            q = t.relu(self.conv1(states))\n",
    "            q = t.relu(self.conv2(q))\n",
    "            q = t.relu(self.conv3(q))\n",
    "            q = self.maxPool(q)\n",
    "            \n",
    "            #q = self.flatten(states)\n",
    "            q= t.flatten(q, 1)\n",
    "            state_action = t.cat([q, action], 1)\n",
    "\n",
    "            q = t.relu(self.fc1(state_action))\n",
    "            q = t.relu(self.fc2(q))\n",
    "            q = t.relu(self.fc3(q))\n",
    "            #q = t.relu(self.fc4(q))\n",
    "            #q = t.relu(self.fc5(q))\n",
    "            #q = t.relu(self.fc6(q))\n",
    "            q = self.fc7(q)\n",
    "            #try clamping the reward\n",
    "            #q = t.clamp(q, -200, 200)\n",
    "            return q\n",
    "        except BaseException as ex:\n",
    "            print(\"AHHHHH \")\n",
    "            print(ex)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actor = ActorDiscrete(observe_dim, action_num)\n",
    "    critic = Critic(observe_dim * agent_num, action_num * agent_num)\n",
    "\n",
    "    \n",
    "    net.static_module_wrapper(actor, \"cuda\", \"cuda\")\n",
    "    net.static_module_wrapper(critic, \"cuda\", \"cuda\")\n",
    "    maddpg = MADDPG(\n",
    "        [deepcopy(actor) for _ in range(agent_num)],\n",
    "        [deepcopy(actor) for _ in range(agent_num)],\n",
    "        [deepcopy(critic) for _ in range(agent_num)],\n",
    "        [deepcopy(critic) for _ in range(agent_num)],\n",
    "        t.optim.Adam,\n",
    "        #t.optim.RAdam,\n",
    "        #t.optim.NAdam,\n",
    "        nn.MSELoss(reduction=\"mean\"),\n",
    "        critic_visible_actors=[list(range(agent_num))] * agent_num,\n",
    "        #replay_device=\"cuda\",\n",
    "        discount = 0.99,\n",
    "        replay_size=50000,\n",
    "        #actor_learning_rate=0.00005,\n",
    "        #critic_learning_rate=0.0005,\n",
    "        #update_rate=0.0005\n",
    "    )\n",
    "    \n",
    "\n",
    "    episode, step, reward_fulfilled = 0, 0, 0\n",
    "    smoothed_total_reward = 0\n",
    "\n",
    "    while episode < max_episodes:\n",
    "        episode += 1\n",
    "        total_reward = 0\n",
    "        terminal = False\n",
    "        step = 0\n",
    "        states = [\n",
    "            t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in env.reset()\n",
    "        ]\n",
    "        tmp_observations_list = [[] for _ in range(agent_num)]\n",
    "\n",
    "        while not terminal and step <= max_steps:\n",
    "            step += 1\n",
    "            with t.no_grad():\n",
    "                old_states = states\n",
    "                # agent model inference\n",
    "                results = maddpg.act_with_noise(\n",
    "                    [{\"state\": st} for st in states], mode = \"uniform\", noise_param=(0, exploration)\n",
    "                )\n",
    "                #actions = [int(r[0]) for r in results]\n",
    "                result = []\n",
    "                for action, *others in results:\n",
    "                    action = t.reshape(action, (1, 4))\n",
    "                    batch_size = action.shape[0]\n",
    "                    action_disc = t.argmax(action, dim=1).view(batch_size, 1)\n",
    "                    result.append((action_disc, action, *others))\n",
    "\n",
    "\n",
    "                actions = [int(r[0]) for r in result]\n",
    "\n",
    "                action_probs = [r[1] for r in result]\n",
    "                states, rewards, terminals, _ = env.step(actions)\n",
    "                states = [\n",
    "                    t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in states\n",
    "                ]\n",
    "                total_reward += float(sum(rewards))# / agent_num\n",
    "\n",
    "                for tmp_observations, ost, act, st, rew, term in zip(\n",
    "                    tmp_observations_list,\n",
    "                    old_states,\n",
    "                    action_probs,\n",
    "                    states,\n",
    "                    rewards,\n",
    "                    terminals,\n",
    "                ):\n",
    "                    tmp_observations.append(\n",
    "                        {\n",
    "                            \"state\": {\"state\": ost},\n",
    "                            \"action\": {\"action\": act},\n",
    "                            \"next_state\": {\"state\": st},\n",
    "                            \"reward\": float(rew),\n",
    "                            \"terminal\": term or step == max_steps,\n",
    "                        }\n",
    "                    )\n",
    "        maddpg.store_episodes(tmp_observations_list)\n",
    "        # total reward is divided by steps here, since:\n",
    "        # \"Agents are rewarded based on minimum agent distance\n",
    "        #  to each landmark, penalized for collisions\"\n",
    "        #total_reward /= \n",
    "        totalAvgReward = total_reward #/ 50 / 50#step\n",
    "        totalSpaceVisited = np.sum(env.seen)\n",
    "        exploration = -episode / max_episodes * 0.2 + 0.2\n",
    "        #maddpg.discount = episode / max_episodes *0.29 + 0.7\n",
    "        print(f\"Episode {episode}\")\n",
    "        print(f\"Number visited {totalSpaceVisited}\")\n",
    "        print(f\"Episode {episode} exploratrion {exploration}\")\n",
    "        # update, update more if episode is longer, else less\n",
    "        if episode > 100:\n",
    "            for _ in range(step):\n",
    "                maddpg.update()\n",
    "\n",
    "            # show reward\n",
    "        smoothed_total_reward = smoothed_total_reward * 0.9 + totalAvgReward * 0.1\n",
    "        print(f\"Episode {episode} total reward={smoothed_total_reward:.2f}\")\n",
    "        print(f\"Episode {episode} reward={total_reward}\")\n",
    "        print(f\"Episode {episode} avg reward={totalAvgReward}\")\n",
    "        \n",
    "        #if smoothed_total_reward > solved_reward and episode > 100:\n",
    "        #    reward_fulfilled += 1\n",
    "        #    if reward_fulfilled >= solved_repeat:\n",
    "        #        logger.info(\"Environment solved!\")\n",
    "        #        exit(0)\n",
    "        #else:\n",
    "        #    reward_fulfilled = 0\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n",
      "Episode 1 total reward=-0.79\n",
      "Episode 1 reward=-2806.0\n",
      "Episode 1 avg reward=-1.1223999999999998\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MAPPO https://github.com/marlbenchmark/on-policy\n",
    "\n",
    "#maddpg.load(\"ModelCheckpoints\", version=3)\n",
    "episode = 0\n",
    "while episode < 1:\n",
    "    episode += 1\n",
    "    total_reward = 0\n",
    "    terminal = False\n",
    "    step = 0\n",
    "    states = [\n",
    "        t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in env.reset()\n",
    "    ]\n",
    "    tmp_observations_list = [[] for _ in range(agent_num)]\n",
    "\n",
    "    while not terminal and step <= max_steps:\n",
    "        step += 1\n",
    "        with t.no_grad():\n",
    "            old_states = states\n",
    "            # agent model inference\n",
    "            results = maddpg.act_discrete(\n",
    "                [{\"state\": st} for st in states]\n",
    "            )\n",
    "            actions = [int(r[0]) for r in results]\n",
    "            action_probs = [r[1] for r in results]\n",
    "            states, rewards, terminals, _ = env.step(actions)\n",
    "            states = [\n",
    "                t.tensor(st, dtype=t.float32).view(1, observe_dim) for st in states\n",
    "            ]\n",
    "            total_reward += float(sum(rewards))# / agent_num\n",
    "            #plt.matshow(np.reshape(states[0], (50,50)) )\n",
    "            for tmp_observations, ost, act, st, rew, term in zip(\n",
    "                tmp_observations_list,\n",
    "                old_states,\n",
    "                action_probs,\n",
    "                states,\n",
    "                rewards,\n",
    "                terminals,\n",
    "            ):\n",
    "                tmp_observations.append(\n",
    "                    {\n",
    "                        \"state\": {\"state\": ost},\n",
    "                        \"action\": {\"action\": act},\n",
    "                        \"next_state\": {\"state\": st},\n",
    "                        \"reward\": float(rew),\n",
    "                        \"terminal\": term or step == max_steps,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    #maddpg.store_episodes(tmp_observations_list)\n",
    "    env.display()\n",
    "    plt.matshow(env.seen)\n",
    "    plt.matshow(np.reshape(states[1],(50,50)))\n",
    "    # total reward is divided by steps here, since:\n",
    "    # \"Agents are rewarded based on minimum agent distance\n",
    "    #  to each landmark, penalized for collisions\"\n",
    "    #total_reward /= \n",
    "    totalAvgReward = total_reward / 50 / 50#step\n",
    "    maddpg.discount = 0.99#episode / max_episodes *0.7 + 0.3\n",
    "    print(f\"Episode {episode}\")\n",
    "    # update, update more if episode is longer, else less\n",
    "    #if episode > 100:\n",
    "    #    for _ in range(step):\n",
    "    #        maddpg.update()\n",
    "\n",
    "        # show reward\n",
    "    smoothed_total_reward = smoothed_total_reward * 0.9 + totalAvgReward * 0.1\n",
    "    print(f\"Episode {episode} total reward={smoothed_total_reward:.2f}\")\n",
    "    print(f\"Episode {episode} reward={total_reward}\")\n",
    "    print(f\"Episode {episode} avg reward={totalAvgReward}\")\n",
    "    \n",
    "    #if smoothed_total_reward > solved_reward and episode > 100:\n",
    "    #    reward_fulfilled += 1\n",
    "    #    if reward_fulfilled >= solved_repeat:\n",
    "    #        logger.info(\"Environment solved!\")\n",
    "    #        exit(0)\n",
    "    #else:\n",
    "    #    reward_fulfilled = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from elegantrl.agents.AgentBase import AgentBase\n",
    "from elegantrl.agents.net import Actor, Critic\n",
    "from elegantrl.agents.AgentDDPG import AgentDDPG\n",
    "\n",
    "class AgentMADDPG(AgentBase):\n",
    "    \"\"\"\n",
    "    Bases: ``AgentBase``\n",
    "    Multi-Agent DDPG algorithm. “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive”. R Lowe. et al.. 2017.\n",
    "    :param net_dim[int]: the dimension of networks (the width of neural networks)\n",
    "    :param state_dim[int]: the dimension of state (the number of state vector)\n",
    "    :param action_dim[int]: the dimension of action (the number of discrete action)\n",
    "    :param learning_rate[float]: learning rate of optimizer\n",
    "    :param gamma[float]: learning rate of optimizer\n",
    "    :param n_agents[int]: number of agents\n",
    "    :param if_per_or_gae[bool]: PER (off-policy) or GAE (on-policy) for sparse reward\n",
    "    :param env_num[int]: the env number of VectorEnv. env_num == 1 means don't use VectorEnv\n",
    "    :param agent_id[int]: if the visible_gpu is '1,9,3,4', agent_id=1 means (1,9,4,3)[agent_id] == 9\n",
    "    \"\"\"\n",
    "\n",
    "    #def __init__(self):\n",
    "        \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net_dim,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        gpu_id,\n",
    "        args,\n",
    "        learning_rate=1e-4,\n",
    "        gamma=0.95,\n",
    "        n_agents=1,\n",
    "        if_use_per=False,\n",
    "        env_num=1,\n",
    "        agent_id=0,\n",
    "        \n",
    "    ):\n",
    "        #self.ClassAct = Actor\n",
    "        #self.ClassCri = Critic\n",
    "        #self.if_use_cri_target = True\n",
    "        #self.if_use_act_target = True\n",
    "        #super().__init__(net_dim=net_dim, state_dim=state_dim, action_dim=action_dim)\n",
    "        \n",
    "        self.__name__ = \"MADDPG\"\n",
    "        self.agents = [AgentDDPG(net_dim=net_dim, state_dim=state_dim,action_dim=action_dim, gpu_id=gpu_id) for i in range(n_agents)]\n",
    "        self.explore_env = self.explore_one_env\n",
    "        self.if_off_policy = True\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "        #for i in range(self.n_agents):\n",
    "        #    self.agents[i].init(\n",
    "        #        net_dim,\n",
    "        #        state_dim,\n",
    "        #        action_dim,\n",
    "        #        learning_rate=1e-4,\n",
    "        #        n_agents=self.n_agents,\n",
    "        #        if_use_per=False,\n",
    "        #        env_num=1,\n",
    "        #        agent_id=0,\n",
    "        #    )\n",
    "        self.n_states = state_dim\n",
    "        self.n_actions = action_dim\n",
    "\n",
    "        self.batch_size = net_dim\n",
    "        self.gamma = gamma\n",
    "        self.update_tau = 0\n",
    "        self.device = torch.device(\n",
    "            f\"cuda:{agent_id}\"\n",
    "            if (torch.cuda.is_available() and (agent_id >= 0))\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "    def update_agent(self, rewards, dones, actions, observations, next_obs, index):\n",
    "        \"\"\"\n",
    "        Update the single agent neural networks, called by update_net.\n",
    "        :param rewards: reward list of the sampled buffer\n",
    "        :param dones: done list of the sampled buffer\n",
    "        :param actions: action list of the sampled buffer\n",
    "        :param observations: observation list of the sampled buffer\n",
    "        :param next_obs: next_observation list of the sample buffer\n",
    "        :param index: ID of the agent\n",
    "        \"\"\"\n",
    "        curr_agent = self.agents[index]\n",
    "        curr_agent.cri_optim.zero_grad()\n",
    "        all_target_actions = []\n",
    "        for i in range(self.n_agents):\n",
    "            if i == index:\n",
    "                all_target_actions.append(curr_agent.act_target(next_obs[:, index]))\n",
    "            if i != index:\n",
    "                action = self.agents[i].act_target(next_obs[:, i])\n",
    "                all_target_actions.append(action)\n",
    "        action_target_all = (\n",
    "            torch.cat(all_target_actions, dim=1)\n",
    "            .to(self.device)\n",
    "            .reshape(actions.shape[0], actions.shape[1] * actions.shape[2])\n",
    "        )\n",
    "\n",
    "        target_value = rewards[:, index] + self.gamma * curr_agent.cri_target(\n",
    "            next_obs.reshape(next_obs.shape[0], next_obs.shape[1] * next_obs.shape[2]),\n",
    "            action_target_all,\n",
    "        ).detach().squeeze(dim=1)\n",
    "        actual_value = curr_agent.cri(\n",
    "            observations.reshape(\n",
    "                next_obs.shape[0], next_obs.shape[1] * next_obs.shape[2]\n",
    "            ),\n",
    "            actions.reshape(actions.shape[0], actions.shape[1] * actions.shape[2]),\n",
    "        ).squeeze(dim=1)\n",
    "        vf_loss = curr_agent.loss_td(actual_value, target_value.detach())\n",
    "        curr_agent.act_optim.zero_grad()\n",
    "        curr_pol_out = curr_agent.act(observations[:, index])\n",
    "        curr_pol_vf_in = curr_pol_out\n",
    "        all_pol_acs = []\n",
    "        for i in range(self.n_agents):\n",
    "            if i == index:\n",
    "                all_pol_acs.append(curr_pol_vf_in)\n",
    "            else:\n",
    "                all_pol_acs.append(actions[:, i])\n",
    "        pol_loss = -torch.mean(\n",
    "            curr_agent.cri(\n",
    "                observations.reshape(\n",
    "                    observations.shape[0], observations.shape[1] * observations.shape[2]\n",
    "                ),\n",
    "                torch.cat(all_pol_acs, dim=1)\n",
    "                .to(self.device)\n",
    "                .reshape(actions.shape[0], actions.shape[1] * actions.shape[2]),\n",
    "            )\n",
    "        )\n",
    "        curr_agent.act_optim.zero_grad()\n",
    "        pol_loss.backward()\n",
    "        curr_agent.act_optim.step()\n",
    "        curr_agent.cri_optim.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        curr_agent.cri_optim.step()\n",
    "\n",
    "    def update_net(self, buffer, batch_size, repeat_times, soft_update_tau):\n",
    "        \"\"\"\n",
    "        Update the neural networks by sampling batch data from ``ReplayBuffer``.\n",
    "        :param buffer: the ReplayBuffer instance that stores the trajectories.\n",
    "        :param batch_size: the size of batch data for Stochastic Gradient Descent (SGD).\n",
    "        :param repeat_times: the re-using times of each trajectory.\n",
    "        :param soft_update_tau: the soft update parameter.\n",
    "        \"\"\"\n",
    "        buffer.update_now_len()\n",
    "        self.batch_size = batch_size\n",
    "        self.update_tau = soft_update_tau\n",
    "        rewards, dones, actions, observations, next_obs = buffer.sample_batch(\n",
    "            self.batch_size\n",
    "        )\n",
    "        for index in range(self.n_agents):\n",
    "            self.update_agent(rewards, dones, actions, observations, next_obs, index)\n",
    "\n",
    "        for agent in self.agents:\n",
    "            self.soft_update(agent.cri_target, agent.cri, self.update_tau)\n",
    "            self.soft_update(agent.act_target, agent.act, self.update_tau)\n",
    "\n",
    "        return\n",
    "\n",
    "    def explore_one_env(self, env, target_step) -> list:\n",
    "        \"\"\"\n",
    "        Exploring the environment for target_step.\n",
    "        param env: the Environment instance to be explored.\n",
    "        param target_step: target steps to explore.\n",
    "        \"\"\"\n",
    "        traj_temp = []\n",
    "        k = 0\n",
    "        for _ in range(target_step):\n",
    "            k += 1\n",
    "            actions = []\n",
    "            for i in range(self.n_agents):\n",
    "                action = self.agents[i].select_actions(self.states[i])\n",
    "                actions.append(action)\n",
    "            # print(actions)\n",
    "            next_s, reward, done, _ = env.step(actions)\n",
    "            traj_temp.append((self.states, reward, done, actions))\n",
    "            global_done = all(done[i] is True for i in range(self.n_agents))\n",
    "            if global_done or k > 100:\n",
    "                state = env.reset()\n",
    "                k = 0\n",
    "            else:\n",
    "                state = next_s\n",
    "        self.states = state\n",
    "        return traj_temp\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        \"\"\"\n",
    "        Select continuous actions for exploration\n",
    "        :param state: states.shape==(n_agents,batch_size, state_dim, )\n",
    "        :return: actions.shape==(n_agents,batch_size, action_dim, ),  -1 < action < +1\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i in range(self.n_agents):\n",
    "            action = self.agents[i].select_actions(states[i])\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def save_or_load_agent(self, cwd, if_save):\n",
    "        \"\"\"\n",
    "        save or load training files for Agent\n",
    "        :param cwd: Current Working Directory. ElegantRL save training files in CWD.\n",
    "        :param if_save: True: save files. False: load files.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_agents):\n",
    "            self.agents[i].save_or_load_agent(cwd + \"/\" + str(i), if_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Arguments Remove cwd: ./Sea_MADDPG_0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jacob\\Desktop\\nbavy\\navy research.ipynb Cell 6'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000004?line=40'>41</a>\u001b[0m args\u001b[39m.\u001b[39mlearner_gpus \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000004?line=42'>43</a>\u001b[0m \u001b[39m#train_agent(args)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000004?line=43'>44</a>\u001b[0m \u001b[39m#evaluate_agent(args)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jacob/Desktop/nbavy/navy%20research.ipynb#ch0000004?line=45'>46</a>\u001b[0m train_and_evaluate(args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\elegantrl\\train\\run.py:22\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=18'>19</a>\u001b[0m \u001b[39m\"\"\"init\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=19'>20</a>\u001b[0m env \u001b[39m=\u001b[39m build_env(args\u001b[39m.\u001b[39menv, args\u001b[39m.\u001b[39menv_func, args\u001b[39m.\u001b[39menv_args)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=21'>22</a>\u001b[0m agent \u001b[39m=\u001b[39m init_agent(args, gpu_id, env)\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=22'>23</a>\u001b[0m buffer \u001b[39m=\u001b[39m init_buffer(args, gpu_id)\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=23'>24</a>\u001b[0m evaluator \u001b[39m=\u001b[39m init_evaluator(args, gpu_id)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\elegantrl\\train\\run.py:79\u001b[0m, in \u001b[0;36minit_agent\u001b[1;34m(args, gpu_id, env)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=76'>77</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=77'>78</a>\u001b[0m     states \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=78'>79</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(states, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=79'>80</a>\u001b[0m     \u001b[39massert\u001b[39;00m states\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (args\u001b[39m.\u001b[39menv_num, args\u001b[39m.\u001b[39mstate_dim)\n\u001b[0;32m     <a href='file:///c%3A/Users/Jacob/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0/LocalCache/local-packages/Python310/site-packages/elegantrl/train/run.py?line=80'>81</a>\u001b[0m agent\u001b[39m.\u001b[39mstates \u001b[39m=\u001b[39m states\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from elegantrl_helloworld.config import Arguments\n",
    "#from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "#from elegantrl_helloworld.env import get_gym_env_args\n",
    "#from elegantrl.agents import AgentMADDPG\n",
    "\n",
    "\n",
    "from elegantrl.train.run import train_and_evaluate\n",
    "\n",
    "from elegantrl.train.config import Arguments\n",
    "from elegantrl.train.run import *\n",
    "\n",
    "agent = AgentMADDPG#AgentMADDPG(net_dim = 2**7, state_dim=50*50, action_dim=4, n_agents=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = Sea()#50,50)\n",
    "\n",
    "env_func = Sea\n",
    "\n",
    "#env_args = get_gym_env_args(env, if_print=True)\n",
    "args = Arguments(agent, env=env)\n",
    "\n",
    "args.if_discrete = True\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** 0\n",
    "args.gamma = 0.99\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step // 2\n",
    "args.net_dim = 2 ** 7\n",
    "args.batch_size = 2 ** 7\n",
    "args.repeat_times = 2 ** 0\n",
    "args.explore_noise = 0.1\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 7\n",
    "args.eval_times = 2 ** 4\n",
    "args.break_step = int(4e5)\n",
    "args.learner_gpus = 0\n",
    "\n",
    "#train_agent(args)\n",
    "#evaluate_agent(args)\n",
    "\n",
    "train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2022-06-15 00:02:41,461] <WARNING>:default_logger:Save name for module \"{r}\" is not specified, module name is used.\u001b[0m\n",
      "\u001b[33m[2022-06-15 00:02:41,485] <WARNING>:default_logger:Save name for module \"{r}\" is not specified, module name is used.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "maddpg.save(\"ModelCheckpoints\", version = 3)\n",
    "#number 3 stopped at 875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAECCAYAAAD+eGJTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAALKUlEQVR4nO3dX6jehX3H8fdnSUw6i2iqhCyR6VBWcrEqC/7BXQydkNlSA5PRUkYuAoHRgWWFzm4wWthFvantxRiEKs1Fae1smSKF4dKUMiixabWdGrakQpkuGkXFtrA0ab+7OL/KMebkPJ7z/N33/YLD+f178vt6OLzP7/k9z4OpKiT19VuzHkDSbBkBqTkjIDVnBKTmjIDUnBGQmpt6BJLsSfKfSU4muW/a5x9FkoeSnE7yzLJtW5M8keTE8P2KWc64XJKrkxxJ8lySZ5PcO2yfy5mTbEnyZJIfDfN+dth+bZKjw+/Gw0kumfWs50uyIclTSR4f1ud+5tVMNQJJNgD/CPwpsAv4aJJd05xhRF8G9py37T7gcFVdDxwe1ufFOeCTVbULuAX4+PBzndeZzwC3V9UHgBuAPUluAe4HHqiq64DXgf2zG3FF9wLHl60vwswXNe0rgZuAk1X1fFX9EvgacPeUZ1hVVX0XeO28zXcDh4blQ8Deac50MVV1qqp+OCz/jKVf0h3M6cy15OfD6qbhq4DbgUeG7XMz728k2Ql8EPjSsB7mfOZRTDsCO4D/Xrb+wrBtEWyrqlPD8kvAtlkOs5Ik1wA3AkeZ45mHy+qngdPAE8BPgDeq6txwyDz+bnwB+BTw62H9fcz/zKvyxuAa1NJ7refu/dZJ3gt8A/hEVb25fN+8zVxVv6qqG4CdLF0hvn+2E11ckg8Bp6vqB7OeZdw2Tvl8LwJXL1vfOWxbBC8n2V5Vp5JsZ+kv2NxIsomlAHylqr45bJ7rmQGq6o0kR4BbgcuTbBz+ss7b78ZtwIeT3AVsAS4Dvsh8zzySaV8JfB+4frijegnwEeCxKc+wVo8B+4blfcCjM5zlbYbnpg8Cx6vq88t2zeXMSa5Kcvmw/B7gTpbuYxwB7hkOm5t5Aarq01W1s6quYen39ttV9THmeOaRVdVUv4C7gP9i6Tng3037/CPO+FXgFHCWped5+1l6/ncYOAH8G7B11nMum/ePWLrU/zHw9PB117zODPwB8NQw7zPA3w/bfw94EjgJ/DOwedazrjD/HwOPL9LMF/vK8B8iqSlvDErNGQGpOSMgNWcEpOaMgNTcTCKQ5MAszrseizbzos0Lzjwr64rAOj4WvIg/uEWbedHmBWeeiTVHYIE+FizpItbz2YG3PhYMkOQ3Hwt+bqUHXJLNtYVL2cJvc1m2LtS7lBZt5kWbF6Yz87krL131mF2/88o7tj33P1e9bX3jq78AFufn/L/8gl/WmVxo33oicKGPBd98sQds4VJuzh3rOKW0Pq/+2a2rHvPkZ/7pHdv+8DN/+bb1Kw9+b2wzTcPROrzivol/inC4cXIAlqopab6sJwIjfSy4qg4CB4GFuGySzv+r///del4dWOSPBUsarPlKoKrOJfkr4F+BDcBDVfXs2CaTNBXruidQVd8CvjWmWSTNgG8blpozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGrOCEjNrRqBJA8lOZ3kmWXbtiZ5IsmJ4fsVkx1T0qSMciXwZWDPedvuAw5X1fXA4WFd0gJaNQJV9V3gtfM23w0cGpYPAXvHO5akaVnrPYFtVXVqWH4J2DameSRN2bpvDFZVAbXS/iQHkhxLcuwsZ9Z7OkljttYIvJxkO8Dw/fRKB1bVwaraXVW7N7F5jaeTNClrjcBjwL5heR/w6HjGkTRto7xE+FXge8DvJ3khyX7gc8CdSU4AfzKsS1pAG1c7oKo+usKuO8Y8i6QZ8B2DUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAam5VSOQ5OokR5I8l+TZJPcO27cmeSLJieH7FZMfV9K4jXIlcA74ZFXtAm4BPp5kF3AfcLiqrgcOD+uSFsyqEaiqU1X1w2H5Z8BxYAdwN3BoOOwQsHdCM0qaoHd1TyDJNcCNwFFgW1WdGna9BGwb72iSpmHkCCR5L/AN4BNV9ebyfVVVQK3wuANJjiU5dpYz6xpW0viNFIEkm1gKwFeq6pvD5peTbB/2bwdOX+ixVXWwqnZX1e5NbB7HzJLGaJRXBwI8CByvqs8v2/UYsG9Y3gc8Ov7xJE3axhGOuQ34C+A/kjw9bPtb4HPA15PsB34K/PlEJpQ0UatGoKr+HcgKu+8Y7ziSps13DErNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1t2oEkmxJ8mSSHyV5Nslnh+3XJjma5GSSh5NcMvlxJY3bKFcCZ4Dbq+oDwA3AniS3APcDD1TVdcDrwP6JTSlpYlaNQC35+bC6afgq4HbgkWH7IWDvJAaUNFkj3RNIsiHJ08Bp4AngJ8AbVXVuOOQFYMdEJpQ0USNFoKp+VVU3ADuBm4D3j3qCJAeSHEty7Cxn1jalpIl5V68OVNUbwBHgVuDyJBuHXTuBF1d4zMGq2l1VuzexeT2zSpqAUV4duCrJ5cPye4A7geMsxeCe4bB9wKMTmlHSBG1c/RC2A4eSbGApGl+vqseTPAd8Lck/AE8BD05wTkkTsmoEqurHwI0X2P48S/cHJC0w3zEoNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJobOQJJNiR5Ksnjw/q1SY4mOZnk4SSXTG5MSZPybq4E7gWOL1u/H3igqq4DXgf2j3MwSdMxUgSS7AQ+CHxpWA9wO/DIcMghYO8E5pM0YaNeCXwB+BTw62H9fcAbVXVuWH8B2DHe0SRNw6oRSPIh4HRV/WAtJ0hyIMmxJMfOcmYt/4SkCdo4wjG3AR9OchewBbgM+CJweZKNw9XATuDFCz24qg4CBwEuy9Yay9SSxmbVK4Gq+nRV7ayqa4CPAN+uqo8BR4B7hsP2AY9ObEpJE7Oe9wn8DfDXSU6ydI/gwfGMJGmaRnk68Jaq+g7wnWH5eeCm8Y8kaZp8x6DUnBGQmjMCUnNGQGrOCEjNGQGpOSMgNWcEpOaMgNScEZCaMwJSc0ZAas4ISM0ZAak5IyA1ZwSk5oyA1JwRkJozAlJzRkBqzghIzRkBqTkjIDVnBKTmjIDUnBGQmjMCUnNGQGruXf0PSaUONu195R3bzv7LVTOYZDq8EpCaMwJSc0ZAas4ISM2lqqZ2ssuytW7OHVM7n6QlR+swb9ZrudA+rwSk5oyA1JwRkJqb6j2BJK8APwWuBF6d2onHY9FmXrR5wZkn6Xer6oLveJpqBN46aXKsqnZP/cTrsGgzL9q84Myz4tMBqTkjIDU3qwgcnNF512PRZl60ecGZZ2Im9wQkzQ+fDkjNGQGpOSMgNWcEpOaMgNTc/wFadSlfFxsTcAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.display()\n",
    "for i in range(len(states)):\n",
    "    plt.matshow( np.reshape(states[i], (50,50)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c7f822a9fda7ffd10530ef71b8007e75e5e59461c46d04a19e3026085bccd1b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
