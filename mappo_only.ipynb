{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21b4c50a388>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 345.6x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random as rand\n",
    "import numpy as np\n",
    "#rand.seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import gym\n",
    "from copy import deepcopy\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "def perlin(x, y):\n",
    "    # permutation table\n",
    "    p = np.arange(256, dtype=int)\n",
    "    np.random.shuffle(p)\n",
    "    p = np.stack([p, p]).flatten()\n",
    "    # coordinates of the top-left\n",
    "    xi, yi = x.astype(int), y.astype(int)\n",
    "    # internal coordinates\n",
    "    xf, yf = x - xi, y - yi\n",
    "    # fade factors\n",
    "    u, v = fade(xf), fade(yf)\n",
    "    # noise components\n",
    "    n00 = gradient(p[p[xi] + yi], xf, yf)\n",
    "    n01 = gradient(p[p[xi] + yi + 1], xf, yf - 1)\n",
    "    n11 = gradient(p[p[xi + 1] + yi + 1], xf - 1, yf - 1)\n",
    "    n10 = gradient(p[p[xi + 1] + yi], xf - 1, yf)\n",
    "    # combine noises\n",
    "    x1 = lerp(n00, n10, u)\n",
    "    x2 = lerp(n01, n11, u)  # FIX1: I was using n10 instead of n01\n",
    "    return lerp(x1, x2, v)  # FIX2: I also had to reverse x1 and x2 here\n",
    "\n",
    "def lerp(a, b, x):\n",
    "    \"linear interpolation\"\n",
    "    return a + x * (b - a)\n",
    "\n",
    "def fade(t):\n",
    "    \"6t^5 - 15t^4 + 10t^3\"\n",
    "    return 6 * t**5 - 15 * t**4 + 10 * t**3\n",
    "\n",
    "def gradient(h, x, y):\n",
    "    \"grad converts h to the right gradient vector and return the dot product with (x,y)\"\n",
    "    vectors = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]])\n",
    "    g = vectors[h % 4]\n",
    "    return g[:, :, 0] * x + g[:, :, 1] * y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate():\n",
    "    lin = np.linspace(0, 5, 50, endpoint=False)\n",
    "    x, y = np.meshgrid(lin, lin)  # FIX3: I thought I had to invert x and y here but it was a mistake\n",
    "    data = perlin(x, y)\n",
    "    data = data*7+1\n",
    "    data = np.round(data)\n",
    "    data[data <= 0] = 1\n",
    "    return data\n",
    "\n",
    "test = generate()\n",
    "plt.matshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Wireless sensor networm combined with autonomous drone swarm and communication reduction\n",
    "#https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391193&casa_token=wZ2spLDNZroAAAAA:YDmwxnfhCvPGV002JGv_1lSta5d7yBgcY3P0YYrw24wKr7-hJWuTdR5tTvuWe1Z4vZgFr-pgs8Y\n",
    "\n",
    "ViewRange = 2\n",
    "CommRange = 5#5\n",
    "AgentAmmount = 5\n",
    "\n",
    "#double distance = 2/3 as efficient transfer\n",
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.priority = rand.randint(1, 10)\n",
    "        self.size = rand.randint(100, 1000)\n",
    "\n",
    "\n",
    "\n",
    "#Vessel\n",
    "#Constraints: Bandwidth - Num of Chanels - communication distance\n",
    "#Objective: Energy Reduction - Task priority \n",
    "class Drone:\n",
    "    def __init__(self, x, y, viewRange, commRange, width, height, index, Sea):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.task = None\n",
    "        self.viewRange = viewRange\n",
    "        self.commRange = commRange\n",
    "        self.observation = [[0]*width]*height\n",
    "        self.id = index\n",
    "        #self.seen = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        self.punish = 0\n",
    "        self.totalReward = 0\n",
    "    def getView(self):\n",
    "        return None\n",
    "\n",
    "    def getObservation(self, Sea):\n",
    "        #Get view\n",
    "        #obs = np.array([[0]*Sea.width]*Sea.height)\n",
    "        reward = 0 \n",
    "        for i in range(self.y-self.viewRange, self.y+self.viewRange):\n",
    "            for j in range(self.x-self.viewRange, self.x+self.viewRange):\n",
    "                if i < 50 and i >= 0  and j < 50 and j >= 0:\n",
    "                    if Sea.board[i][j] == 0:\n",
    "                        self.obs[i][j] = Sea.priorityMap[i][j] + 8\n",
    "                    else:\n",
    "                        self.obs[i][j] = Sea.board[i][j]\n",
    "\n",
    "                    if Sea.seen[i][j] == 0:\n",
    "                        Sea.seen[i][j] = 1\n",
    "                        reward += 1\n",
    "                        self.totalReward += Sea.priorityMap[i][j]\n",
    "        #reward += self.punish\n",
    "        res = deepcopy(self.obs)\n",
    "        #make them seperate\n",
    "        res[self.y][self.x] = 3 + self.id\n",
    "        reward = float(self.totalReward)/Sea.maxReward\n",
    "        #reward = -1/(10*(reward - 1.1))\n",
    "        reward = reward**2\n",
    "        return res, reward\n",
    "\n",
    "    def move(self,x, y, see):\n",
    "        x = x + self.x\n",
    "        y = y + self.y\n",
    "\n",
    "        self.punish = 0\n",
    "\n",
    "        if (x < 50 and x >= 0 and y < 50 and y >= 0) and (see.board[y][x] == 0 or see.board[y][x] == 2 or see.board[y][x] == -2) :\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "        else:\n",
    "            #punishment\n",
    "            self.punish -= 4\n",
    "\n",
    "    \n",
    "    def addData(self, drone):\n",
    "        pass\n",
    "    def setData(self, obs):\n",
    "        self.observation = abs\n",
    "#Constraints: Bandwidth, Num of Chanels\n",
    "#Objective Explore the sea\n",
    "class Ship:\n",
    "    def __init__(self, x, y, bandwidth):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "\n",
    "#Actions move up down left right \n",
    "class Sea:\n",
    "\n",
    "    def __init__(self):#, width, height):\n",
    "        self.width = 50#width\n",
    "        self.height =50# height\n",
    "\n",
    "        self.observe_dim = 50*50#env.observation_space[0].shape[0]\n",
    "        self.action_num = 4\n",
    "        self.max_step = 200\n",
    "        self.step_count = 0 \n",
    "        self.state_dim = 50*50\n",
    "        self.action_dim = 4\n",
    "        self.target_return = 50*50\n",
    "        self.env_num = 500\n",
    "        self.if_discrete = True\n",
    "        self.discrete_action_input = True\n",
    "        self.discrete_action_space = True\n",
    "        self.env_name = \"Sea\"\n",
    "        self.reward_range = (-200*4, 50*50)\n",
    "        #self.objects = objects\n",
    "        #int array -2 = dead zone (ie no communication) -1 = object 0 = sea 1 = ship 2 = drone\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "\n",
    "\n",
    "        self.observation_space = []\n",
    "        self.action_space = []\n",
    "        self.shared_observation_space = []\n",
    "\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            self.action_space.append(spaces.Discrete(4, start=0))\n",
    "            self.observation_space.append(spaces.Box(low=-5, high=9, shape=(50*50,)))\n",
    "            \n",
    "        self.share_observation_space = [spaces.Box(low=-5, high=9, shape=(50*50*5,))]#[spaces.Discrete(50*50*5, start=-5)] \n",
    "\n",
    "        self.cmap = ListedColormap([ 'k', 'b'])\n",
    "\n",
    "    def AddShip(self, ship):\n",
    "        self.ship = ship\n",
    "        self.board[ship.y][ship.x] = 2\n",
    "        for i in range(ship.y - 2, ship.y+2):\n",
    "            for j in range(ship.x - 2, ship.x + 2):\n",
    "                if i >= 0 and j >= 0 and i < self.height and j < self.width and self.board[i][j] == -1:\n",
    "                    self.board[i][j] = 0\n",
    "\n",
    "    def display(self):\n",
    "        newBoard = np.copy(self.board)\n",
    "        if ( hasattr(self, 'ship')):\n",
    "            newBoard[self.ship.y][self.ship.x] = 2\n",
    "            #self.calculateDeadZone2(newBoard)\n",
    "            self.cmap = ListedColormap([ 'k',  'b', 'g', 'y', 'r'])\n",
    "\n",
    "        for drone in self.drones:\n",
    "            newBoard[drone.y][drone.x] = 3\n",
    "\n",
    "        plt.matshow(newBoard, cmap=self.cmap)\n",
    "\n",
    "    def interestMap(self):\n",
    "        interest = [[0]*self.width]*self.height\n",
    "        samples = np.random.multivariate_normal([-0.5, -0.5], [[1, 0],[0, 1]], 50)\n",
    "        huh  = np.reshape(samples, (10,10))\n",
    "        print(huh)\n",
    "        plt.close()\n",
    "        plt.matshow(huh)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.array( [ [0]*self.width]*self.height )\n",
    "        #for obj in objects:\n",
    "        #    self.board[obj.y][obj.x] = -1\n",
    "\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        rock = rand.randint(0, 30)\n",
    "        #        if(rock == 0):\n",
    "        #            self.board[i][j] = -1\n",
    "        self.priorityMap = generate()\n",
    "        self.maxReward = np.sum(self.priorityMap)\n",
    "        shipx = rand.randint(0, 49)\n",
    "        shipy = rand.randint(1, 49)\n",
    "        ship = Ship(shipx, shipy, 100)\n",
    "        self.AddShip(ship)\n",
    "        self.seen = np.array([[0]*50]*50)\n",
    "        self.drones = []\n",
    "        self.step_count = 0\n",
    "        for i in range(AgentAmmount):\n",
    "            self.drones.append(Drone(shipx, shipy-1,ViewRange, CommRange, self.width, self.height, i, self))\n",
    "        observations, rewards = self.getObservation()\n",
    "\n",
    "        return observations\n",
    "\n",
    "    def step(self, actions):\n",
    "        droneIdx= 0\n",
    "        for act in actions:\n",
    "            if int(act[0]) == 1:\n",
    "                self.drones[droneIdx].move(1,0, self)\n",
    "            elif int(act[1]) == 1:\n",
    "                self.drones[droneIdx].move(-1,0, self)\n",
    "            elif int(act[2]) == 1:\n",
    "                self.drones[droneIdx].move(0,1, self)\n",
    "            elif int(act[3]) == 1:\n",
    "                self.drones[droneIdx].move(0, -1, self)\n",
    "            else:\n",
    "                print(\"Im not meantr to thbe there\")\n",
    "            droneIdx+= 1\n",
    "                \n",
    "                \n",
    "        #count  = [0] * AgentAmmount\n",
    "        #for i in range(self.height):\n",
    "        #    for j in range(self.width):\n",
    "        #        for drone in self.drones:\n",
    "        #            if drone.seen[i][j] == 1:\n",
    "        #                count[drone.id] += 1\n",
    "                        \n",
    "        #reward = count / float(self.width*self.height)\n",
    "        #reward = [t/float(self.width*self.height) for t in count]\n",
    "\n",
    "\n",
    "        observations, rewards = self.getObservation()\n",
    "        done = False\n",
    "        if(self.step_count >= self.max_step):\n",
    "            done = True\n",
    "        return observations, rewards, [done]*AgentAmmount, None\n",
    "\n",
    "    def getObservation(self):\n",
    "        currentIndex = 1\n",
    "        droneConnection = [0]*AgentAmmount\n",
    "\n",
    "        for drone in self.drones:\n",
    "            \n",
    "            for connectDrone in self.drones:\n",
    "                if drone.id == connectDrone.id:\n",
    "                    continue\n",
    "                if (droneConnection[drone.id] == 0 or droneConnection[drone.id] != droneConnection[connectDrone.id]) \\\n",
    "                            and math.sqrt( (drone.x - connectDrone.x)**2 + (drone.y - connectDrone.y)**2 ) < CommRange:\n",
    "                    #do stuff\n",
    "                    if droneConnection[drone.id] == 0 and droneConnection[connectDrone.id] == 0:\n",
    "                        droneConnection[drone.id] = currentIndex\n",
    "                        droneConnection[connectDrone.id] = currentIndex\n",
    "                        currentIndex += 1\n",
    "                    elif droneConnection[drone.id] != 0 and droneConnection[connectDrone.id] != 0:\n",
    "                        swap = droneConnection[connectDrone.id]\n",
    "                        for i in droneConnection:\n",
    "                            if i == swap:\n",
    "                                i = droneConnection[drone.id]\n",
    "                    else:\n",
    "                        if(droneConnection[drone.id] == 0):\n",
    "                            droneConnection[drone.id] = droneConnection[connectDrone.id]\n",
    "                        else:\n",
    "                            droneConnection[connectDrone.id] = droneConnection[drone.id]\n",
    "\n",
    "        for t in range(len(droneConnection)):\n",
    "            if droneConnection[t] == 0:\n",
    "                droneConnection[t] = currentIndex\n",
    "                currentIndex += 1\n",
    "\n",
    "        obsDict = {}\n",
    "        rewardList = [0] * AgentAmmount\n",
    "        index = 0\n",
    "        for i in droneConnection:\n",
    "\n",
    "            values, reward = self.drones[index].getObservation(self)\n",
    "\n",
    "            if str(i) in obsDict:\n",
    "                values = np.array(values).flatten()\n",
    "                curr = obsDict[str(i)]\n",
    "                \n",
    "                for i in range(len(curr)):\n",
    "                    \n",
    "                    if(curr[i] == 1 and values[i] != 0):\n",
    "                        curr[i] = values[i]\n",
    "                    if(curr[i] == 0):\n",
    "                        curr[i] = values[i]\n",
    "\n",
    "            else:\n",
    "                obsDict[str(i)] = np.array(values).flatten()\n",
    "            \n",
    "            rewardList[index]= reward\n",
    "            index += 1\n",
    "\n",
    "        observations = []\n",
    "\n",
    "\n",
    "        for i in droneConnection:\n",
    "            observations.append(obsDict[str(i)])\n",
    "\n",
    "        for index, i in enumerate(observations):\n",
    "            t = deepcopy(i)\n",
    "            t[t == 3] = 1\n",
    "            t[ t == 4] = 1\n",
    "            t[t == 5 ] = 1\n",
    "            t[t == 6 ] = 1\n",
    "            t[ t == 7 ] = 1\n",
    "            t[ t == 8 ] = 1\n",
    "            self.drones[index].obs = np.reshape(t, (50,50))\n",
    "\n",
    "\n",
    "        return observations, rewardList\n",
    "class Object:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "\n",
    "see = Sea()#50, 50)\n",
    "#see.reset()\n",
    "#obs, reward, _, _ = see.step([[0,1], [1,0]])\n",
    "#print(obs[0])\n",
    "#plt.matshow( np.reshape(obs[0], (50,50)) )\n",
    "#plt.matshow( np.reshape(obs[1], (50,50)) )\n",
    "#see.display()\n",
    "\n",
    "#print(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.19 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>ckpt\\wandb\\run-20220624_215746-360ldir7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/17donj/test/runs/360ldir7\" target=\"_blank\">na-mappo_check_seed1_priorityZones_noNoise</a></strong> to <a href=\"https://wandb.ai/17donj/test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Priority Sum Of Visited 376.0\n",
      "Priority Total 3923.0\n",
      "Episode Reward: 0.2129993582166624\n",
      "Smoothed Avg Reward: 0.02129993582166624\n",
      "Visited: 309\n",
      "\n",
      " Scenario tewzt Algo na-mappo Exp check updates 0/781 episodes, total num timesteps 6400/5000000, FPS 160.\n",
      "\n",
      "eval average episode rewards of agent0: 0.056671466202044773\n",
      "eval average episode rewards of agent1: 0.7208111350821151\n",
      "eval average episode rewards of agent2: 0.026998106672114365\n",
      "eval average episode rewards of agent3: 0.5530169616924578\n",
      "eval average episode rewards of agent4: 0.027524071094258053\n",
      "Episode 1\n",
      "Priority Sum Of Visited 676.0\n",
      "Priority Total 4457.0\n",
      "Episode Reward: 0.352800919855973\n",
      "Smoothed Avg Reward: 0.05445003422509692\n",
      "Visited: 409\n",
      "Episode 2\n",
      "Priority Sum Of Visited 1190.0\n",
      "Priority Total 4329.0\n",
      "Episode Reward: 1.9999909286118571\n",
      "Smoothed Avg Reward: 0.24900412366377295\n",
      "Visited: 668\n",
      "Episode 3\n",
      "Priority Sum Of Visited 867.0\n",
      "Priority Total 4253.0\n",
      "Episode Reward: 0.942047961165876\n",
      "Smoothed Avg Reward: 0.31830850741398325\n",
      "Visited: 557\n",
      "Episode 4\n",
      "Priority Sum Of Visited 698.0\n",
      "Priority Total 4057.0\n",
      "Episode Reward: 0.5808996510108089\n",
      "Smoothed Avg Reward: 0.34456762177366584\n",
      "Visited: 412\n",
      "Episode 5\n",
      "Priority Sum Of Visited 477.0\n",
      "Priority Total 3972.0\n",
      "Episode Reward: 0.35735514918629796\n",
      "Smoothed Avg Reward: 0.34584637451492906\n",
      "Visited: 296\n",
      "Episode 6\n",
      "Priority Sum Of Visited 1034.0\n",
      "Priority Total 3974.0\n",
      "Episode Reward: 1.5376376365854654\n",
      "Smoothed Avg Reward: 0.46502550072198273\n",
      "Visited: 652\n",
      "Episode 7\n",
      "Priority Sum Of Visited 1249.0\n",
      "Priority Total 4212.0\n",
      "Episode Reward: 1.6759201201469314\n",
      "Smoothed Avg Reward: 0.5861149626644776\n",
      "Visited: 760\n",
      "Episode 8\n",
      "Priority Sum Of Visited 1380.0\n",
      "Priority Total 3970.0\n",
      "Episode Reward: 2.9729939280117255\n",
      "Smoothed Avg Reward: 0.8248028591992024\n",
      "Visited: 912\n",
      "Episode 9\n",
      "Priority Sum Of Visited 1145.0\n",
      "Priority Total 4163.0\n",
      "Episode Reward: 1.6811418597969863\n",
      "Smoothed Avg Reward: 0.9104367592589808\n",
      "Visited: 637\n",
      "Episode 10\n",
      "Priority Sum Of Visited 1566.0\n",
      "Priority Total 4110.0\n",
      "Episode Reward: 3.658475500381835\n",
      "Smoothed Avg Reward: 1.1852406333712664\n",
      "Visited: 909\n",
      "Episode 11\n",
      "Priority Sum Of Visited 1423.0\n",
      "Priority Total 4172.0\n",
      "Episode Reward: 1.9457889491096654\n",
      "Smoothed Avg Reward: 1.2612954649451065\n",
      "Visited: 793\n",
      "Episode 12\n",
      "Priority Sum Of Visited 878.0\n",
      "Priority Total 4332.0\n",
      "Episode Reward: 1.261474018428001\n",
      "Smoothed Avg Reward: 1.261313320293396\n",
      "Visited: 558\n",
      "Episode 13\n",
      "Priority Sum Of Visited 1729.0\n",
      "Priority Total 4093.0\n",
      "Episode Reward: 3.4138063777256975\n",
      "Smoothed Avg Reward: 1.476562626036626\n",
      "Visited: 1064\n",
      "Episode 14\n",
      "Priority Sum Of Visited 836.0\n",
      "Priority Total 3885.0\n",
      "Episode Reward: 1.1065272001179334\n",
      "Smoothed Avg Reward: 1.4395590834447567\n",
      "Visited: 500\n",
      "Episode 15\n",
      "Priority Sum Of Visited 1205.0\n",
      "Priority Total 3918.0\n",
      "Episode Reward: 2.353001591325593\n",
      "Smoothed Avg Reward: 1.5309033342328404\n",
      "Visited: 712\n",
      "Episode 16\n",
      "Priority Sum Of Visited 942.0\n",
      "Priority Total 4196.0\n",
      "Episode Reward: 1.2335898458834542\n",
      "Smoothed Avg Reward: 1.5011719853979018\n",
      "Visited: 476\n",
      "Episode 17\n",
      "Priority Sum Of Visited 1250.0\n",
      "Priority Total 3885.0\n",
      "Episode Reward: 3.2418586484995737\n",
      "Smoothed Avg Reward: 1.675240651708069\n",
      "Visited: 729\n",
      "Episode 18\n",
      "Priority Sum Of Visited 1525.0\n",
      "Priority Total 4383.0\n",
      "Episode Reward: 2.71097949688322\n",
      "Smoothed Avg Reward: 1.778814536225584\n",
      "Visited: 908\n",
      "Episode 19\n",
      "Priority Sum Of Visited 1315.0\n",
      "Priority Total 4103.0\n",
      "Episode Reward: 2.521388290039882\n",
      "Smoothed Avg Reward: 1.8530719116070138\n",
      "Visited: 855\n",
      "Episode 20\n",
      "Priority Sum Of Visited 684.0\n",
      "Priority Total 4027.0\n",
      "Episode Reward: 1.3743918394393835\n",
      "Smoothed Avg Reward: 1.8052039043902508\n",
      "Visited: 429\n",
      "Episode 21\n",
      "Priority Sum Of Visited 1571.0\n",
      "Priority Total 3888.0\n",
      "Episode Reward: 3.989404125704923\n",
      "Smoothed Avg Reward: 2.0236239265217177\n",
      "Visited: 979\n",
      "Episode 22\n",
      "Priority Sum Of Visited 1587.0\n",
      "Priority Total 3863.0\n",
      "Episode Reward: 3.6242285865310917\n",
      "Smoothed Avg Reward: 2.1836843925226552\n",
      "Visited: 1014\n",
      "Episode 23\n",
      "Priority Sum Of Visited 1856.0\n",
      "Priority Total 4057.0\n",
      "Episode Reward: 4.732865333041626\n",
      "Smoothed Avg Reward: 2.438602486574552\n",
      "Visited: 991\n",
      "Episode 24\n",
      "Priority Sum Of Visited 1024.0\n",
      "Priority Total 4140.0\n",
      "Episode Reward: 1.6013437303087603\n",
      "Smoothed Avg Reward: 2.354876610947973\n",
      "Visited: 579\n",
      "Episode 25\n",
      "Priority Sum Of Visited 1491.0\n",
      "Priority Total 4325.0\n",
      "Episode Reward: 2.8034738213772594\n",
      "Smoothed Avg Reward: 2.3997363319909017\n",
      "Visited: 939\n",
      "Episode 26\n",
      "Priority Sum Of Visited 1345.0\n",
      "Priority Total 4139.0\n",
      "Episode Reward: 2.84904935235292\n",
      "Smoothed Avg Reward: 2.4446676340271036\n",
      "Visited: 821\n",
      "Episode 27\n",
      "Priority Sum Of Visited 1111.0\n",
      "Priority Total 3882.0\n",
      "Episode Reward: 2.618073986305438\n",
      "Smoothed Avg Reward: 2.462008269254937\n",
      "Visited: 795\n",
      "Episode 28\n",
      "Priority Sum Of Visited 1506.0\n",
      "Priority Total 4294.0\n",
      "Episode Reward: 2.1178181273075434\n",
      "Smoothed Avg Reward: 2.4275892550601976\n",
      "Visited: 906\n",
      "Episode 29\n",
      "Priority Sum Of Visited 1298.0\n",
      "Priority Total 4090.0\n",
      "Episode Reward: 3.2148998391927313\n",
      "Smoothed Avg Reward: 2.5063203134734513\n",
      "Visited: 806\n",
      "Episode 30\n",
      "Priority Sum Of Visited 1258.0\n",
      "Priority Total 4344.0\n",
      "Episode Reward: 1.886713569912055\n",
      "Smoothed Avg Reward: 2.4443596391173115\n",
      "Visited: 748\n",
      "Episode 31\n",
      "Priority Sum Of Visited 1255.0\n",
      "Priority Total 4416.0\n",
      "Episode Reward: 1.73630540828148\n",
      "Smoothed Avg Reward: 2.373554216033728\n",
      "Visited: 618\n",
      "Episode 32\n",
      "Priority Sum Of Visited 1413.0\n",
      "Priority Total 4018.0\n",
      "Episode Reward: 4.218822045444581\n",
      "Smoothed Avg Reward: 2.5580809989748134\n",
      "Visited: 795\n",
      "Episode 33\n",
      "Priority Sum Of Visited 802.0\n",
      "Priority Total 3876.0\n",
      "Episode Reward: 1.249558421489285\n",
      "Smoothed Avg Reward: 2.4272287412262603\n",
      "Visited: 597\n",
      "Episode 34\n",
      "Priority Sum Of Visited 887.0\n",
      "Priority Total 4212.0\n",
      "Episode Reward: 1.0538492765661163\n",
      "Smoothed Avg Reward: 2.289890794760246\n",
      "Visited: 629\n",
      "Episode 35\n",
      "Priority Sum Of Visited 1601.0\n",
      "Priority Total 4277.0\n",
      "Episode Reward: 2.3710107988808007\n",
      "Smoothed Avg Reward: 2.2980027951723017\n",
      "Visited: 1045\n",
      "Episode 36\n",
      "Priority Sum Of Visited 1397.0\n",
      "Priority Total 4082.0\n",
      "Episode Reward: 3.494208569979313\n",
      "Smoothed Avg Reward: 2.4176233726530025\n",
      "Visited: 857\n",
      "Episode 37\n",
      "Priority Sum Of Visited 1430.0\n",
      "Priority Total 4297.0\n",
      "Episode Reward: 2.6660625429445712\n",
      "Smoothed Avg Reward: 2.4424672896821593\n",
      "Visited: 801\n",
      "Episode 38\n",
      "Priority Sum Of Visited 1521.0\n",
      "Priority Total 4288.0\n",
      "Episode Reward: 3.920959459059643\n",
      "Smoothed Avg Reward: 2.5903165066199074\n",
      "Visited: 837\n",
      "Episode 39\n",
      "Priority Sum Of Visited 1261.0\n",
      "Priority Total 4005.0\n",
      "Episode Reward: 2.984219164246938\n",
      "Smoothed Avg Reward: 2.6297067723826104\n",
      "Visited: 679\n",
      "Episode 40\n",
      "Priority Sum Of Visited 1468.0\n",
      "Priority Total 4089.0\n",
      "Episode Reward: 2.7517833965842318\n",
      "Smoothed Avg Reward: 2.6419144348027723\n",
      "Visited: 858\n",
      "Episode 41\n",
      "Priority Sum Of Visited 1604.0\n",
      "Priority Total 3977.0\n",
      "Episode Reward: 3.5439100449915437\n",
      "Smoothed Avg Reward: 2.7321139958216496\n",
      "Visited: 1062\n",
      "Episode 42\n",
      "Priority Sum Of Visited 1599.0\n",
      "Priority Total 4023.0\n",
      "Episode Reward: 3.5032640739807746\n",
      "Smoothed Avg Reward: 2.809229003637562\n",
      "Visited: 998\n",
      "Episode 43\n",
      "Priority Sum Of Visited 842.0\n",
      "Priority Total 3857.0\n",
      "Episode Reward: 1.0196948209885306\n",
      "Smoothed Avg Reward: 2.630275585372659\n",
      "Visited: 542\n",
      "Episode 44\n",
      "Priority Sum Of Visited 1262.0\n",
      "Priority Total 4085.0\n",
      "Episode Reward: 2.11702053516987\n",
      "Smoothed Avg Reward: 2.57895008035238\n",
      "Visited: 785\n",
      "Episode 45\n",
      "Priority Sum Of Visited 935.0\n",
      "Priority Total 3957.0\n",
      "Episode Reward: 1.675169430999111\n",
      "Smoothed Avg Reward: 2.4885720154170534\n",
      "Visited: 628\n",
      "Episode 46\n",
      "Priority Sum Of Visited 1424.0\n",
      "Priority Total 3710.0\n",
      "Episode Reward: 4.183932694473309\n",
      "Smoothed Avg Reward: 2.658108083322679\n",
      "Visited: 951\n",
      "Episode 47\n",
      "Priority Sum Of Visited 1302.0\n",
      "Priority Total 4191.0\n",
      "Episode Reward: 2.4415570832978206\n",
      "Smoothed Avg Reward: 2.636452983320193\n",
      "Visited: 715\n",
      "Episode 48\n",
      "Priority Sum Of Visited 1489.0\n",
      "Priority Total 4086.0\n",
      "Episode Reward: 2.6504524960054816\n",
      "Smoothed Avg Reward: 2.6378529345887216\n",
      "Visited: 883\n",
      "Episode 49\n",
      "Priority Sum Of Visited 600.0\n",
      "Priority Total 4000.0\n",
      "Episode Reward: 0.7145759375\n",
      "Smoothed Avg Reward: 2.4455252348798493\n",
      "Visited: 437\n",
      "Episode 50\n",
      "Priority Sum Of Visited 1349.0\n",
      "Priority Total 4262.0\n",
      "Episode Reward: 2.62963215794375\n",
      "Smoothed Avg Reward: 2.463935927186239\n",
      "Visited: 705\n",
      "\n",
      " Scenario tewzt Algo na-mappo Exp check updates 50/781 episodes, total num timesteps 326400/5000000, FPS 168.\n",
      "\n",
      "eval average episode rewards of agent0: 0.634743793795217\n",
      "eval average episode rewards of agent1: 1.091687746801992\n",
      "eval average episode rewards of agent2: 0.0\n",
      "eval average episode rewards of agent3: 0.09420226136488039\n",
      "eval average episode rewards of agent4: 0.0\n",
      "Episode 51\n",
      "Priority Sum Of Visited 1416.0\n",
      "Priority Total 4119.0\n",
      "Episode Reward: 2.3320162410341387\n",
      "Smoothed Avg Reward: 2.4507439585710293\n",
      "Visited: 907\n",
      "Episode 52\n",
      "Priority Sum Of Visited 1091.0\n",
      "Priority Total 4198.0\n",
      "Episode Reward: 1.954694049958226\n",
      "Smoothed Avg Reward: 2.401138967709749\n",
      "Visited: 733\n",
      "Episode 53\n",
      "Priority Sum Of Visited 1190.0\n",
      "Priority Total 3995.0\n",
      "Episode Reward: 2.3328122606324238\n",
      "Smoothed Avg Reward: 2.3943062970020166\n",
      "Visited: 837\n",
      "Episode 54\n",
      "Priority Sum Of Visited 956.0\n",
      "Priority Total 4172.0\n",
      "Episode Reward: 1.0771740264503622\n",
      "Smoothed Avg Reward: 2.262593069946851\n",
      "Visited: 621\n",
      "Episode 55\n",
      "Priority Sum Of Visited 1545.0\n",
      "Priority Total 4013.0\n",
      "Episode Reward: 2.711202360084521\n",
      "Smoothed Avg Reward: 2.307453998960618\n",
      "Visited: 887\n",
      "Episode 56\n",
      "Priority Sum Of Visited 1419.0\n",
      "Priority Total 3895.0\n",
      "Episode Reward: 3.0416677185621928\n",
      "Smoothed Avg Reward: 2.3808753709207755\n",
      "Visited: 859\n",
      "Episode 57\n",
      "Priority Sum Of Visited 1480.0\n",
      "Priority Total 4233.0\n",
      "Episode Reward: 2.4218960303631683\n",
      "Smoothed Avg Reward: 2.3849774368650145\n",
      "Visited: 943\n",
      "Episode 58\n",
      "Priority Sum Of Visited 1585.0\n",
      "Priority Total 4137.0\n",
      "Episode Reward: 3.094479393791409\n",
      "Smoothed Avg Reward: 2.4559276325576542\n",
      "Visited: 947\n",
      "Episode 59\n",
      "Priority Sum Of Visited 1005.0\n",
      "Priority Total 4243.0\n",
      "Episode Reward: 1.6084131082462771\n",
      "Smoothed Avg Reward: 2.3711761801265165\n",
      "Visited: 577\n",
      "Episode 60\n",
      "Priority Sum Of Visited 1693.0\n",
      "Priority Total 4024.0\n",
      "Episode Reward: 3.4267694652166507\n",
      "Smoothed Avg Reward: 2.47673550863553\n",
      "Visited: 1057\n",
      "Episode 61\n",
      "Priority Sum Of Visited 1592.0\n",
      "Priority Total 4622.0\n",
      "Episode Reward: 3.237070425509965\n",
      "Smoothed Avg Reward: 2.5527690003229737\n",
      "Visited: 956\n",
      "Episode 62\n",
      "Priority Sum Of Visited 1112.0\n",
      "Priority Total 3971.0\n",
      "Episode Reward: 3.0979553919022966\n",
      "Smoothed Avg Reward: 2.607287639480906\n",
      "Visited: 664\n",
      "Episode 63\n",
      "Priority Sum Of Visited 1628.0\n",
      "Priority Total 4397.0\n",
      "Episode Reward: 3.0968972218275446\n",
      "Smoothed Avg Reward: 2.65624859771557\n",
      "Visited: 866\n",
      "Episode 64\n",
      "Priority Sum Of Visited 1602.0\n",
      "Priority Total 4008.0\n",
      "Episode Reward: 3.349907221485173\n",
      "Smoothed Avg Reward: 2.7256144600925305\n",
      "Visited: 972\n",
      "Episode 65\n",
      "Priority Sum Of Visited 711.0\n",
      "Priority Total 3838.0\n",
      "Episode Reward: 0.9789789632812607\n",
      "Smoothed Avg Reward: 2.5509509104114034\n",
      "Visited: 536\n",
      "Episode 66\n",
      "Priority Sum Of Visited 1158.0\n",
      "Priority Total 3929.0\n",
      "Episode Reward: 1.7981662418335227\n",
      "Smoothed Avg Reward: 2.4756724435536155\n",
      "Visited: 768\n",
      "Episode 67\n",
      "Priority Sum Of Visited 1443.0\n",
      "Priority Total 4007.0\n",
      "Episode Reward: 3.2259966321727083\n",
      "Smoothed Avg Reward: 2.550704862415525\n",
      "Visited: 926\n",
      "Episode 68\n",
      "Priority Sum Of Visited 1352.0\n",
      "Priority Total 4053.0\n",
      "Episode Reward: 2.908512541906344\n",
      "Smoothed Avg Reward: 2.586485630364607\n",
      "Visited: 947\n",
      "Episode 69\n",
      "Priority Sum Of Visited 1367.0\n",
      "Priority Total 4000.0\n",
      "Episode Reward: 3.2171685625\n",
      "Smoothed Avg Reward: 2.6495539235781465\n",
      "Visited: 810\n",
      "Episode 70\n",
      "Priority Sum Of Visited 1424.0\n",
      "Priority Total 4116.0\n",
      "Episode Reward: 3.047150787984224\n",
      "Smoothed Avg Reward: 2.689313610018754\n",
      "Visited: 829\n",
      "Episode 71\n",
      "Priority Sum Of Visited 1381.0\n",
      "Priority Total 4044.0\n",
      "Episode Reward: 3.1741323312014917\n",
      "Smoothed Avg Reward: 2.737795482137028\n",
      "Visited: 803\n",
      "Episode 72\n",
      "Priority Sum Of Visited 1521.0\n",
      "Priority Total 4009.0\n",
      "Episode Reward: 3.7949609014538956\n",
      "Smoothed Avg Reward: 2.8435120240687146\n",
      "Visited: 971\n",
      "Episode 73\n",
      "Priority Sum Of Visited 1136.0\n",
      "Priority Total 4036.0\n",
      "Episode Reward: 2.411501209137581\n",
      "Smoothed Avg Reward: 2.800310942575601\n",
      "Visited: 793\n",
      "Episode 74\n",
      "Priority Sum Of Visited 912.0\n",
      "Priority Total 4047.0\n",
      "Episode Reward: 1.9123046359953015\n",
      "Smoothed Avg Reward: 2.7115103119175714\n",
      "Visited: 614\n",
      "Episode 75\n",
      "Priority Sum Of Visited 1639.0\n",
      "Priority Total 4050.0\n",
      "Episode Reward: 3.8457087639079393\n",
      "Smoothed Avg Reward: 2.8249301571166083\n",
      "Visited: 993\n",
      "Episode 76\n",
      "Priority Sum Of Visited 1420.0\n",
      "Priority Total 4115.0\n",
      "Episode Reward: 4.580728420014494\n",
      "Smoothed Avg Reward: 3.000509983406397\n",
      "Visited: 827\n",
      "Episode 77\n",
      "Priority Sum Of Visited 1329.0\n",
      "Priority Total 4266.0\n",
      "Episode Reward: 2.650132844244959\n",
      "Smoothed Avg Reward: 2.9654722694902533\n",
      "Visited: 811\n",
      "Episode 78\n",
      "Priority Sum Of Visited 1316.0\n",
      "Priority Total 3609.0\n",
      "Episode Reward: 3.740178432340379\n",
      "Smoothed Avg Reward: 3.042942885775266\n",
      "Visited: 946\n",
      "Episode 79\n",
      "Priority Sum Of Visited 1424.0\n",
      "Priority Total 3982.0\n",
      "Episode Reward: 3.138655214159348\n",
      "Smoothed Avg Reward: 3.0525141186136744\n",
      "Visited: 922\n",
      "Episode 80\n",
      "Priority Sum Of Visited 980.0\n",
      "Priority Total 4050.0\n",
      "Episode Reward: 1.6490122237463793\n",
      "Smoothed Avg Reward: 2.9121639291269448\n",
      "Visited: 630\n",
      "Episode 81\n",
      "Priority Sum Of Visited 1254.0\n",
      "Priority Total 4220.0\n",
      "Episode Reward: 3.4313115720671115\n",
      "Smoothed Avg Reward: 2.9640786934209618\n",
      "Visited: 738\n",
      "Episode 82\n",
      "Priority Sum Of Visited 1559.0\n",
      "Priority Total 3661.0\n",
      "Episode Reward: 4.641905820380496\n",
      "Smoothed Avg Reward: 3.1318614061169154\n",
      "Visited: 998\n",
      "Episode 83\n",
      "Priority Sum Of Visited 1274.0\n",
      "Priority Total 4208.0\n",
      "Episode Reward: 2.437324027020776\n",
      "Smoothed Avg Reward: 3.0624076682073014\n",
      "Visited: 740\n",
      "Episode 84\n",
      "Priority Sum Of Visited 966.0\n",
      "Priority Total 4084.0\n",
      "Episode Reward: 1.4805372078611645\n",
      "Smoothed Avg Reward: 2.904220622172688\n",
      "Visited: 653\n",
      "Episode 85\n",
      "Priority Sum Of Visited 772.0\n",
      "Priority Total 3949.0\n",
      "Episode Reward: 2.089976396318188\n",
      "Smoothed Avg Reward: 2.822796199587238\n",
      "Visited: 456\n",
      "Episode 86\n",
      "Priority Sum Of Visited 1083.0\n",
      "Priority Total 4219.0\n",
      "Episode Reward: 1.8586160385407569\n",
      "Smoothed Avg Reward: 2.72637818348259\n",
      "Visited: 738\n",
      "Episode 87\n",
      "Priority Sum Of Visited 1685.0\n",
      "Priority Total 4317.0\n",
      "Episode Reward: 2.9047167092471105\n",
      "Smoothed Avg Reward: 2.744212036059042\n",
      "Visited: 936\n",
      "Episode 88\n",
      "Priority Sum Of Visited 1325.0\n",
      "Priority Total 4190.0\n",
      "Episode Reward: 2.9175282095681867\n",
      "Smoothed Avg Reward: 2.761543653409957\n",
      "Visited: 681\n",
      "Episode 89\n",
      "Priority Sum Of Visited 979.0\n",
      "Priority Total 4153.0\n",
      "Episode Reward: 1.6890939966692942\n",
      "Smoothed Avg Reward: 2.6542986877358907\n",
      "Visited: 618\n",
      "Episode 90\n",
      "Priority Sum Of Visited 656.0\n",
      "Priority Total 4047.0\n",
      "Episode Reward: 1.1055623359062026\n",
      "Smoothed Avg Reward: 2.4994250525529216\n",
      "Visited: 491\n",
      "Episode 91\n",
      "Priority Sum Of Visited 1057.0\n",
      "Priority Total 3789.0\n",
      "Episode Reward: 2.860038375592525\n",
      "Smoothed Avg Reward: 2.535486384856882\n",
      "Visited: 658\n",
      "Episode 92\n",
      "Priority Sum Of Visited 673.0\n",
      "Priority Total 3898.0\n",
      "Episode Reward: 1.0729381685520543\n",
      "Smoothed Avg Reward: 2.3892315632263994\n",
      "Visited: 498\n",
      "Episode 93\n",
      "Priority Sum Of Visited 1078.0\n",
      "Priority Total 4049.0\n",
      "Episode Reward: 2.0160488327691874\n",
      "Smoothed Avg Reward: 2.3519132901806783\n",
      "Visited: 738\n",
      "Episode 94\n",
      "Priority Sum Of Visited 893.0\n",
      "Priority Total 3850.0\n",
      "Episode Reward: 1.5874818013155663\n",
      "Smoothed Avg Reward: 2.275470141294167\n",
      "Visited: 692\n",
      "Episode 95\n",
      "Priority Sum Of Visited 1588.0\n",
      "Priority Total 4005.0\n",
      "Episode Reward: 3.92719088654787\n",
      "Smoothed Avg Reward: 2.4406422158195373\n",
      "Visited: 913\n",
      "Episode 96\n",
      "Priority Sum Of Visited 846.0\n",
      "Priority Total 3766.0\n",
      "Episode Reward: 1.397627090249596\n",
      "Smoothed Avg Reward: 2.3363407032625436\n",
      "Visited: 569\n",
      "Episode 97\n",
      "Priority Sum Of Visited 1786.0\n",
      "Priority Total 4347.0\n",
      "Episode Reward: 4.03744425726602\n",
      "Smoothed Avg Reward: 2.506451058662891\n",
      "Visited: 1048\n",
      "Episode 98\n",
      "Priority Sum Of Visited 1054.0\n",
      "Priority Total 3788.0\n",
      "Episode Reward: 2.4351849864352384\n",
      "Smoothed Avg Reward: 2.499324451440126\n",
      "Visited: 697\n",
      "Episode 99\n",
      "Priority Sum Of Visited 1355.0\n",
      "Priority Total 4077.0\n",
      "Episode Reward: 2.129616544505754\n",
      "Smoothed Avg Reward: 2.4623536607466887\n",
      "Visited: 872\n",
      "Episode 100\n",
      "Priority Sum Of Visited 993.0\n",
      "Priority Total 3920.0\n",
      "Episode Reward: 2.102159712099126\n",
      "Smoothed Avg Reward: 2.4263342658819327\n",
      "Visited: 614\n",
      "\n",
      " Scenario tewzt Algo na-mappo Exp check updates 100/781 episodes, total num timesteps 646400/5000000, FPS 170.\n",
      "\n",
      "eval average episode rewards of agent0: 0.8459144068077409\n",
      "eval average episode rewards of agent1: 1.0447314921540336\n",
      "eval average episode rewards of agent2: 0.018671475346761272\n",
      "eval average episode rewards of agent3: 0.0\n",
      "eval average episode rewards of agent4: 0.0\n",
      "Episode 101\n",
      "Priority Sum Of Visited 1794.0\n",
      "Priority Total 4071.0\n",
      "Episode Reward: 4.16802782301691\n",
      "Smoothed Avg Reward: 2.6005036215954305\n",
      "Visited: 1054\n",
      "Episode 102\n",
      "Priority Sum Of Visited 1540.0\n",
      "Priority Total 4133.0\n",
      "Episode Reward: 3.546246978270123\n",
      "Smoothed Avg Reward: 2.6950779572629\n",
      "Visited: 863\n",
      "Episode 103\n",
      "Priority Sum Of Visited 621.0\n",
      "Priority Total 4037.0\n",
      "Episode Reward: 0.6606576190304096\n",
      "Smoothed Avg Reward: 2.491635923439651\n",
      "Visited: 355\n",
      "Episode 104\n",
      "Priority Sum Of Visited 1279.0\n",
      "Priority Total 3980.0\n",
      "Episode Reward: 2.2214832958763675\n",
      "Smoothed Avg Reward: 2.4646206606833223\n",
      "Visited: 787\n",
      "Episode 105\n",
      "Priority Sum Of Visited 1494.0\n",
      "Priority Total 4046.0\n",
      "Episode Reward: 3.4382509568044592\n",
      "Smoothed Avg Reward: 2.561983690295436\n",
      "Visited: 906\n",
      "Episode 106\n",
      "Priority Sum Of Visited 1433.0\n",
      "Priority Total 4047.0\n",
      "Episode Reward: 3.1247041114202423\n",
      "Smoothed Avg Reward: 2.6182557324079165\n",
      "Visited: 931\n",
      "Episode 107\n",
      "Priority Sum Of Visited 1638.0\n",
      "Priority Total 4342.0\n",
      "Episode Reward: 4.424258381865051\n",
      "Smoothed Avg Reward: 2.79885599735363\n",
      "Visited: 932\n",
      "Episode 108\n",
      "Priority Sum Of Visited 1213.0\n",
      "Priority Total 4048.0\n",
      "Episode Reward: 2.486172293642301\n",
      "Smoothed Avg Reward: 2.7675876269824973\n",
      "Visited: 762\n",
      "Episode 109\n",
      "Priority Sum Of Visited 994.0\n",
      "Priority Total 4012.0\n",
      "Episode Reward: 1.5996250406308494\n",
      "Smoothed Avg Reward: 2.6507913683473325\n",
      "Visited: 674\n",
      "Episode 110\n",
      "Priority Sum Of Visited 1159.0\n",
      "Priority Total 3914.0\n",
      "Episode Reward: 2.663991778788145\n",
      "Smoothed Avg Reward: 2.6521114093914138\n",
      "Visited: 761\n",
      "Episode 111\n",
      "Priority Sum Of Visited 1537.0\n",
      "Priority Total 4276.0\n",
      "Episode Reward: 3.473794006358285\n",
      "Smoothed Avg Reward: 2.7342796690881013\n",
      "Visited: 885\n",
      "Episode 112\n",
      "Priority Sum Of Visited 1255.0\n",
      "Priority Total 4007.0\n",
      "Episode Reward: 2.836663864192242\n",
      "Smoothed Avg Reward: 2.7445180885985154\n",
      "Visited: 871\n",
      "Episode 113\n",
      "Priority Sum Of Visited 1496.0\n",
      "Priority Total 3719.0\n",
      "Episode Reward: 3.4855292412436127\n",
      "Smoothed Avg Reward: 2.818619203863025\n",
      "Visited: 1022\n",
      "Episode 114\n",
      "Priority Sum Of Visited 1638.0\n",
      "Priority Total 4029.0\n",
      "Episode Reward: 3.5051954861136125\n",
      "Smoothed Avg Reward: 2.8872768320880837\n",
      "Visited: 989\n",
      "Episode 115\n",
      "Priority Sum Of Visited 1162.0\n",
      "Priority Total 4092.0\n",
      "Episode Reward: 2.44414828686066\n",
      "Smoothed Avg Reward: 2.8429639775653412\n",
      "Visited: 613\n",
      "Episode 116\n",
      "Priority Sum Of Visited 1308.0\n",
      "Priority Total 4306.0\n",
      "Episode Reward: 2.464430646788665\n",
      "Smoothed Avg Reward: 2.8051106444876734\n",
      "Visited: 711\n",
      "Episode 117\n",
      "Priority Sum Of Visited 1968.0\n",
      "Priority Total 3775.0\n",
      "Episode Reward: 6.065522670058328\n",
      "Smoothed Avg Reward: 3.131151847044739\n",
      "Visited: 1223\n",
      "Episode 118\n",
      "Priority Sum Of Visited 807.0\n",
      "Priority Total 3877.0\n",
      "Episode Reward: 0.9730517248571275\n",
      "Smoothed Avg Reward: 2.915341834825978\n",
      "Visited: 501\n",
      "Episode 119\n",
      "Priority Sum Of Visited 1758.0\n",
      "Priority Total 3982.0\n",
      "Episode Reward: 4.047395726777528\n",
      "Smoothed Avg Reward: 3.0285472240211333\n",
      "Visited: 1041\n",
      "Episode 120\n",
      "Priority Sum Of Visited 1887.0\n",
      "Priority Total 4204.0\n",
      "Episode Reward: 4.123560735958051\n",
      "Smoothed Avg Reward: 3.1380485752148255\n",
      "Visited: 1062\n",
      "Episode 121\n",
      "Priority Sum Of Visited 1073.0\n",
      "Priority Total 4186.0\n",
      "Episode Reward: 2.2609623596868875\n",
      "Smoothed Avg Reward: 3.0503399536620317\n",
      "Visited: 603\n",
      "Episode 122\n",
      "Priority Sum Of Visited 1247.0\n",
      "Priority Total 4272.0\n",
      "Episode Reward: 2.000025753447237\n",
      "Smoothed Avg Reward: 2.9453085336405525\n",
      "Visited: 702\n",
      "Episode 123\n",
      "Priority Sum Of Visited 1376.0\n",
      "Priority Total 4225.0\n",
      "Episode Reward: 2.823983978152025\n",
      "Smoothed Avg Reward: 2.9331760780917\n",
      "Visited: 770\n",
      "Episode 124\n",
      "Priority Sum Of Visited 1423.0\n",
      "Priority Total 4166.0\n",
      "Episode Reward: 2.336588813403617\n",
      "Smoothed Avg Reward: 2.873517351622892\n",
      "Visited: 962\n",
      "Episode 125\n",
      "Priority Sum Of Visited 1299.0\n",
      "Priority Total 4442.0\n",
      "Episode Reward: 2.8066544208499735\n",
      "Smoothed Avg Reward: 2.8668310585456003\n",
      "Visited: 707\n",
      "Episode 126\n",
      "Priority Sum Of Visited 1660.0\n",
      "Priority Total 4043.0\n",
      "Episode Reward: 3.097051367597976\n",
      "Smoothed Avg Reward: 2.8898530894508383\n",
      "Visited: 952\n",
      "Episode 127\n",
      "Priority Sum Of Visited 1418.0\n",
      "Priority Total 4020.0\n",
      "Episode Reward: 2.4071608376030307\n",
      "Smoothed Avg Reward: 2.841583864266058\n",
      "Visited: 880\n",
      "Episode 128\n",
      "Priority Sum Of Visited 951.0\n",
      "Priority Total 4029.0\n",
      "Episode Reward: 1.4728270300928834\n",
      "Smoothed Avg Reward: 2.7047081808487405\n",
      "Visited: 719\n",
      "Episode 129\n",
      "Priority Sum Of Visited 1237.0\n",
      "Priority Total 4062.0\n",
      "Episode Reward: 1.4644983310145243\n",
      "Smoothed Avg Reward: 2.580687195865319\n",
      "Visited: 917\n",
      "Episode 130\n",
      "Priority Sum Of Visited 931.0\n",
      "Priority Total 3992.0\n",
      "Episode Reward: 1.9148107085915302\n",
      "Smoothed Avg Reward: 2.51409954713794\n",
      "Visited: 616\n",
      "Episode 131\n",
      "Priority Sum Of Visited 1188.0\n",
      "Priority Total 3815.0\n",
      "Episode Reward: 2.5859418828553222\n",
      "Smoothed Avg Reward: 2.521283780709678\n",
      "Visited: 821\n",
      "Episode 132\n",
      "Priority Sum Of Visited 1254.0\n",
      "Priority Total 4316.0\n",
      "Episode Reward: 3.0362415298894283\n",
      "Smoothed Avg Reward: 2.5727795556276534\n",
      "Visited: 551\n",
      "Episode 133\n",
      "Priority Sum Of Visited 1660.0\n",
      "Priority Total 4303.0\n",
      "Episode Reward: 3.8885698162040883\n",
      "Smoothed Avg Reward: 2.7043585816852973\n",
      "Visited: 908\n",
      "Episode 134\n",
      "Priority Sum Of Visited 653.0\n",
      "Priority Total 3937.0\n",
      "Episode Reward: 1.5087272755190666\n",
      "Smoothed Avg Reward: 2.5847954510686746\n",
      "Visited: 409\n",
      "Episode 135\n",
      "Priority Sum Of Visited 1817.0\n",
      "Priority Total 4387.0\n",
      "Episode Reward: 3.6392402402834634\n",
      "Smoothed Avg Reward: 2.6902399299901534\n",
      "Visited: 1011\n",
      "Episode 136\n",
      "Priority Sum Of Visited 687.0\n",
      "Priority Total 3698.0\n",
      "Episode Reward: 0.946327747651881\n",
      "Smoothed Avg Reward: 2.5158487117563264\n",
      "Visited: 512\n",
      "Episode 137\n",
      "Priority Sum Of Visited 1504.0\n",
      "Priority Total 3912.0\n",
      "Episode Reward: 3.3823917206142484\n",
      "Smoothed Avg Reward: 2.6025030126421185\n",
      "Visited: 910\n",
      "Episode 138\n",
      "Priority Sum Of Visited 365.0\n",
      "Priority Total 3770.0\n",
      "Episode Reward: 0.26862336328265174\n",
      "Smoothed Avg Reward: 2.3691150477061718\n",
      "Visited: 310\n",
      "Episode 139\n",
      "Priority Sum Of Visited 1464.0\n",
      "Priority Total 4467.0\n",
      "Episode Reward: 2.842163378142696\n",
      "Smoothed Avg Reward: 2.4164198807498245\n",
      "Visited: 925\n",
      "Episode 140\n",
      "Priority Sum Of Visited 563.0\n",
      "Priority Total 3975.0\n",
      "Episode Reward: 0.5876043510937071\n",
      "Smoothed Avg Reward: 2.233538327784213\n",
      "Visited: 369\n",
      "Episode 141\n",
      "Priority Sum Of Visited 1523.0\n",
      "Priority Total 4144.0\n",
      "Episode Reward: 2.939916155468762\n",
      "Smoothed Avg Reward: 2.304176110552668\n",
      "Visited: 817\n",
      "Episode 142\n",
      "Priority Sum Of Visited 729.0\n",
      "Priority Total 4011.0\n",
      "Episode Reward: 0.8782121914672321\n",
      "Smoothed Avg Reward: 2.1615797186441243\n",
      "Visited: 388\n",
      "Episode 143\n",
      "Priority Sum Of Visited 1418.0\n",
      "Priority Total 4233.0\n",
      "Episode Reward: 2.026259984979594\n",
      "Smoothed Avg Reward: 2.148047745277671\n",
      "Visited: 824\n",
      "Episode 144\n",
      "Priority Sum Of Visited 1154.0\n",
      "Priority Total 4377.0\n",
      "Episode Reward: 1.5358829142449144\n",
      "Smoothed Avg Reward: 2.0868312621743956\n",
      "Visited: 756\n",
      "Episode 145\n",
      "Priority Sum Of Visited 1417.0\n",
      "Priority Total 4317.0\n",
      "Episode Reward: 2.2020799625938126\n",
      "Smoothed Avg Reward: 2.0983561322163373\n",
      "Visited: 908\n",
      "Episode 146\n",
      "Priority Sum Of Visited 1077.0\n",
      "Priority Total 4041.0\n",
      "Episode Reward: 1.2134230913635116\n",
      "Smoothed Avg Reward: 2.009862828131055\n",
      "Visited: 699\n",
      "Episode 147\n",
      "Priority Sum Of Visited 672.0\n",
      "Priority Total 4123.0\n",
      "Episode Reward: 0.97123746751966\n",
      "Smoothed Avg Reward: 1.9060002920699155\n",
      "Visited: 430\n",
      "Episode 148\n",
      "Priority Sum Of Visited 1563.0\n",
      "Priority Total 4472.0\n",
      "Episode Reward: 2.893802893215907\n",
      "Smoothed Avg Reward: 2.004780552184515\n",
      "Visited: 859\n",
      "Episode 149\n",
      "Priority Sum Of Visited 1485.0\n",
      "Priority Total 4397.0\n",
      "Episode Reward: 2.475898990198881\n",
      "Smoothed Avg Reward: 2.051892395985951\n",
      "Visited: 876\n",
      "Episode 150\n",
      "Priority Sum Of Visited 1236.0\n",
      "Priority Total 3928.0\n",
      "Episode Reward: 2.6457901467763953\n",
      "Smoothed Avg Reward: 2.1112821710649956\n",
      "Visited: 825\n",
      "\n",
      " Scenario tewzt Algo na-mappo Exp check updates 150/781 episodes, total num timesteps 966400/5000000, FPS 170.\n",
      "\n",
      "eval average episode rewards of agent0: 0.024941462147973138\n",
      "eval average episode rewards of agent1: 0.9423069622380749\n",
      "eval average episode rewards of agent2: 0.021926995180882505\n",
      "eval average episode rewards of agent3: 0.0\n",
      "eval average episode rewards of agent4: 0.0\n",
      "Episode 151\n",
      "Priority Sum Of Visited 1389.0\n",
      "Priority Total 3866.0\n",
      "Episode Reward: 2.8426427188732513\n",
      "Smoothed Avg Reward: 2.184418225845821\n",
      "Visited: 921\n",
      "Episode 152\n",
      "Priority Sum Of Visited 1484.0\n",
      "Priority Total 4335.0\n",
      "Episode Reward: 3.0977804916661023\n",
      "Smoothed Avg Reward: 2.275754452427849\n",
      "Visited: 791\n",
      "Episode 153\n",
      "Priority Sum Of Visited 282.0\n",
      "Priority Total 4161.0\n",
      "Episode Reward: 0.23153420880226877\n",
      "Smoothed Avg Reward: 2.071332428065291\n",
      "Visited: 230\n",
      "Episode 154\n",
      "Priority Sum Of Visited 1402.0\n",
      "Priority Total 4212.0\n",
      "Episode Reward: 2.046740917506982\n",
      "Smoothed Avg Reward: 2.06887327700946\n",
      "Visited: 814\n",
      "Episode 155\n",
      "Priority Sum Of Visited 958.0\n",
      "Priority Total 3998.0\n",
      "Episode Reward: 1.4451416553699568\n",
      "Smoothed Avg Reward: 2.00650011484551\n",
      "Visited: 532\n",
      "Episode 156\n",
      "Priority Sum Of Visited 1136.0\n",
      "Priority Total 3996.0\n",
      "Episode Reward: 1.731060640219798\n",
      "Smoothed Avg Reward: 1.9789561673829388\n",
      "Visited: 761\n",
      "Episode 157\n",
      "Priority Sum Of Visited 1251.0\n",
      "Priority Total 4424.0\n",
      "Episode Reward: 2.625721191577094\n",
      "Smoothed Avg Reward: 2.043632669802354\n",
      "Visited: 647\n",
      "Episode 158\n",
      "Priority Sum Of Visited 1261.0\n",
      "Priority Total 4167.0\n",
      "Episode Reward: 2.2261012495530252\n",
      "Smoothed Avg Reward: 2.061879527777421\n",
      "Visited: 816\n",
      "Episode 159\n",
      "Priority Sum Of Visited 1274.0\n",
      "Priority Total 4223.0\n",
      "Episode Reward: 2.880023353500554\n",
      "Smoothed Avg Reward: 2.1436939103497346\n",
      "Visited: 767\n",
      "Episode 160\n",
      "Priority Sum Of Visited 1013.0\n",
      "Priority Total 4228.0\n",
      "Episode Reward: 1.8424095143517676\n",
      "Smoothed Avg Reward: 2.113565470749938\n",
      "Visited: 552\n",
      "Episode 161\n",
      "Priority Sum Of Visited 1090.0\n",
      "Priority Total 3681.0\n",
      "Episode Reward: 1.9426738228076486\n",
      "Smoothed Avg Reward: 2.096476305955709\n",
      "Visited: 723\n",
      "Episode 162\n",
      "Priority Sum Of Visited 958.0\n",
      "Priority Total 3970.0\n",
      "Episode Reward: 1.6025666681471236\n",
      "Smoothed Avg Reward: 2.0470853421748507\n",
      "Visited: 560\n",
      "Episode 163\n",
      "Priority Sum Of Visited 1017.0\n",
      "Priority Total 4213.0\n",
      "Episode Reward: 1.4621607675179884\n",
      "Smoothed Avg Reward: 1.9885928847091645\n",
      "Visited: 680\n",
      "Episode 164\n",
      "Priority Sum Of Visited 1086.0\n",
      "Priority Total 4147.0\n",
      "Episode Reward: 2.516209375384683\n",
      "Smoothed Avg Reward: 2.0413545337767163\n",
      "Visited: 685\n",
      "Episode 165\n",
      "Priority Sum Of Visited 1387.0\n",
      "Priority Total 3993.0\n",
      "Episode Reward: 2.4319221547801306\n",
      "Smoothed Avg Reward: 2.0804112958770578\n",
      "Visited: 892\n",
      "Episode 166\n",
      "Priority Sum Of Visited 1443.0\n",
      "Priority Total 3952.0\n",
      "Episode Reward: 3.325627609758401\n",
      "Smoothed Avg Reward: 2.204932927265192\n",
      "Visited: 835\n",
      "Episode 167\n",
      "Priority Sum Of Visited 1096.0\n",
      "Priority Total 3699.0\n",
      "Episode Reward: 1.8777209099351797\n",
      "Smoothed Avg Reward: 2.172211725532191\n",
      "Visited: 766\n",
      "Episode 168\n",
      "Priority Sum Of Visited 841.0\n",
      "Priority Total 4337.0\n",
      "Episode Reward: 0.9627583704868518\n",
      "Smoothed Avg Reward: 2.0512663900276573\n",
      "Visited: 580\n",
      "Episode 169\n",
      "Priority Sum Of Visited 1748.0\n",
      "Priority Total 4206.0\n",
      "Episode Reward: 3.106827270961552\n",
      "Smoothed Avg Reward: 2.1568224781210468\n",
      "Visited: 1047\n",
      "Episode 170\n",
      "Priority Sum Of Visited 1378.0\n",
      "Priority Total 4172.0\n",
      "Episode Reward: 3.1680799104471276\n",
      "Smoothed Avg Reward: 2.2579482213536552\n",
      "Visited: 788\n",
      "Episode 171\n",
      "Priority Sum Of Visited 1094.0\n",
      "Priority Total 3618.0\n",
      "Episode Reward: 2.724029948531405\n",
      "Smoothed Avg Reward: 2.30455639407143\n",
      "Visited: 699\n",
      "Episode 172\n",
      "Priority Sum Of Visited 610.0\n",
      "Priority Total 4051.0\n",
      "Episode Reward: 0.868601338854073\n",
      "Smoothed Avg Reward: 2.1609608885496945\n",
      "Visited: 456\n",
      "Episode 173\n",
      "Priority Sum Of Visited 1424.0\n",
      "Priority Total 4166.0\n",
      "Episode Reward: 3.1327475766261856\n",
      "Smoothed Avg Reward: 2.2581395573573437\n",
      "Visited: 736\n",
      "Episode 174\n",
      "Priority Sum Of Visited 508.0\n",
      "Priority Total 4096.0\n",
      "Episode Reward: 0.26987481117248535\n",
      "Smoothed Avg Reward: 2.059313082738858\n",
      "Visited: 358\n",
      "Episode 175\n",
      "Priority Sum Of Visited 971.0\n",
      "Priority Total 3872.0\n",
      "Episode Reward: 1.391102175611297\n",
      "Smoothed Avg Reward: 1.9924919920261017\n",
      "Visited: 578\n",
      "Episode 176\n",
      "Priority Sum Of Visited 979.0\n",
      "Priority Total 3895.0\n",
      "Episode Reward: 1.3685766123251393\n",
      "Smoothed Avg Reward: 1.9301004540560054\n",
      "Visited: 632\n",
      "Episode 177\n",
      "Priority Sum Of Visited 1351.0\n",
      "Priority Total 4070.0\n",
      "Episode Reward: 3.278209467005535\n",
      "Smoothed Avg Reward: 2.0649113553509584\n",
      "Visited: 840\n",
      "Episode 178\n",
      "Priority Sum Of Visited 617.0\n",
      "Priority Total 4052.0\n",
      "Episode Reward: 0.4346291278532095\n",
      "Smoothed Avg Reward: 1.9018831326011836\n",
      "Visited: 517\n",
      "Episode 179\n",
      "Priority Sum Of Visited 785.0\n",
      "Priority Total 3828.0\n",
      "Episode Reward: 0.7355646236442904\n",
      "Smoothed Avg Reward: 1.7852512817054944\n",
      "Visited: 499\n",
      "Episode 180\n",
      "Priority Sum Of Visited 1321.0\n",
      "Priority Total 4023.0\n",
      "Episode Reward: 2.2723817294899336\n",
      "Smoothed Avg Reward: 1.8339643264839385\n",
      "Visited: 904\n",
      "Episode 181\n",
      "Priority Sum Of Visited 1244.0\n",
      "Priority Total 3972.0\n",
      "Episode Reward: 2.6735024070811884\n",
      "Smoothed Avg Reward: 1.9179181345436636\n",
      "Visited: 700\n",
      "Episode 182\n",
      "Priority Sum Of Visited 1588.0\n",
      "Priority Total 4197.0\n",
      "Episode Reward: 3.6339501609129035\n",
      "Smoothed Avg Reward: 2.0895213371805874\n",
      "Visited: 939\n",
      "Episode 183\n",
      "Priority Sum Of Visited 960.0\n",
      "Priority Total 4057.0\n",
      "Episode Reward: 1.4409193274857184\n",
      "Smoothed Avg Reward: 2.0246611362111007\n",
      "Visited: 577\n",
      "Episode 184\n",
      "Priority Sum Of Visited 729.0\n",
      "Priority Total 4051.0\n",
      "Episode Reward: 1.2006816203745367\n",
      "Smoothed Avg Reward: 1.9422631846274443\n",
      "Visited: 479\n",
      "Episode 185\n",
      "Priority Sum Of Visited 1639.0\n",
      "Priority Total 4295.0\n",
      "Episode Reward: 2.901463515119648\n",
      "Smoothed Avg Reward: 2.0381832176766648\n",
      "Visited: 939\n",
      "Episode 186\n",
      "Priority Sum Of Visited 1494.0\n",
      "Priority Total 4038.0\n",
      "Episode Reward: 3.5437932263604695\n",
      "Smoothed Avg Reward: 2.1887442185450454\n",
      "Visited: 856\n",
      "Episode 187\n",
      "Priority Sum Of Visited 1159.0\n",
      "Priority Total 4037.0\n",
      "Episode Reward: 1.9773284264472388\n",
      "Smoothed Avg Reward: 2.1676026393352648\n",
      "Visited: 812\n",
      "Episode 188\n",
      "Priority Sum Of Visited 1016.0\n",
      "Priority Total 3955.0\n",
      "Episode Reward: 1.658502719436902\n",
      "Smoothed Avg Reward: 2.1166926473454284\n",
      "Visited: 709\n",
      "Episode 189\n",
      "Priority Sum Of Visited 941.0\n",
      "Priority Total 3953.0\n",
      "Episode Reward: 1.543949719346516\n",
      "Smoothed Avg Reward: 2.0594183545455373\n",
      "Visited: 491\n",
      "Episode 190\n",
      "Priority Sum Of Visited 1343.0\n",
      "Priority Total 4221.0\n",
      "Episode Reward: 1.483581348680162\n",
      "Smoothed Avg Reward: 2.001834653959\n",
      "Visited: 820\n",
      "Episode 191\n",
      "Priority Sum Of Visited 1202.0\n",
      "Priority Total 3827.0\n",
      "Episode Reward: 2.586534046423414\n",
      "Smoothed Avg Reward: 2.060304593205441\n",
      "Visited: 779\n",
      "Episode 192\n",
      "Priority Sum Of Visited 1397.0\n",
      "Priority Total 4375.0\n",
      "Episode Reward: 2.4061198628571443\n",
      "Smoothed Avg Reward: 2.0948861201706115\n",
      "Visited: 774\n",
      "Episode 193\n",
      "Priority Sum Of Visited 1497.0\n",
      "Priority Total 4181.0\n",
      "Episode Reward: 2.6177259674221274\n",
      "Smoothed Avg Reward: 2.147170104895763\n",
      "Visited: 924\n",
      "Episode 194\n",
      "Priority Sum Of Visited 1000.0\n",
      "Priority Total 4074.0\n",
      "Episode Reward: 1.2858937708359992\n",
      "Smoothed Avg Reward: 2.061042471489787\n",
      "Visited: 532\n",
      "Episode 195\n",
      "Priority Sum Of Visited 912.0\n",
      "Priority Total 4126.0\n",
      "Episode Reward: 1.6617169908897353\n",
      "Smoothed Avg Reward: 2.0211099234297816\n",
      "Visited: 602\n",
      "Episode 196\n",
      "Priority Sum Of Visited 1464.0\n",
      "Priority Total 4121.0\n",
      "Episode Reward: 3.372371293722809\n",
      "Smoothed Avg Reward: 2.1562360604590847\n",
      "Visited: 908\n",
      "Episode 197\n",
      "Priority Sum Of Visited 536.0\n",
      "Priority Total 4133.0\n",
      "Episode Reward: 0.4755128137504436\n",
      "Smoothed Avg Reward: 1.9881637357882205\n",
      "Visited: 347\n",
      "Episode 198\n",
      "Priority Sum Of Visited 1163.0\n",
      "Priority Total 3999.0\n",
      "Episode Reward: 1.544880406148049\n",
      "Smoothed Avg Reward: 1.9438354028242033\n",
      "Visited: 697\n",
      "Episode 199\n",
      "Priority Sum Of Visited 537.0\n",
      "Priority Total 3964.0\n",
      "Episode Reward: 0.6539374043485209\n",
      "Smoothed Avg Reward: 1.814845602976635\n",
      "Visited: 405\n",
      "Episode 200\n",
      "Priority Sum Of Visited 1444.0\n",
      "Priority Total 4186.0\n",
      "Episode Reward: 2.3440624323016985\n",
      "Smoothed Avg Reward: 1.8677672859091414\n",
      "Visited: 864\n",
      "\n",
      " Scenario tewzt Algo na-mappo Exp check updates 200/781 episodes, total num timesteps 1286400/5000000, FPS 170.\n",
      "\n",
      "eval average episode rewards of agent0: 0.20933151245117193\n",
      "eval average episode rewards of agent1: 1.3407688847294559\n",
      "eval average episode rewards of agent2: 0.0\n",
      "eval average episode rewards of agent3: 0.0\n",
      "eval average episode rewards of agent4: 0.02523841386959877\n",
      "Episode 201\n",
      "Priority Sum Of Visited 1031.0\n",
      "Priority Total 4118.0\n",
      "Episode Reward: 1.4275188991293968\n",
      "Smoothed Avg Reward: 1.823742447231167\n",
      "Visited: 769\n",
      "Episode 202\n",
      "Priority Sum Of Visited 1488.0\n",
      "Priority Total 3857.0\n",
      "Episode Reward: 2.6355019937889734\n",
      "Smoothed Avg Reward: 1.9049184018869476\n",
      "Visited: 879\n",
      "Episode 203\n",
      "Priority Sum Of Visited 1428.0\n",
      "Priority Total 4153.0\n",
      "Episode Reward: 2.495399743810794\n",
      "Smoothed Avg Reward: 1.9639665360793321\n",
      "Visited: 792\n",
      "Episode 204\n",
      "Priority Sum Of Visited 1499.0\n",
      "Priority Total 4039.0\n",
      "Episode Reward: 2.614295957322764\n",
      "Smoothed Avg Reward: 2.0289994782036755\n",
      "Visited: 945\n",
      "Episode 205\n",
      "Priority Sum Of Visited 1294.0\n",
      "Priority Total 3583.0\n",
      "Episode Reward: 2.883795381000723\n",
      "Smoothed Avg Reward: 2.1144790684833805\n",
      "Visited: 878\n",
      "Episode 206\n",
      "Priority Sum Of Visited 1389.0\n",
      "Priority Total 4035.0\n",
      "Episode Reward: 2.123910577981693\n",
      "Smoothed Avg Reward: 2.1154222194332117\n",
      "Visited: 901\n",
      "Episode 207\n",
      "Priority Sum Of Visited 1847.0\n",
      "Priority Total 4234.0\n",
      "Episode Reward: 3.3700172524242533\n",
      "Smoothed Avg Reward: 2.2408817227323157\n",
      "Visited: 900\n",
      "Episode 208\n",
      "Priority Sum Of Visited 1406.0\n",
      "Priority Total 3954.0\n",
      "Episode Reward: 2.9095671287074993\n",
      "Smoothed Avg Reward: 2.3077502633298344\n",
      "Visited: 895\n",
      "Episode 209\n",
      "Priority Sum Of Visited 910.0\n",
      "Priority Total 4161.0\n",
      "Episode Reward: 1.269320103747729\n",
      "Smoothed Avg Reward: 2.203907247371624\n",
      "Visited: 530\n",
      "Episode 210\n",
      "Priority Sum Of Visited 668.0\n",
      "Priority Total 4052.0\n",
      "Episode Reward: 0.9336390375269593\n",
      "Smoothed Avg Reward: 2.0768804263871576\n",
      "Visited: 389\n",
      "Episode 211\n",
      "Priority Sum Of Visited 1028.0\n",
      "Priority Total 4133.0\n",
      "Episode Reward: 1.8734503947472643\n",
      "Smoothed Avg Reward: 2.056537423223168\n",
      "Visited: 666\n",
      "Episode 212\n",
      "Priority Sum Of Visited 1410.0\n",
      "Priority Total 3890.0\n",
      "Episode Reward: 3.154231203864631\n",
      "Smoothed Avg Reward: 2.1663068012873143\n",
      "Visited: 900\n",
      "Episode 213\n",
      "Priority Sum Of Visited 807.0\n",
      "Priority Total 4145.0\n",
      "Episode Reward: 0.8912467096695332\n",
      "Smoothed Avg Reward: 2.0388007921255364\n",
      "Visited: 581\n",
      "Episode 214\n",
      "Priority Sum Of Visited 625.0\n",
      "Priority Total 4073.0\n",
      "Episode Reward: 0.6064918599179022\n",
      "Smoothed Avg Reward: 1.895569898904773\n",
      "Visited: 389\n",
      "Episode 215\n",
      "Priority Sum Of Visited 798.0\n",
      "Priority Total 4036.0\n",
      "Episode Reward: 0.8752717121722142\n",
      "Smoothed Avg Reward: 1.7935400802315171\n",
      "Visited: 445\n",
      "Episode 216\n",
      "Priority Sum Of Visited 552.0\n",
      "Priority Total 4067.0\n",
      "Episode Reward: 0.7827339929309229\n",
      "Smoothed Avg Reward: 1.6924594715014578\n",
      "Visited: 329\n",
      "Episode 217\n",
      "Priority Sum Of Visited 901.0\n",
      "Priority Total 3897.0\n",
      "Episode Reward: 1.4683008563662887\n",
      "Smoothed Avg Reward: 1.670043609987941\n",
      "Visited: 535\n",
      "Episode 218\n",
      "Priority Sum Of Visited 987.0\n",
      "Priority Total 3836.0\n",
      "Episode Reward: 1.6034791547286509\n",
      "Smoothed Avg Reward: 1.663387164462012\n",
      "Visited: 588\n",
      "Episode 219\n",
      "Priority Sum Of Visited 600.0\n",
      "Priority Total 4091.0\n",
      "Episode Reward: 0.7971564889475756\n",
      "Smoothed Avg Reward: 1.5767640969105683\n",
      "Visited: 356\n",
      "Episode 220\n",
      "Priority Sum Of Visited 1251.0\n",
      "Priority Total 4045.0\n",
      "Episode Reward: 1.914223942329878\n",
      "Smoothed Avg Reward: 1.6105100814524993\n",
      "Visited: 852\n",
      "Episode 221\n",
      "Priority Sum Of Visited 1455.0\n",
      "Priority Total 4210.0\n",
      "Episode Reward: 2.450434154625622\n",
      "Smoothed Avg Reward: 1.6945024887698117\n",
      "Visited: 796\n",
      "Episode 222\n",
      "Priority Sum Of Visited 1721.0\n",
      "Priority Total 4271.0\n",
      "Episode Reward: 3.6806055508443665\n",
      "Smoothed Avg Reward: 1.8931127949772673\n",
      "Visited: 968\n",
      "Episode 223\n",
      "Priority Sum Of Visited 1300.0\n",
      "Priority Total 3925.0\n",
      "Episode Reward: 2.5130415676092297\n",
      "Smoothed Avg Reward: 1.9551056722404634\n",
      "Visited: 789\n",
      "Episode 224\n",
      "Priority Sum Of Visited 1190.0\n",
      "Priority Total 4227.0\n",
      "Episode Reward: 2.94174376322546\n",
      "Smoothed Avg Reward: 2.0537694813389633\n",
      "Visited: 583\n",
      "Episode 225\n",
      "Priority Sum Of Visited 1305.0\n",
      "Priority Total 3584.0\n",
      "Episode Reward: 3.644482593147118\n",
      "Smoothed Avg Reward: 2.2128407925197786\n",
      "Visited: 901\n",
      "Episode 226\n",
      "Priority Sum Of Visited 1540.0\n",
      "Priority Total 3957.0\n",
      "Episode Reward: 3.8756659359788195\n",
      "Smoothed Avg Reward: 2.3791233068656825\n",
      "Visited: 942\n",
      "Episode 227\n",
      "Priority Sum Of Visited 1751.0\n",
      "Priority Total 3942.0\n",
      "Episode Reward: 4.752377317372833\n",
      "Smoothed Avg Reward: 2.6164487079163976\n",
      "Visited: 1085\n",
      "Episode 228\n",
      "Priority Sum Of Visited 860.0\n",
      "Priority Total 4036.0\n",
      "Episode Reward: 1.2042212260124678\n",
      "Smoothed Avg Reward: 2.475225959726005\n",
      "Visited: 641\n",
      "Episode 229\n",
      "Priority Sum Of Visited 1548.0\n",
      "Priority Total 4367.0\n",
      "Episode Reward: 3.1855786122882077\n",
      "Smoothed Avg Reward: 2.546261224982225\n",
      "Visited: 948\n",
      "Episode 230\n",
      "Priority Sum Of Visited 1146.0\n",
      "Priority Total 4267.0\n",
      "Episode Reward: 1.8759780766922542\n",
      "Smoothed Avg Reward: 2.4792329101532284\n",
      "Visited: 757\n",
      "Episode 231\n",
      "Priority Sum Of Visited 1373.0\n",
      "Priority Total 4092.0\n",
      "Episode Reward: 3.420803257721478\n",
      "Smoothed Avg Reward: 2.573389944910054\n",
      "Visited: 756\n",
      "Episode 232\n",
      "Priority Sum Of Visited 1918.0\n",
      "Priority Total 4149.0\n",
      "Episode Reward: 3.9521274324611415\n",
      "Smoothed Avg Reward: 2.711263693665163\n",
      "Visited: 1103\n",
      "Episode 233\n",
      "Priority Sum Of Visited 1070.0\n",
      "Priority Total 3997.0\n",
      "Episode Reward: 2.5668473897329434\n",
      "Smoothed Avg Reward: 2.696822063271941\n",
      "Visited: 699\n",
      "Episode 234\n",
      "Priority Sum Of Visited 1437.0\n",
      "Priority Total 4194.0\n",
      "Episode Reward: 2.905840405111282\n",
      "Smoothed Avg Reward: 2.7177238974558753\n",
      "Visited: 885\n",
      "Episode 235\n",
      "Priority Sum Of Visited 1362.0\n",
      "Priority Total 4101.0\n",
      "Episode Reward: 2.6629543790087915\n",
      "Smoothed Avg Reward: 2.712246945611167\n",
      "Visited: 767\n",
      "Episode 236\n",
      "Priority Sum Of Visited 1328.0\n",
      "Priority Total 4409.0\n",
      "Episode Reward: 2.5508576680382347\n",
      "Smoothed Avg Reward: 2.6961080178538737\n",
      "Visited: 692\n",
      "Episode 237\n",
      "Priority Sum Of Visited 1504.0\n",
      "Priority Total 4069.0\n",
      "Episode Reward: 3.579808454081086\n",
      "Smoothed Avg Reward: 2.7844780614765954\n",
      "Visited: 928\n",
      "Episode 238\n",
      "Priority Sum Of Visited 1125.0\n",
      "Priority Total 4267.0\n",
      "Episode Reward: 1.7802002813268905\n",
      "Smoothed Avg Reward: 2.684050283461625\n",
      "Visited: 666\n",
      "Episode 239\n",
      "Priority Sum Of Visited 1261.0\n",
      "Priority Total 3983.0\n",
      "Episode Reward: 2.4586911521846355\n",
      "Smoothed Avg Reward: 2.661514370333926\n",
      "Visited: 791\n",
      "Episode 240\n",
      "Priority Sum Of Visited 800.0\n",
      "Priority Total 3763.0\n",
      "Episode Reward: 1.0817749420928524\n",
      "Smoothed Avg Reward: 2.5035404275098188\n",
      "Visited: 639\n",
      "Episode 241\n",
      "Priority Sum Of Visited 1421.0\n",
      "Priority Total 3978.0\n",
      "Episode Reward: 3.465938099466621\n",
      "Smoothed Avg Reward: 2.5997801947054993\n",
      "Visited: 916\n",
      "Episode 242\n",
      "Priority Sum Of Visited 1112.0\n",
      "Priority Total 4220.0\n",
      "Episode Reward: 2.0428909952606635\n",
      "Smoothed Avg Reward: 2.5440912747610156\n",
      "Visited: 586\n",
      "Episode 243\n",
      "Priority Sum Of Visited 1145.0\n",
      "Priority Total 3847.0\n",
      "Episode Reward: 2.2736036959313726\n",
      "Smoothed Avg Reward: 2.5170425168780515\n",
      "Visited: 850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1952\\1089166900.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[0mrunner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMPERunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1952\\1089166900.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;31m# compute return and update network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mtrain_infos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#self.noise_vector)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;31m# post process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jacob\\downloads\\noisy-mappo-master\\noisy-mappo-master\\onpolicy\\runner\\separated\\base_runner.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprep_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mtrain_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[0mtrain_infos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jacob\\downloads\\noisy-mappo-master\\noisy-mappo-master\\onpolicy\\algorithms\\r_mappo\\r_mappo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, buffer, noise_vector, update_actor)\u001b[0m\n\u001b[0;32m    216\u001b[0m                     \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_actor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m                 \u001b[0mtrain_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m                 \u001b[0mtrain_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'policy_loss'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 \u001b[0mtrain_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dist_entropy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdist_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import onpolicy\n",
    "#from onpolicy.runner.separated.mpe_runner import MPERunner\n",
    "from onpolicy.envs.env_wrappers import SubprocVecEnv, DummyVecEnv\n",
    "import torch\n",
    "\n",
    "from onpolicy.config import get_config\n",
    "\n",
    "import wandb\n",
    "import socket\n",
    "\n",
    "\n",
    "import time\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "from onpolicy.utils.util import update_linear_schedule\n",
    "from onpolicy.runner.separated.base_runner import Runner\n",
    "import imageio\n",
    "\n",
    "def _t2n(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "class MPERunner(Runner):\n",
    "    def __init__(self, config):\n",
    "        super(MPERunner, self).__init__(config)\n",
    "        self.noise_vector = None\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # init noise\n",
    "        if self.noise_vector is None:\n",
    "            self.noise_vector = []\n",
    "            for i in range(self.num_agents):#self.args.num_agents):\n",
    "                self.noise_vector.append(np.random.randn(4)*1)# self.args.noise_dim) * self.args.sigma)\n",
    "            self.noise_vector = np.array(self.noise_vector)\n",
    "        else:\n",
    "            # shuffle noise\n",
    "            np.random.shuffle(self.noise_vector)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.warmup()   \n",
    "\n",
    "        start = time.time()\n",
    "        episodes = int(self.num_env_steps) // self.episode_length // self.n_rollout_threads\n",
    "        total_reward = 0\n",
    "        for episode in range(episodes):\n",
    "            obs = self.envs.reset()\n",
    "            print(f\"Episode {episode}\")\n",
    "            if self.use_linear_lr_decay:\n",
    "                for agent_id in range(self.num_agents):\n",
    "                    self.trainer[agent_id].policy.lr_decay(episode, episodes)\n",
    "\n",
    "            episode_reward_total = 0\n",
    "            for step in range(self.episode_length):\n",
    "                # Sample actions\n",
    "                values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env = self.collect(step)\n",
    "                    \n",
    "                # Obser reward and next obs\n",
    "                obs, rewards, dones, infos = self.envs.step(actions_env)\n",
    "\n",
    "                data = obs, rewards, dones, infos, values, actions, action_log_probs, rnn_states, rnn_states_critic \n",
    "                episode_reward_total += np.sum(rewards)\n",
    "\n",
    "                # insert data into buffer\n",
    "                self.insert(data)\n",
    "\n",
    "            self.reset_noise()\n",
    "            endResult = 0\n",
    "            for drone in self.envs.envs[0].drones:\n",
    "                endResult += drone.totalReward\n",
    "            \n",
    "            print(f\"Priority Sum Of Visited {endResult}\")\n",
    "            print(f\"Priority Total {self.envs.envs[0].maxReward}\")\n",
    "            print(f\"Episode Reward: {episode_reward_total}\")\n",
    "            total_reward = total_reward * 0.9 + episode_reward_total * 0.1\n",
    "            print(f\"Smoothed Avg Reward: {total_reward}\")\n",
    "            visited = np.sum(self.envs.envs[0].seen)\n",
    "            print(f\"Visited: {visited}\")\n",
    "            # compute return and update network\n",
    "            self.compute()\n",
    "            train_infos = self.train()#self.noise_vector)\n",
    "            \n",
    "            # post process\n",
    "            total_num_steps = (episode + 1) * self.episode_length * self.n_rollout_threads\n",
    "            \n",
    "            # save model\n",
    "            if (episode % self.save_interval == 0 or episode == episodes - 1):\n",
    "                self.save()\n",
    "\n",
    "            # log information\n",
    "            if episode % self.log_interval == 0:\n",
    "                end = time.time()\n",
    "                print(\"\\n Scenario {} Algo {} Exp {} updates {}/{} episodes, total num timesteps {}/{}, FPS {}.\\n\"\n",
    "                        .format(self.all_args.scenario_name,\n",
    "                                self.algorithm_name,\n",
    "                                self.experiment_name,\n",
    "                                episode,\n",
    "                                episodes,\n",
    "                                total_num_steps,\n",
    "                                self.num_env_steps,\n",
    "                                int(total_num_steps / (end - start))))\n",
    "\n",
    "                if self.env_name == \"MPE\":\n",
    "                    for agent_id in range(self.num_agents):\n",
    "                        idv_rews = []\n",
    "                        for info in infos:\n",
    "                            if 'individual_reward' in info[agent_id].keys():\n",
    "                                idv_rews.append(info[agent_id]['individual_reward'])\n",
    "                        train_infos[agent_id].update({'individual_rewards': np.mean(idv_rews)})\n",
    "                        train_infos[agent_id].update({\"average_episode_rewards\": np.mean(self.buffer[agent_id].rewards) * self.episode_length})\n",
    "                self.log_train(train_infos, total_num_steps)\n",
    "\n",
    "            # eval\n",
    "            if episode % self.eval_interval == 0 and self.use_eval:\n",
    "                self.eval(total_num_steps)\n",
    "\n",
    "    def warmup(self):\n",
    "        # reset env\n",
    "        obs = self.envs.reset()\n",
    "\n",
    "        share_obs = []\n",
    "        for o in obs:\n",
    "            share_obs.append(list(chain(*o)))\n",
    "        share_obs = np.array(share_obs)\n",
    "\n",
    "        for agent_id in range(self.num_agents):\n",
    "            if not self.use_centralized_V:\n",
    "                share_obs = np.array(list(obs[:, agent_id]))\n",
    "            self.buffer[agent_id].share_obs[0] = share_obs.copy()\n",
    "            self.buffer[agent_id].obs[0] = np.array(list(obs[:, agent_id])).copy()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect(self, step):\n",
    "        values = []\n",
    "        actions = []\n",
    "        temp_actions_env = []\n",
    "        action_log_probs = []\n",
    "        rnn_states = []\n",
    "        rnn_states_critic = []\n",
    "\n",
    "        for agent_id in range(self.num_agents):\n",
    "            self.trainer[agent_id].prep_rollout()\n",
    "            value, action, action_log_prob, rnn_state, rnn_state_critic \\\n",
    "                = self.trainer[agent_id].policy.get_actions(self.buffer[agent_id].share_obs[step],\n",
    "                                                            self.buffer[agent_id].obs[step],\n",
    "                                                            self.buffer[agent_id].rnn_states[step],\n",
    "                                                            self.buffer[agent_id].rnn_states_critic[step],\n",
    "                                                            self.buffer[agent_id].masks[step])\n",
    "            # [agents, envs, dim]\n",
    "            values.append(_t2n(value))\n",
    "            action = _t2n(action)\n",
    "            # rearrange action\n",
    "            if self.envs.action_space[agent_id].__class__.__name__ == 'MultiDiscrete':\n",
    "                for i in range(self.envs.action_space[agent_id].shape):\n",
    "                    uc_action_env = np.eye(self.envs.action_space[agent_id].high[i]+1)[action[:, i]]\n",
    "                    if i == 0:\n",
    "                        action_env = uc_action_env\n",
    "                    else:\n",
    "                        action_env = np.concatenate((action_env, uc_action_env), axis=1)\n",
    "            elif self.envs.action_space[agent_id].__class__.__name__ == 'Discrete':\n",
    "                action_env = np.squeeze(np.eye(self.envs.action_space[agent_id].n)[action], 1)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            actions.append(action)\n",
    "            temp_actions_env.append(action_env)\n",
    "            action_log_probs.append(_t2n(action_log_prob))\n",
    "            rnn_states.append(_t2n(rnn_state))\n",
    "            rnn_states_critic.append( _t2n(rnn_state_critic))\n",
    "\n",
    "        # [envs, agents, dim]\n",
    "        actions_env = []\n",
    "        for i in range(self.n_rollout_threads):\n",
    "            one_hot_action_env = []\n",
    "            for temp_action_env in temp_actions_env:\n",
    "                one_hot_action_env.append(temp_action_env[i])\n",
    "            actions_env.append(one_hot_action_env)\n",
    "\n",
    "        values = np.array(values).transpose(1, 0, 2)\n",
    "        actions = np.array(actions).transpose(1, 0, 2)\n",
    "        action_log_probs = np.array(action_log_probs).transpose(1, 0, 2)\n",
    "        rnn_states = np.array(rnn_states).transpose(1, 0, 2, 3)\n",
    "        rnn_states_critic = np.array(rnn_states_critic).transpose(1, 0, 2, 3)\n",
    "\n",
    "        return values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env\n",
    "\n",
    "    def insert(self, data):\n",
    "        obs, rewards, dones, infos, values, actions, action_log_probs, rnn_states, rnn_states_critic = data\n",
    "        \n",
    "\n",
    "        #rnn_states[dones == True] = np.zeros(((dones == True).sum(), self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "        if(dones.any()):\n",
    "            rnn_states = np.zeros((5, self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "            rnn_states_critic= np.zeros((5, self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "        #rnn_states_critic[dones == True] = np.zeros(((dones == True).sum(), self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "        masks = np.ones((self.n_rollout_threads, self.num_agents, 1), dtype=np.float32)\n",
    "        #masks[dones == True] = np.zeros(((dones == True).sum(), 1), dtype=np.float32)\n",
    "        if(dones.any()):\n",
    "            masks = np.zeros((5, 1), dtype=np.float32)\n",
    "        share_obs = []\n",
    "        for o in obs:\n",
    "            share_obs.append(list(chain(*o)))\n",
    "        share_obs = np.array(share_obs)\n",
    "\n",
    "        for agent_id in range(self.num_agents):\n",
    "            if not self.use_centralized_V:\n",
    "                share_obs = np.array(list(obs[:, agent_id]))\n",
    "\n",
    "            self.buffer[agent_id].insert(share_obs,\n",
    "                                        np.array(list(obs[:, agent_id])),\n",
    "                                        rnn_states[:, agent_id],\n",
    "                                        rnn_states_critic[:, agent_id],\n",
    "                                        actions[:, agent_id],\n",
    "                                        action_log_probs[:, agent_id],\n",
    "                                        values[:, agent_id],\n",
    "                                        rewards[:, agent_id],\n",
    "                                        masks[:, agent_id])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval(self, total_num_steps):\n",
    "        eval_episode_rewards = []\n",
    "        eval_obs = self.eval_envs.reset()\n",
    "\n",
    "        eval_rnn_states = np.zeros((self.n_eval_rollout_threads, self.num_agents, self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "        eval_masks = np.ones((self.n_eval_rollout_threads, self.num_agents, 1), dtype=np.float32)\n",
    "\n",
    "        for eval_step in range(self.episode_length):\n",
    "            eval_temp_actions_env = []\n",
    "            for agent_id in range(self.num_agents):\n",
    "                self.trainer[agent_id].prep_rollout()\n",
    "                eval_action, eval_rnn_state = self.trainer[agent_id].policy.act(np.array(list(eval_obs[:, agent_id])),\n",
    "                                                                                eval_rnn_states[:, agent_id],\n",
    "                                                                                eval_masks[:, agent_id],\n",
    "                                                                                deterministic=True)\n",
    "\n",
    "                eval_action = eval_action.detach().cpu().numpy()\n",
    "                # rearrange action\n",
    "                if self.eval_envs.action_space[agent_id].__class__.__name__ == 'MultiDiscrete':\n",
    "                    for i in range(self.eval_envs.action_space[agent_id].shape):\n",
    "                        eval_uc_action_env = np.eye(self.eval_envs.action_space[agent_id].high[i]+1)[eval_action[:, i]]\n",
    "                        if i == 0:\n",
    "                            eval_action_env = eval_uc_action_env\n",
    "                        else:\n",
    "                            eval_action_env = np.concatenate((eval_action_env, eval_uc_action_env), axis=1)\n",
    "                elif self.eval_envs.action_space[agent_id].__class__.__name__ == 'Discrete':\n",
    "                    eval_action_env = np.squeeze(np.eye(self.eval_envs.action_space[agent_id].n)[eval_action], 1)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "                eval_temp_actions_env.append(eval_action_env)\n",
    "                eval_rnn_states[:, agent_id] = _t2n(eval_rnn_state)\n",
    "                \n",
    "            # [envs, agents, dim]\n",
    "            eval_actions_env = []\n",
    "            for i in range(self.n_eval_rollout_threads):\n",
    "                eval_one_hot_action_env = []\n",
    "                for eval_temp_action_env in eval_temp_actions_env:\n",
    "                    eval_one_hot_action_env.append(eval_temp_action_env[i])\n",
    "                eval_actions_env.append(eval_one_hot_action_env)\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            eval_obs, eval_rewards, eval_dones, eval_infos = self.eval_envs.step(eval_actions_env)\n",
    "            eval_episode_rewards.append(eval_rewards)\n",
    "\n",
    "            eval_rnn_states[eval_dones == True] = np.zeros(((eval_dones == True).sum(), self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "            eval_masks = np.ones((self.n_eval_rollout_threads, self.num_agents, 1), dtype=np.float32)\n",
    "            eval_masks[eval_dones == True] = np.zeros(((eval_dones == True).sum(), 1), dtype=np.float32)\n",
    "\n",
    "        eval_episode_rewards = np.array(eval_episode_rewards)\n",
    "        \n",
    "        eval_train_infos = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            eval_average_episode_rewards = np.mean(np.sum(eval_episode_rewards[:, :, agent_id], axis=0))\n",
    "            eval_train_infos.append({'eval_average_episode_rewards': eval_average_episode_rewards})\n",
    "            print(\"eval average episode rewards of agent%i: \" % agent_id + str(eval_average_episode_rewards))\n",
    "\n",
    "        self.log_train(eval_train_infos, total_num_steps)  \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def render(self):        \n",
    "        all_frames = []\n",
    "        for episode in range(self.all_args.render_episodes):\n",
    "            episode_rewards = []\n",
    "            obs = self.envs.reset()\n",
    "            if self.all_args.save_gifs:\n",
    "                image = self.envs.render('rgb_array')[0][0]\n",
    "                all_frames.append(image)\n",
    "\n",
    "            rnn_states = np.zeros((self.n_rollout_threads, self.num_agents, self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "            masks = np.ones((self.n_rollout_threads, self.num_agents, 1), dtype=np.float32)\n",
    "\n",
    "            for step in range(self.episode_length):\n",
    "                calc_start = time.time()\n",
    "                \n",
    "                temp_actions_env = []\n",
    "                for agent_id in range(self.num_agents):\n",
    "                    if not self.use_centralized_V:\n",
    "                        share_obs = np.array(list(obs[:, agent_id]))\n",
    "                    self.trainer[agent_id].prep_rollout()\n",
    "                    action, rnn_state = self.trainer[agent_id].policy.act(np.array(list(obs[:, agent_id])),\n",
    "                                                                        rnn_states[:, agent_id],\n",
    "                                                                        masks[:, agent_id],\n",
    "                                                                        deterministic=True)\n",
    "\n",
    "                    action = action.detach().cpu().numpy()\n",
    "                    # rearrange action\n",
    "                    if self.envs.action_space[agent_id].__class__.__name__ == 'MultiDiscrete':\n",
    "                        for i in range(self.envs.action_space[agent_id].shape):\n",
    "                            uc_action_env = np.eye(self.envs.action_space[agent_id].high[i]+1)[action[:, i]]\n",
    "                            if i == 0:\n",
    "                                action_env = uc_action_env\n",
    "                            else:\n",
    "                                action_env = np.concatenate((action_env, uc_action_env), axis=1)\n",
    "                    elif self.envs.action_space[agent_id].__class__.__name__ == 'Discrete':\n",
    "                        action_env = np.squeeze(np.eye(self.envs.action_space[agent_id].n)[action], 1)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "\n",
    "                    temp_actions_env.append(action_env)\n",
    "                    rnn_states[:, agent_id] = _t2n(rnn_state)\n",
    "                   \n",
    "                # [envs, agents, dim]\n",
    "                actions_env = []\n",
    "                for i in range(self.n_rollout_threads):\n",
    "                    one_hot_action_env = []\n",
    "                    for temp_action_env in temp_actions_env:\n",
    "                        one_hot_action_env.append(temp_action_env[i])\n",
    "                    actions_env.append(one_hot_action_env)\n",
    "\n",
    "                # Obser reward and next obs\n",
    "                obs, rewards, dones, infos = self.envs.step(actions_env)\n",
    "                episode_rewards.append(rewards)\n",
    "\n",
    "                rnn_states[dones == True] = np.zeros(((dones == True).sum(), self.recurrent_N, self.hidden_size), dtype=np.float32)\n",
    "                masks = np.ones((self.n_rollout_threads, self.num_agents, 1), dtype=np.float32)\n",
    "                masks[dones == True] = np.zeros(((dones == True).sum(), 1), dtype=np.float32)\n",
    "\n",
    "                if self.all_args.save_gifs:\n",
    "                    image = self.envs.render('rgb_array')[0][0]\n",
    "                    all_frames.append(image)\n",
    "                    calc_end = time.time()\n",
    "                    elapsed = calc_end - calc_start\n",
    "                    if elapsed < self.all_args.ifi:\n",
    "                        time.sleep(self.all_args.ifi - elapsed)\n",
    "\n",
    "            episode_rewards = np.array(episode_rewards)\n",
    "            for agent_id in range(self.num_agents):\n",
    "                average_episode_rewards = np.mean(np.sum(episode_rewards[:, :, agent_id], axis=0))\n",
    "                print(\"eval average episode rewards of agent%i: \" % agent_id + str(average_episode_rewards))\n",
    "        \n",
    "        if self.all_args.save_gifs:\n",
    "            imageio.mimsave(str(self.gif_dir) + '/render.gif', all_frames, duration=self.all_args.ifi)\n",
    "\n",
    "\n",
    "envs = DummyVecEnv([Sea])\n",
    "\n",
    "num_agents = 5\n",
    "device = torch.device(\"cuda:0\")\n",
    "run_dir = \"ckpt\"\n",
    "\n",
    "parser = get_config()\n",
    "\n",
    "parser.add_argument('--scenario_name', type=str,\n",
    "                        default='tewzt', help=\"Which scenario to run on\")\n",
    "parser.add_argument('--num_agents', type=int,\n",
    "                    default=5, help=\"number of players\")\n",
    "\n",
    "all_args = parser.parse_known_args(\"\")[0]\n",
    "all_args.algorithm_name = \"na-mappo\"\n",
    "all_args.num_env_steps=5000000\n",
    "all_args.use_wandb = True\n",
    "all_args.use_eval = True\n",
    "all_args.log_interval = 50\n",
    "all_args.eval_episodes = 50\n",
    "all_args.eval_interval = 50\n",
    "all_args.hidden_size = 1024\n",
    "all_args.layer_N = 10\n",
    "all_args.save_interval = 40\n",
    "all_args.ppo_epoch = 15\n",
    "all_args.num_mini_batch = 3\n",
    "\n",
    "#all_args.use_adv_noise = True\n",
    "#all_args.use_value_noise = True\n",
    "#all_args.alpha = 0.1\n",
    "#all_args.use_centralized_V = False\n",
    "\n",
    "config = {\n",
    "        \"all_args\": all_args,\n",
    "        \"envs\": envs,\n",
    "        \"eval_envs\": envs,\n",
    "        \"num_agents\": num_agents,\n",
    "        \"device\": device,\n",
    "        \"run_dir\": run_dir\n",
    "    }\n",
    "\n",
    "run = wandb.init(config=all_args,\n",
    "                         project=\"test\",#all_args.env_name,\n",
    "                         entity=\"17donj\",#all_args.user_name,\n",
    "                         notes=socket.gethostname(),\n",
    "                         name=str(all_args.algorithm_name) + \"_\" +\n",
    "                         str(all_args.experiment_name) +\n",
    "                         \"_seed\" + str(all_args.seed) + \"_priorityZones_noNoise\",\n",
    "                         group=all_args.scenario_name,\n",
    "                         dir=str(run_dir),\n",
    "                         job_type=\"training\",\n",
    "                         reinit=True)\n",
    "\n",
    "runner = MPERunner(config)\n",
    "runner.run()\n",
    "\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 ('Mappo': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdb73360a735dae95f2654b25c6fc00ea09d61055dc71ce1863ad76a31391363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
